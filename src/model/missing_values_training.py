import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from src.utils.common import load_yaml, save_json_file, save_missing_values_summary, save_missing_values_correlation, save_missing_count_per_row

def missing_values_summary(df):
    missing_values = df.isnull().sum().reset_index()
    missing_values.columns = ['column_name', 'missing_count']
    missing_values = missing_values[missing_values['missing_count'] > 0]
    missing_values['missing_percentage'] = (missing_values['missing_count'] / len(df)) * 100
    missing_values = missing_values.sort_values(by='missing_percentage', ascending=False)
    return missing_values

def plot_missing_values(missing_values, paths, top_n=20):
    plt.figure(figsize=(12, 8))
    sns.barplot(x='missing_percentage', y='column_name', data=missing_values.head(top_n))
    plt.title('Top Rows with Missing Values')
    plt.xlabel('Missing Percentage')
    plt.ylabel('Column Name')
    plt.savefig(os.path.join(paths['reports']['figures']['missing_values'], 'missing_values_bar_plot.png'))
    plt.show()

def high_missing_value_correlation_pairs(corr_matrix, threshold=0.2):
    high_corr_pairs = corr_matrix.stack().reset_index()
    high_corr_pairs.columns = ['Feature1', 'Feature2', 'Correlation']
    high_corr_pairs = high_corr_pairs[high_corr_pairs['Feature1'] != high_corr_pairs['Feature2']]
    high_corr_pairs['abs_corr'] = high_corr_pairs['Correlation'].abs()
    high_corr_pairs = high_corr_pairs[high_corr_pairs['abs_corr'] > threshold]
    high_corr_pairs = high_corr_pairs.sort_values(by='abs_corr', ascending=False).drop(columns='abs_corr')
    return high_corr_pairs

def missing_values_correlation(df):
    return df.isnull().corr()

def plot_missing_values_correlation(corr_matrix, paths):
    plt.figure(figsize=(12, 8))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
    plt.title('Correlation of Missing Values')
    plt.savefig(os.path.join(paths['reports']['figures']['missing_values'], 'missing_values_correlation_heatmap.png'))
    plt.show()

def missing_count_per_row(dataframe):
    missing_count = dataframe.isnull().sum(axis=1).value_counts().reset_index()
    missing_count.columns = ['missing_count', 'row_count']
    missing_count['missing_percentage'] = (missing_count['row_count'] / len(dataframe)) * 100
    return missing_count

def plot_missing_count_per_row(dataframe, paths):
    missing_count = missing_count_per_row(dataframe)
    plt.figure(figsize=(12, 6))
    sns.histplot(missing_count['missing_percentage'], bins=30, kde=True)
    plt.title('Distribution of Missing Values per Row (Percentage)')
    plt.xlabel('Percentage of Missing Values per Row')
    plt.ylabel('Count of Rows')
    plt.savefig(os.path.join(paths['reports']['figures']['missing_values'], 'missing_values_distribution_plot.png'))
    plt.show()

def calculate_missing_values_correlation(df, target_column):
    missing_values_check = df.isnull().sum()
    features_with_missing_values = missing_values_check[missing_values_check > 0].index.tolist()
    missing_indicators = df[features_with_missing_values].isnull().astype(int)
    missing_indicators[target_column] = df[target_column].values
    correlation_with_target = missing_indicators.corr()[target_column].drop(target_column, errors='ignore')
    return correlation_with_target.sort_values(ascending=False).rename(index=str.strip)

def analyze_missing_values(df, paths):
    missing_values = missing_values_summary(df)
    print("\n--- Top Rows with Missing Values ---")
    print(missing_values.head(20))
    plot_missing_values(missing_values, paths)
    save_missing_values_summary(missing_values, paths['reports']['missing_values_summary'])

def analyze_missing_values_correlation(df, paths):
    missing_corr = missing_values_correlation(df)
    save_missing_values_correlation(missing_corr.to_dict(), paths['reports']['missing_values_correlation'])
    plot_missing_values_correlation(missing_corr, paths)

    # Calculate correlation with target
    missing_values_correlation_with_target = calculate_missing_values_correlation(df, 'HasDetections')
    save_json_file(missing_values_correlation_with_target.to_dict(), paths['reports']['missing_values_correlation_with_target'])

def analyze_missing_values_pattern(df, paths):
    missing_count = missing_count_per_row(df)
    missing_count['percent_of_rows'] = (missing_count['row_count'] / len(df)) * 100
    missing_count['percent_of_features_with_missing_values'] = (missing_count['missing_count'] / len(df.columns)) * 100
    missing_count = missing_count.sort_values(by='percent_of_features_with_missing_values', ascending=False)
    save_missing_count_per_row(missing_count.set_index('missing_count').T.to_dict(), paths['reports']['missing_count_per_row'])

    plt.figure(figsize=(12, 6))
    sns.barplot(x=missing_count['percent_of_features_with_missing_values'].round(2), y=missing_count['percent_of_rows'].round(2))
    plt.title('Distribution of Missing Values per Row')
    plt.xlabel('% Missing Features per Row')
    plt.ylabel('% of Rows')
    plt.xticks(rotation=45)
    plt.savefig(os.path.join(paths['reports']['figures']['missing_values'], 'missing_values_distribution_bar_plot.png'))
    plt.show()

    missing_count = missing_count.rename(columns={
        'missing_count': 'missing features per row',
        'percent_of_features_with_missing_values': '% missing features per row',
        'percent_of_rows': '% of rows'
    })
    print("\n--- Distribution of Missing Values per Row ---")
    print(missing_count[['missing features per row', '% missing features per row', '% of rows']].to_string(index=False))





# # src/model/missing_values_training.py

# import pandas as pd
# from sklearn.impute import SimpleImputer
# from sklearn.compose import ColumnTransformer
# from sklearn.pipeline import Pipeline
# from sklearn.preprocessing import OneHotEncoder, LabelEncoder
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import train_test_split

# def prepare_data(train_sample, feature_metadata):
#     """Prepare data by separating features into numeric and categorical, and defining imputation strategies."""
#     numeric_features = [f for f, meta in feature_metadata['features'].items() if meta['classified_data_type'] == 'numerical']
#     categorical_features = [f for f, meta in feature_metadata['features'].items() if meta['classified_data_type'] == 'categorical']

#     numeric_transformer = SimpleImputer(strategy='mean')
#     categorical_transformer = Pipeline(steps=[
#         ('imputer', SimpleImputer(strategy='most_frequent')),
#         ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
#     ])

#     preprocessor = ColumnTransformer(
#         transformers=[
#             ('num', numeric_transformer, numeric_features),
#             ('cat', categorical_transformer, categorical_features)
#         ])
#     return preprocessor, numeric_features, categorical_features

# def train_model(X_train, y_train, preprocessor):
#     """Train the RandomForest model with the given preprocessor."""
#     model = Pipeline(steps=[
#         ('preprocessor', preprocessor),
#         ('classifier', RandomForestClassifier(random_state=42))
#     ])
#     model.fit(X_train, y_train)
#     return model

# def extract_feature_importance(model, numeric_features, categorical_features):
#     """Extract feature importance from the trained model."""
#     feature_importances = model.named_steps['classifier'].feature_importances_
#     categorical_feature_names = list(model.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features))
#     features = numeric_features + categorical_feature_names
    
#     # Aggregate feature importance for one-hot encoded features
#     feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
#     aggregated_importance_df = feature_importance_df.groupby('Feature').agg({'Importance': 'sum'}).reset_index()
#     aggregated_importance_df = aggregated_importance_df.sort_values(by='Importance', ascending=False)
    
#     # Ensure we have all original features
#     all_features = numeric_features + categorical_features
#     for feature in all_features:
#         if feature not in aggregated_importance_df['Feature'].values:
#             new_row = pd.DataFrame({'Feature': [feature], 'Importance': [0]})
#             aggregated_importance_df = pd.concat([aggregated_importance_df, new_row], ignore_index=True)
    
#     return aggregated_importance_df

# def train_model_and_extract_feature_importance(X_train, y_train, numeric_features, categorical_features, feature_metadata):
#     """Train the model and extract feature importance."""
#     preprocessor, numeric_features, categorical_features = prepare_data(X_train, feature_metadata)
#     model = train_model(X_train, y_train, preprocessor)
#     importance_df = extract_feature_importance(model, numeric_features, categorical_features)
#     return importance_df




# # # src/model/missing_values_training.py

# # import pandas as pd
# # from sklearn.impute import SimpleImputer
# # from sklearn.compose import ColumnTransformer
# # from sklearn.pipeline import Pipeline
# # from sklearn.preprocessing import OneHotEncoder
# # from sklearn.ensemble import RandomForestClassifier
# # from sklearn.model_selection import train_test_split

# # def prepare_data(train_sample, feature_metadata):
# #     """Prepare data by separating features into numeric and categorical, and defining imputation strategies."""
# #     numeric_features = [f for f, meta in feature_metadata['features'].items() if meta['classified_data_type'] == 'numerical']
# #     categorical_features = [f for f, meta in feature_metadata['features'].items() if meta['classified_data_type'] == 'categorical']

# #     numeric_transformer = SimpleImputer(strategy='mean')
# #     categorical_transformer = Pipeline(steps=[
# #         ('imputer', SimpleImputer(strategy='most_frequent')),
# #         ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
# #     ])

# #     preprocessor = ColumnTransformer(
# #         transformers=[
# #             ('num', numeric_transformer, numeric_features),
# #             ('cat', categorical_transformer, categorical_features)
# #         ])
# #     return preprocessor, numeric_features, categorical_features

# # def train_model(X_train, y_train, preprocessor):
# #     """Train the RandomForest model with the given preprocessor."""
# #     model = Pipeline(steps=[
# #         ('preprocessor', preprocessor),
# #         ('classifier', RandomForestClassifier(random_state=42))
# #     ])
# #     model.fit(X_train, y_train)
# #     return model

# # def extract_feature_importance(model, numeric_features, categorical_features):
# #     """Extract feature importance from the trained model."""
# #     feature_importances = model.named_steps['classifier'].feature_importances_
# #     categorical_feature_names = list(model.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features))
# #     features = numeric_features + categorical_feature_names
    
# #     # Debugging statements
# #     print(f"Total features after one-hot encoding: {len(features)}")
# #     print(f"Expected number of features: {len(numeric_features) + len(categorical_features)}")

# #     importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
    
# #     # Aggregate feature importance for one-hot encoded features
# #     aggregated_importance_df = importance_df.groupby('Feature').agg({'Importance': 'sum'}).reset_index()
# #     aggregated_importance_df = aggregated_importance_df.sort_values(by='Importance', ascending=False)
    
# #     # Ensure we have all original features
# #     all_features = numeric_features + categorical_features
# #     for feature in all_features:
# #         if feature not in aggregated_importance_df['Feature'].values:
# #             new_row = pd.DataFrame({'Feature': [feature], 'Importance': [0]})
# #             aggregated_importance_df = pd.concat([aggregated_importance_df, new_row], ignore_index=True)
    
# #     return aggregated_importance_df

# # def train_model_and_extract_feature_importance(X_train, y_train, numeric_features, categorical_features, feature_metadata):
# #     """Train the model and extract feature importance."""
# #     preprocessor, numeric_features, categorical_features = prepare_data(X_train, feature_metadata)
# #     model = train_model(X_train, y_train, preprocessor)
# #     importance_df = extract_feature_importance(model, numeric_features, categorical_features)
# #     return importance_df
