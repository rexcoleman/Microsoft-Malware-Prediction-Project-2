# src/data/data_preparation.py

import pandas as pd
import os
import yaml
from src.data.load_data import load_sample_data, read_csv_with_progress
from src.config_loader import load_paths

def load_and_prepare_data(nrows=10000):
    paths = load_paths()
    processed_train_path = paths['data']['processed_train']

    if not os.path.exists(processed_train_path):
        print(f"{processed_train_path} not found. Generating from raw data...")
        train_sample, _ = load_sample_data(nrows=nrows)
        train_sample.to_csv(processed_train_path, index=False)
    else:
        train_sample = pd.read_csv(processed_train_path)
    
    return train_sample

def load_metadata(metadata_path):
    with open(metadata_path, 'r') as file:
        metadata = yaml.safe_load(file)
    return metadata

def create_or_load_comparison_table(train_sample, metadata, comparison_table_path):
    from src.analysis.univariate_analysis import create_comparison_table
    
    if not os.path.exists(comparison_table_path):
        comparison_table = create_comparison_table(train_sample, metadata)
        comparison_table.to_csv(comparison_table_path, index=False)
        print("\nManual review file created.")
    else:
        print("\nManual review file already exists. Skipping creation.")
        comparison_table = pd.read_csv(comparison_table_path)
    
    return comparison_table

def generate_sample_data(file_path, nrows=10000, stratify_by=None):
    """Generate a sample of the dataset with the specified number of rows."""
    data = read_csv_with_progress(file_path, chunksize=10000)
    if stratify_by:
        sample_data = data.groupby(stratify_by, group_keys=False).apply(lambda x: x.sample(min(len(x), nrows // len(data[stratify_by].unique())), random_state=42)).sample(n=nrows, random_state=42)
    else:
        sample_data = data.sample(n=nrows, random_state=42)
    return sample_data




# # src/data/data_preparation.py

# import pandas as pd
# import os
# import yaml
# from sklearn.model_selection import train_test_split
# from src.data.load_data import load_sample_data, read_csv_with_progress
# from src.config_loader import load_paths

# def load_and_prepare_data(nrows=10000):
#     paths = load_paths()
#     processed_train_path = paths['data']['processed_train']

#     if not os.path.exists(processed_train_path):
#         print(f"{processed_train_path} not found. Generating from raw data...")
#         train_sample, _ = load_sample_data(nrows=nrows)
#         train_sample.to_csv(processed_train_path, index=False)
#     else:
#         train_sample = pd.read_csv(processed_train_path)
    
#     return train_sample

# def load_metadata(metadata_path='config/feature_metadata.yaml'):
#     with open(metadata_path, 'r') as file:
#         metadata = yaml.safe_load(file)
#     return metadata

# def create_or_load_comparison_table(train_sample, metadata, comparison_table_path):
#     from src.analysis.univariate_analysis import create_comparison_table
    
#     if not os.path.exists(comparison_table_path):
#         comparison_table = create_comparison_table(train_sample, metadata)
#         comparison_table.to_csv(comparison_table_path, index=False)
#         print("\nManual review file created.")
#     else:
#         print("\nManual review file already exists. Skipping creation.")
#         comparison_table = pd.read_csv(comparison_table_path)
    
#     return comparison_table

# def generate_sample_data(file_path, nrows=10000, stratify_by=None):
#     """Generate a sample of the dataset with the specified number of rows."""
#     data = read_csv_with_progress(file_path, chunksize=10000)
#     if stratify_by:
#         sample_data, _ = train_test_split(data, train_size=nrows, stratify=data[stratify_by], random_state=42)
#     else:
#         sample_data = data.sample(n=nrows, random_state=42)
#     return sample_data










# # src/data/data_preparation.py

# import pandas as pd
# import os
# import yaml
# from sklearn.model_selection import train_test_split
# from src.data.load_data import load_sample_data, read_csv_with_progress
# from src.config_loader import load_paths

# def load_and_prepare_data(nrows=10000):
#     """Load and prepare the training data."""
#     paths = load_paths()
#     processed_train_path = paths['data']['processed_train']

#     if not os.path.exists(processed_train_path):
#         print(f"{processed_train_path} not found. Generating from raw data...")
#         train_sample, _ = load_sample_data(nrows=nrows)
#         train_sample.to_csv(processed_train_path, index=False)
#     else:
#         train_sample = pd.read_csv(processed_train_path)
    
#     return train_sample

# def load_metadata(metadata_path='config/feature_metadata.yaml'):
#     with open(metadata_path, 'r') as file:
#         metadata = yaml.safe_load(file)
#     return metadata

# def create_or_load_comparison_table(train_sample, metadata, comparison_table_path):
#     from src.analysis.univariate_analysis import create_comparison_table
    
#     if not os.path.exists(comparison_table_path):
#         comparison_table = create_comparison_table(train_sample, metadata)
#         comparison_table.to_csv(comparison_table_path, index=False)
#         print("\nManual review file created.")
#     else:
#         print("\nManual review file already exists. Skipping creation.")
#         comparison_table = pd.read_csv(comparison_table_path)
    
#     return comparison_table

# def generate_sample_data(file_path, nrows=10000, stratify_by=None):
#     """Generate a sample of the dataset with the specified number of rows, optionally stratified by a target variable."""
#     data = read_csv_with_progress(file_path, chunksize=10000)
#     if stratify_by and stratify_by in data.columns:
#         _, sample_data = train_test_split(data, train_size=nrows, stratify=data[stratify_by], random_state=42)
#     else:
#         sample_data = data.sample(n=nrows, random_state=42)
#     return sample_data
