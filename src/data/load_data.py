# src/data/load_data.py

import pandas as pd
from tqdm import tqdm
from sklearn.model_selection import train_test_split

def load_data(file_path, nrows=None, chunksize=10000, dtype=None):
    """
    Load data from a CSV file with a progress bar. Optionally load a random sample.

    Parameters:
    - file_path (str): The path to the CSV file.
    - nrows (int): Number of rows to read. Reads all rows if None.
    - chunksize (int): Size of the chunks to read at a time.
    - dtype (dict): Dictionary of column data types.

    Returns:
    - DataFrame: Loaded data.
    """
    if nrows is not None:
        # Calculate the number of chunks to read
        chunks = []
        with pd.read_csv(file_path, chunksize=chunksize, dtype=dtype) as reader:
            for i, chunk in enumerate(tqdm(reader, desc=f"Loading {file_path}")):
                chunks.append(chunk)
                if len(chunks) * chunksize >= nrows:
                    break
        data = pd.concat(chunks, ignore_index=True)
        return data.sample(n=nrows)

    # Use chunksize to load data in chunks with a progress bar
    chunks = []
    with pd.read_csv(file_path, chunksize=chunksize, dtype=dtype) as reader:
        for chunk in tqdm(reader, desc=f"Loading {file_path}"):
            chunks.append(chunk)
    return pd.concat(chunks, ignore_index=True)

def sample_data(df, n_samples=10000, random_state=42, stratify_col=None):
    """
    Sample the dataframe with a progress bar.

    Parameters:
    - df (DataFrame): The dataframe to sample from.
    - n_samples (int): The number of samples to draw.
    - random_state (int): Random seed for reproducibility.
    - stratify_col (str): Column name to stratify the sample by.

    Returns:
    - DataFrame: Sampled data.
    """
    if stratify_col:
        _, sample = train_test_split(df, test_size=n_samples, random_state=random_state, stratify=df[stratify_col])
        return sample
    return df.sample(n=n_samples, random_state=random_state)






# # src/data/load_data.py

# import pandas as pd
# from tqdm import tqdm
# from sklearn.model_selection import train_test_split
# import logging

# # Configure logging
# logger = logging.getLogger(__name__)

# def load_data(file_path, nrows=None, chunksize=10000):
#     """
#     Load data from a CSV file with a progress bar. Optionally load a random sample.

#     Parameters:
#     - file_path (str): The path to the CSV file.
#     - nrows (int): Number of rows to read. Reads all rows if None.
#     - chunksize (int): Size of the chunks to read at a time.

#     Returns:
#     - DataFrame: Loaded data.
#     """
#     try:
#         if nrows is not None:
#             # Calculate the number of chunks to read
#             chunks = []
#             logger.info(f"Loading data from {file_path} with {nrows} rows in chunks of {chunksize}.")
#             with pd.read_csv(file_path, chunksize=chunksize) as reader:
#                 for i, chunk in enumerate(tqdm(reader, desc=f"Loading {file_path}")):
#                     chunks.append(chunk)
#                     if len(chunks) * chunksize >= nrows:
#                         break
#             data = pd.concat(chunks, ignore_index=True)
#             return data.sample(n=nrows)

#         # Use chunksize to load data in chunks with a progress bar
#         chunks = []
#         logger.info(f"Loading data from {file_path} in chunks of {chunksize}.")
#         with pd.read_csv(file_path, chunksize=chunksize) as reader:
#             for chunk in tqdm(reader, desc=f"Loading {file_path}"):
#                 chunks.append(chunk)
#         return pd.concat(chunks, ignore_index=True)

#     except Exception as e:
#         logger.error(f"Error loading data from {file_path}: {e}")
#         raise

# def sample_data(df, n_samples=10000, random_state=42, stratify_col=None):
#     """
#     Sample the dataframe with a progress bar.

#     Parameters:
#     - df (DataFrame): The dataframe to sample from.
#     - n_samples (int): The number of samples to draw.
#     - random_state (int): Random seed for reproducibility.
#     - stratify_col (str): Column name to stratify the sample by.

#     Returns:
#     - DataFrame: Sampled data.
#     """
#     try:
#         logger.info(f"Sampling {n_samples} entries from dataframe.")
#         if stratify_col:
#             _, sample = train_test_split(df, test_size=n_samples, random_state=random_state, stratify=df[stratify_col])
#             return sample
#         return df.sample(n=n_samples, random_state=random_state)
    
#     except Exception as e:
#         logger.error(f"Error sampling data: {e}")
#         raise




# # # # src/data/load_data.py

# # # import pandas as pd
# # # from tqdm import tqdm
# # # from sklearn.model_selection import train_test_split

# # # def load_data(file_path, nrows=None, chunksize=10000):
# # #     """
# # #     Load data from a CSV file with a progress bar. Optionally load a random sample.

# # #     Parameters:
# # #     - file_path (str): The path to the CSV file.
# # #     - nrows (int): Number of rows to read. Reads all rows if None.
# # #     - chunksize (int): Size of the chunks to read at a time.

# # #     Returns:
# # #     - DataFrame: Loaded data.
# # #     """
# # #     tqdm.pandas(desc="Loading data")
    
# # #     if nrows is not None:
# # #         # Calculate the number of chunks to read
# # #         chunks = []
# # #         total_chunks = (nrows // chunksize) + 1
# # #         with pd.read_csv(file_path, chunksize=chunksize) as reader:
# # #             for chunk in tqdm(reader, desc=f"Loading {file_path}", total=total_chunks):
# # #                 chunks.append(chunk)
# # #                 if len(chunks) * chunksize >= nrows:
# # #                     break
# # #         data = pd.concat(chunks, ignore_index=True)
# # #         return data.sample(n=nrows)

# # #     # Use chunksize to load data in chunks with a progress bar
# # #     chunks = []
# # #     with pd.read_csv(file_path, chunksize=chunksize) as reader:
# # #         for chunk in tqdm(reader, desc=f"Loading {file_path}"):
# # #             chunks.append(chunk)
# # #     return pd.concat(chunks, ignore_index=True)

# # # def sample_data(df, n_samples=10000, random_state=42, stratify_col=None):
# # #     """
# # #     Sample the dataframe with a progress bar.

# # #     Parameters:
# # #     - df (DataFrame): The dataframe to sample from.
# # #     - n_samples (int): The number of samples to draw.
# # #     - random_state (int): Random seed for reproducibility.
# # #     - stratify_col (str): Column name to stratify the sample by.

# # #     Returns:
# # #     - DataFrame: Sampled data.
# # #     """
# # #     if stratify_col:
# # #         _, sample = train_test_split(df, test_size=n_samples, random_state=random_state, stratify=df[stratify_col])
# # #         return sample
# # #     return df.sample(n=n_samples, random_state=random_state)



