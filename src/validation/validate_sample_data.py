# src/validation/validate_sample_data.py

import logging
import pandas as pd
from tqdm.notebook import tqdm
from src.utils.display_utils import display_dataframe_as_html
from src.utils.data_loading_utils import read_csv_with_progress



def validate_samples(train_data_path, test_data_path, train_sample_path, test_sample_path, stratify_by):
    """
    Validate the generated sample datasets against the original datasets.

    Parameters:
    train_data_path (str): Path to the raw training data.
    test_data_path (str): Path to the raw testing data.
    train_sample_path (str): Path to the saved training sample.
    test_sample_path (str): Path to the saved testing sample.
    stratify_by (str): Column to stratify by.
    """
    try:
        original_train_data = read_csv_with_progress(train_data_path)
        original_test_data = read_csv_with_progress(test_data_path)
        train_sample = pd.read_csv(train_sample_path)
        test_sample = pd.read_csv(test_sample_path)

        # Validate that the target column exists in the original train dataset
        if stratify_by not in original_train_data.columns:
            raise KeyError(f"Column '{stratify_by}' not found in original train data.")

        # Validate stratification for the training data
        is_train_valid, comparison_df = validate_stratification_with_progress(original_train_data, train_sample, stratify_by)

        display_dataframe_as_html(comparison_df, title="Class Distribution Comparison", description="Comparison of class distributions between original and sample dataframes.")

        if is_train_valid:
            logging.info("Stratification is correct for the train sample.")
        else:
            logging.warning("Stratification is incorrect for the train sample.")

        # Validate size and shape for both train and test data
        original_train_shape, train_sample_shape, train_sample_size = validate_size_and_shape(original_train_data, train_sample)
        original_test_shape, test_sample_shape, test_sample_size = validate_size_and_shape(original_test_data, test_sample)

        # Display size and shape validation results
        validation_results = pd.DataFrame({
            "Dataset": ["Original Train", "Train Sample", "Original Test", "Test Sample"],
            "Shape": [original_train_shape, train_sample_shape, original_test_shape, test_sample_shape],
            "Size (MB)": ["N/A", train_sample_size, "N/A", test_sample_size]
        })

        display_dataframe_as_html(validation_results, title="Size and Shape Validation Results", description="Validation results of size and shape for original and sample datasets.")
    except Exception as e:
        logging.error(f"Error validating samples: {e}")
        raise




def validate_stratification_with_progress(original_df, sample_df, stratify_by):
    """
    Validate that the stratification of the sample matches the original dataset.
    
    Parameters:
    original_df (pd.DataFrame): The original dataframe.
    sample_df (pd.DataFrame): The sampled dataframe.
    stratify_by (str): The column to stratify by.
    
    Returns:
    bool: True if stratification is correct, False otherwise.
    pd.DataFrame: DataFrame showing the comparison of distributions.
    """
    if stratify_by not in original_df.columns or stratify_by not in sample_df.columns:
        raise KeyError(f"Column '{stratify_by}' not found in one of the dataframes.")
    
    original_distribution = original_df[stratify_by].value_counts(normalize=True)
    sample_distribution = sample_df[stratify_by].value_counts(normalize=True)
    
    comparison_df = pd.DataFrame({
        'Original': original_distribution,
        'Sample': sample_distribution
    })

    progress_bar = tqdm(total=len(comparison_df), desc="Validating stratification")
    for i in range(len(comparison_df)):
        progress_bar.update(1)
    progress_bar.close()

    return comparison_df['Original'].equals(comparison_df['Sample']), comparison_df


def validate_size_and_shape(original_df, sample_df):
    """
    Validate the size and shape of the sample dataset against the original dataset.
    
    Parameters:
    original_df (pd.DataFrame): The original dataframe.
    sample_df (pd.DataFrame): The sampled dataframe.

    Returns:
    tuple: Shapes of original and sample datasets and size of the sample dataset in MB.
    """
    original_shape = original_df.shape
    sample_shape = sample_df.shape
    sample_size_mb = sample_df.memory_usage(deep=True).sum() / (1024 * 1024)

    return original_shape, sample_shape, sample_size_mb















# def ensure_all_features_in_comparison_table(dataframe, comparison_table, metadata, comparison_table_path):
#     """
#     Ensure all features in the dataset are included in the comparison table.
    
#     Parameters:
#     dataframe (pd.DataFrame): The dataset.
#     comparison_table (pd.DataFrame): The existing comparison table.
#     metadata (dict): The feature metadata.
#     comparison_table_path (str): The path to save the updated comparison table.
    
#     Returns:
#     pd.DataFrame: The updated comparison table.
#     """
#     # Extract existing features from the comparison table
#     existing_features = comparison_table['Feature'].tolist()
    
#     # Find missing features
#     missing_features = [feature for feature in dataframe.columns if feature not in existing_features]
    
#     # Create entries for missing features
#     new_entries = []
#     for feature in missing_features:
#         classified_type = metadata['features'][feature].get('classified_data_type', 'Unknown')
#         determined_type = determine_feature_type(feature, metadata)
#         new_entries.append({
#             'Feature': feature,
#             'Previously Analyzed Type': classified_type,
#             'Current Analyzed Type': determined_type,
#             'Discrepancy': classified_type != determined_type,
#             'Manual Review and Update': determined_type  # Initialize with current analyzed type
#         })
    
#     # Append new entries to the comparison table
#     if new_entries:
#         new_entries_df = pd.DataFrame(new_entries)
#         comparison_table = pd.concat([comparison_table, new_entries_df], ignore_index=True)
#         comparison_table.sort_values(by=['Discrepancy', 'Previously Analyzed Type'], ascending=[False, True], inplace=True)
    
#     # Save the updated comparison table
#     comparison_table.to_csv(comparison_table_path, index=False)
    
#     # Print validation results
#     print(f"Total features in the dataset: {len(dataframe.columns)}")
#     print(f"Total features in the comparison table: {comparison_table.shape[0]}")
#     print(f"Missing features added: {len(missing_features)}")
    
#     return comparison_table

# # src/utils/validation_utils.py
# import logging


# def validate_data_types(data, metadata):
#     """
#     Validate that data types of the DataFrame columns match the metadata specifications.

#     Parameters:
#     data (pd.DataFrame): The DataFrame to validate.
#     metadata (dict): Metadata containing feature information.

#     Returns:
#     None
#     """
#     logging.info("Starting data type validation")
#     validation_errors = []
    
#     for feature, details in metadata['features'].items():
#         if feature in data.columns:
#             expected_dtype = details.get('technical_data_type', None)
#             actual_dtype = str(data[feature].dtype)
            
#             if expected_dtype and expected_dtype != actual_dtype:
#                 error_message = f"Feature '{feature}': Expected dtype technical_data_type '{expected_dtype}', but got dtype '{actual_dtype}'"
#                 validation_errors.append(error_message)
#                 logging.error(error_message)
#             else:
#                 logging.info(f"Feature '{feature}': Data type dtype '{actual_dtype}' matches the expected technical_data_type type.")
    
#     if not validation_errors:
#         logging.info("Data type validation passed with no errors.")
#     else:
#         logging.warning("Data type validation found discrepancies.")
#         for error in validation_errors:
#             logging.warning(error)

#     logging.info("Data type validation completed.")




# import os
# import json
# import yaml
# import logging
# from jsonschema import validate, ValidationError

# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# def load_yaml(file_path):
#     """
#     Load a YAML file from the specified path.

#     Parameters:
#     file_path (str): Path to the YAML file.

#     Returns:
#     dict: Parsed YAML content.
#     """
#     try:
#         with open(file_path, 'r') as file:
#             return yaml.safe_load(file)
#     except Exception as e:
#         logging.error(f"Error loading YAML file {file_path}: {e}")
#         raise

# def load_json(file_path):
#     """
#     Load a JSON file from the specified path.

#     Parameters:
#     file_path (str): Path to the JSON file.

#     Returns:
#     dict: Parsed JSON content.
#     """
#     try:
#         with open(file_path, 'r') as file:
#             return json.load(file)
#     except FileNotFoundError as e:
#         logging.error(f"Error loading JSON file {file_path}: {e}")
#         raise
#     except Exception as e:
#         logging.error(f"Error loading JSON file {file_path}: {e}")
#         raise


# def load_json_files(config_path="config/json_files.yaml", base_dir=""):
#     """
#     Load JSON file list from a YAML configuration file and load the JSON data.

#     Parameters:
#     config_path (str): Path to the YAML configuration file.
#     base_dir (str): Base directory for JSON files.

#     Returns:
#     dict: Dictionary of loaded JSON data.
#     """
#     try:
#         with open(config_path, 'r') as file:
#             config = yaml.safe_load(file)
#         json_files = config['json_files']

#         json_data = {}
#         for json_file in json_files:
#             file_path = os.path.join(base_dir, json_file)
#             if os.path.isfile(file_path):
#                 with open(file_path, 'r') as f:
#                     json_data[os.path.splitext(json_file)[0]] = json.load(f)
#                 logging.debug(f"Loaded {json_file} from {file_path}")
#             else:
#                 logging.warning(f"File {json_file} not found in {base_dir}")
#         return json_data
#     except FileNotFoundError as e:
#         logging.error(f"Error: {e}")
#         raise
#     except yaml.YAMLError as e:
#         logging.error(f"Error parsing YAML file: {e}")
#         raise
#     except json.JSONDecodeError as e:
#         logging.error(f"Error decoding JSON file: {e}")
#         raise


# # Validate metadata schema
# def validate_schema(metadata, schema):
#     try:
#         validate(instance=metadata, schema=schema)
#         logging.info("Metadata validation successful.")
#         return True
#     except ValidationError as e:
#         logging.error(f"Metadata validation error: {e.message}")
#         return False

# def validate_against_source(metadata, source_data, field_name):
#     logging.info(f"Validating metadata against source data for field: {field_name}")
#     if field_name == "data_overview":
#         # Accessing nested data structure
#         source_data = source_data["data_overview"]
    
#     for feature, details in source_data.items():
#         if feature not in metadata['features']:
#             logging.warning(f"Feature {feature} not found in metadata.")
#             continue
        
#         if isinstance(details, dict):
#             for key, value in details.items():
#                 if metadata['features'][feature].get(key) != value:
#                     logging.warning(f"Mismatch found for feature {feature} in field {key}: Metadata value - {metadata['features'][feature].get(key)}, Source value - {value}")
#         else:
#             # Handling non-dict details (e.g., lists)
#             if metadata['features'][feature] != details:
#                 logging.warning(f"Mismatch found for feature {feature}: Metadata value - {metadata['features'][feature]}, Source value - {details}")

                

# def validate_missing_values(metadata, source_data):
#     logging.info("Validating missing values")
#     for feature, details in source_data.items():
#         if feature not in metadata['features']:
#             logging.warning(f"Feature {feature} not found in metadata.")
#             continue
        
#         mv_metadata = metadata['features'][feature].get('missing_values', {})
#         if mv_metadata.get('count') != details.get('count') or mv_metadata.get('percentage') != details.get('percentage'):
#             logging.warning(f"Mismatch in missing values for feature {feature}: Metadata - {mv_metadata}, Source - {details}")

# def validate_correlation_data(metadata, source_data, correlation_field):
#     logging.info(f"Validating correlation data for field: {correlation_field}")
#     for feature, details in source_data.items():
#         if feature not in metadata['features']:
#             logging.warning(f"Feature {feature} not found in metadata.")
#             continue
        
#         correlation_metadata = metadata['features'][feature].get('correlations', {}).get(correlation_field, {})
#         for other_feature, corr_value in details.items():
#             if correlation_metadata.get(other_feature) != corr_value:
#                 logging.warning(f"Mismatch in correlation data for feature {feature} with {other_feature}: Metadata - {correlation_metadata.get(other_feature)}, Source - {corr_value}")


# def validate_feature_metadata(metadata_path, schema_path, source_json_files, analysis_results_dir):
#     metadata = load_yaml(metadata_path)
#     schema = load_yaml(schema_path)

#     # Validate metadata against schema
#     if not validate_schema(metadata, schema):
#         logging.error("Schema validation failed.")
#         return

#     # Validate metadata against source JSON files
#     for json_file in source_json_files:
#         source_data_path = os.path.join(analysis_results_dir, json_file)
#         source_data = load_json(source_data_path)
#         field_name = os.path.splitext(os.path.basename(json_file))[0]
        
#         if field_name == 'missing_values':
#             validate_missing_values(metadata, source_data)
#         elif field_name == 'correlations':
#             validate_correlation_data(metadata, source_data, 'feature_correlation_with_other_features')
#         elif field_name == 'target_correlations':
#             validate_correlation_data(metadata, source_data, 'feature_correlation_with_target')
#         else:
#             validate_against_source(metadata, source_data, field_name)
        
#     logging.info("Validation completed.")

