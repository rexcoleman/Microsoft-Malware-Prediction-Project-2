# src/feature_engineering/imputation.py

import os
import yaml
import json
import logging
import pandas as pd
from tqdm.notebook import tqdm
from typing import List, Tuple, Dict
from src.feature_engineering.shared_utils import extract_missing_percentage, should_label_missing

def impute_missing_values(data: pd.DataFrame, metadata: dict, features_to_drop: List[str], imputation_values: dict = None, paths: dict = None, is_train: bool = True) -> pd.DataFrame:
    data = convert_data_types(data, metadata)
    data.replace('nan', pd.NA, inplace=True)
    for feature in tqdm(metadata['features'].keys(), desc="Imputing missing values"):
        if feature not in features_to_drop:
            imputation_strategy = extract_imputation_strategy(metadata, feature)
            if is_train:
                if imputation_strategy == 'Mode':
                    mode_value = data[feature].mode().dropna()
                    imputation_values[feature] = mode_value[0] if not mode_value.empty else None
                elif imputation_strategy == 'Median':
                    median_value = data[feature].median()
                    imputation_values[feature] = median_value if pd.notna(median_value) else None
                elif imputation_strategy == 'Mean':
                    mean_value = data[feature].mean()
                    imputation_values[feature] = mean_value if pd.notna(mean_value) else None

                if imputation_values[feature] is not None:
                    data[feature] = data[feature].fillna(imputation_values[feature])
                    logging.info(f"Feature '{feature}' imputed with value: {imputation_values[feature]}")
                else:
                    logging.warning(f"Imputation for feature '{feature}' resulted in NaN. No imputation applied.")
            else:
                if feature in imputation_values:
                    data[feature] = data[feature].fillna(imputation_values[feature])
                    logging.info(f"Feature '{feature}' imputed in test set with value: {imputation_values[feature]}")
                else:
                    logging.warning(f"No imputation value found for feature '{feature}' in test data.")
                    data[feature] = data[feature].fillna(data[feature].mode()[0] if not data[feature].mode().empty else 0)
    return data

 

def extract_imputation_data(metadata: dict, features_to_drop: List[str]) -> pd.DataFrame:
    """
    Extract imputation strategy information into a DataFrame.

    Parameters:
    - metadata (dict): Metadata containing feature information.
    - features_to_drop (List[str]): List of features to drop.

    Returns:
    - pd.DataFrame: DataFrame with imputation data.
    """
    imputation_data = []
    for feature in metadata['features'].keys():
        if feature not in features_to_drop:
            imputation_strategy = extract_imputation_strategy(metadata, feature)
            missing_percentage = extract_missing_percentage(metadata, feature)
            classified_data_type = extract_classified_data_type(metadata, feature)
            if missing_percentage is not None:
                feature_info = {
                    'Feature': feature,
                    'Classification': classified_data_type,
                    'Imputation Strategy': imputation_strategy,
                    'Missing Percentage (%)': missing_percentage
                }
                imputation_data.append(feature_info)
    return pd.DataFrame(imputation_data)


def convert_numpy_types(obj):
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()  # Convert numpy arrays to lists
    elif isinstance(obj, pd.Series):
        return obj.tolist()  # Convert pandas Series to lists
    else:
        return obj







def validate_and_impute(train_data, imputation_values):
    # Check for any residual <NA> values
    for feature in train_data.columns:
        if train_data[feature].isna().any():
            # Impute again using the pre-calculated imputation value or a fallback strategy
            train_data[feature] = train_data[feature].fillna(imputation_values.get(feature, fallback_value))
    return train_data


def save_essential_missing_value_features(essential_missing_value_features: Dict[str, Dict], essential_missing_value_features_path: str) -> None:
    """
    Save essential missing value features to a JSON file using save_json_with_pipeline.
    
    Parameters:
    - essential_missing_value_features (Dict[str, Dict]): Dictionary representing essential missing value features.
    - essential_missing_value_features_path (str): Path to save the JSON file.
    """
    save_json_with_pipeline(essential_missing_value_features, essential_missing_value_features_path)
    logging.info(f"Essential missing value features saved to {essential_missing_value_features_path}")
    
    # with open(essential_missing_value_features_path, 'r') as f:
    #     essential_missing_value_features_content = json.load(f)
    # logging.info(f"Content of essential_missing_value_features.json: {essential_missing_value_features_content}")


def create_imputation_strategy_table(metadata: dict, high_missing_values_df: pd.DataFrame, moderate_missing_values_df: pd.DataFrame) -> pd.DataFrame:
    """
    Create the imputation strategy table based on feature data.

    Parameters:
    - metadata (dict): Metadata dictionary.
    - high_missing_values_df (pd.DataFrame): DataFrame of high missing values features.
    - moderate_missing_values_df (pd.DataFrame): DataFrame of moderate missing values features.

    Returns:
    - pd.DataFrame: DataFrame containing imputation strategy data.
    """
    if isinstance(metadata, str):
        logging.error("Expected metadata to be a dictionary, but got a string.")
        raise TypeError("Metadata should be a dictionary, not a string.")

    imputation_data = []
    for feature, details in metadata['features'].items():
        missing_percentage = details.get('missing_values', {}).get('percentage', None)
        classification = details.get('general_attributes', {}).get('classified_data_type', 'Unknown')
        imputation_strategy = 'Keep'
        comments = ''
        
        if feature in high_missing_values_df['Feature'].values:
            imputation_strategy = 'Drop'
            comments = 'High missing values.'
        elif feature in moderate_missing_values_df['Feature'].values:
            if classification == 'categorical':
                imputation_strategy = 'Mode'
            elif classification == 'numerical':
                imputation_strategy = 'Median'
            elif classification == 'binary':
                imputation_strategy = 'Mode'
            comments = 'Moderate missing values.'
        elif missing_percentage is not None and missing_percentage < 10:
            if classification == 'categorical':
                imputation_strategy = 'Mode'
            elif classification == 'numerical':
                imputation_strategy = 'Median'
            elif classification == 'binary':
                imputation_strategy = 'Mode'
        
        imputation_info = {
            'Feature': feature,
            'Classification': classification,
            'Imputation Strategy': imputation_strategy,
            'Comments': comments
        }
        imputation_data.append(imputation_info)
    
    return pd.DataFrame(imputation_data)


def save_imputation_strategy(imputation_df: pd.DataFrame, save_path: str) -> None:
    """
    Save the transformed imputation strategy to a JSON file.
    
    Parameters:
    - imputation_df (pd.DataFrame): DataFrame containing imputation strategies.
    - save_path (str): The path where the JSON file should be saved.
    """
    imputation_strategy_dict = transform_imputation_strategy_to_schema(imputation_df)
    save_json_with_pipeline(imputation_strategy_dict, save_path)
    logging.info(f"Imputation strategy saved to {save_path}")


def save_dataframes(
    high_missing_values_df: pd.DataFrame,
    moderate_missing_values_df: pd.DataFrame,
    low_missing_values_df: pd.DataFrame,
    high_corr_with_target_df: pd.DataFrame,
    moderate_corr_with_target_df: pd.DataFrame,
    low_corr_with_target_df: pd.DataFrame,
    high_missing_corr_with_target_df: pd.DataFrame,
    moderate_missing_corr_with_target_df: pd.DataFrame,
    low_missing_corr_with_target_df: pd.DataFrame,
    paths: Dict[str, Dict[str, str]]
) -> None:
    """
    Save various DataFrames to CSV files.

    Parameters:
    - high_missing_values_df (pd.DataFrame): DataFrame of high missing values features.
    - moderate_missing_values_df (pd.DataFrame): DataFrame of moderate missing values features.
    - low_missing_values_df (pd.DataFrame): DataFrame of low missing values features.
    - high_corr_with_target_df (pd.DataFrame): DataFrame of high correlation with target features.
    - moderate_corr_with_target_df (pd.DataFrame): DataFrame of moderate correlation with target features.
    - low_corr_with_target_df (pd.DataFrame): DataFrame of low correlation with target features.
    - high_missing_corr_with_target_df (pd.DataFrame): DataFrame of high missing values correlation with target.
    - moderate_missing_corr_with_target_df (pd.DataFrame): DataFrame of moderate missing values correlation with target.
    - low_missing_corr_with_target_df (pd.DataFrame): DataFrame of low missing values correlation with target.
    - paths (Dict[str, Dict[str, str]]): Dictionary containing file paths.
    """
    output_dir = paths['reports']['analysis_results']
    high_output_path = os.path.join(output_dir, 'high_missing_values_handling_strategy.csv')
    moderate_output_path = os.path.join(output_dir, 'moderate_missing_values_handling_strategy.csv')
    low_output_path = os.path.join(output_dir, 'low_missing_values_handling_strategy.csv')
    high_corr_with_target_output_path = os.path.join(output_dir, 'high_corr_with_target.csv')
    moderate_corr_with_target_output_path = os.path.join(output_dir, 'moderate_corr_with_target.csv')
    low_corr_with_target_output_path = os.path.join(output_dir, 'low_corr_with_target.csv')
    high_missing_corr_with_target_output_path = os.path.join(output_dir, 'high_missing_corr_with_target.csv')
    moderate_missing_corr_with_target_output_path = os.path.join(output_dir, 'moderate_missing_corr_with_target.csv')
    low_missing_corr_with_target_output_path = os.path.join(output_dir, 'low_missing_corr_with_target.csv')

    save_dataframe(high_missing_values_df, high_output_path)
    save_dataframe(moderate_missing_values_df, moderate_output_path)
    save_dataframe(low_missing_values_df, low_output_path)
    save_dataframe(high_corr_with_target_df, high_corr_with_target_output_path)
    save_dataframe(moderate_corr_with_target_df, moderate_corr_with_target_output_path)
    save_dataframe(low_corr_with_target_df, low_corr_with_target_output_path)
    save_dataframe(high_missing_corr_with_target_df, high_missing_corr_with_target_output_path)
    save_dataframe(moderate_missing_corr_with_target_df, moderate_missing_corr_with_target_output_path)
    save_dataframe(low_missing_corr_with_target_df, low_missing_corr_with_target_output_path)

    logging.info(f"High missing values handling strategy table saved to {high_output_path}")
    logging.info(f"Moderate missing values handling strategy table saved to {moderate_output_path}")
    logging.info(f"Low missing values handling strategy table saved to {low_output_path}")
    logging.info(f"High Correlation with Target table saved to {high_corr_with_target_output_path}")
    logging.info(f"Moderate Correlation with Target table saved to {moderate_corr_with_target_output_path}")
    logging.info(f"Low Correlation with Target table saved to {low_corr_with_target_output_path}")
    logging.info(f"High Missing Values Correlation with Target table saved to {high_missing_corr_with_target_output_path}")
    logging.info(f"Moderate Missing Values Correlation with Target table saved to {moderate_missing_corr_with_target_output_path}")
    logging.info(f"Low Missing Values Correlation with Target table saved to {low_missing_corr_with_target_output_path}")


def extract_feature_data_via_pipeline(metadata_path: str, features: list) -> pd.DataFrame:
    """
    Extract relevant feature data for analysis using the metadata extraction pipeline.

    Parameters:
    metadata_path (str): Path to the feature metadata JSON file.
    features (list): List of feature names to extract.

    Returns:
    pd.DataFrame: DataFrame containing extracted feature data.
    """
    try:
        # Load the metadata
        metadata = load_json_file(metadata_path)

        # Store DataFrames for each feature
        data = []

        for feature in features:
            # Extract relevant metadata using modular functions
            general_attributes = extract_general_attributes(metadata, feature)
            missing_values = extract_missing_values(metadata, feature)
            correlations = extract_correlations(metadata, feature)

            # Combine all extracted data into a single DataFrame
            combined_data = {'Feature': feature, **general_attributes, **missing_values, **correlations}
            feature_df = pd.DataFrame([combined_data])

            data.append(feature_df)

        # Concatenate all DataFrames into a single DataFrame
        if data:
            df_combined = pd.concat(data, ignore_index=True)
        else:
            df_combined = pd.DataFrame()  # Return an empty DataFrame if no data

        df_combined['Correlation with Target'] = pd.to_numeric(df_combined['Correlation with Target'], errors='coerce')
        df_combined['Missing Values Correlation with Target'] = pd.to_numeric(df_combined['Missing Values Correlation with Target'], errors='coerce')

        return df_combined.sort_values(by='Missing Percentage (%)', ascending=False)
    except Exception as e:
        logging.error(f"Error extracting feature data via pipeline: {e}", exc_info=True)
        raise


def display_dataframes(
    high_missing_values_df: pd.DataFrame,
    moderate_missing_values_df: pd.DataFrame,
    low_missing_values_df: pd.DataFrame,
    high_corr_with_target_df: pd.DataFrame,
    moderate_corr_with_target_df: pd.DataFrame,
    low_corr_with_target_df: pd.DataFrame,
    high_missing_corr_with_target_df: pd.DataFrame,
    moderate_missing_corr_with_target_df: pd.DataFrame,
    low_missing_corr_with_target_df: pd.DataFrame
) -> None:
    """
    Display DataFrames in a visually appealing format.

    Parameters:
    - high_missing_values_df (pd.DataFrame): DataFrame of high missing values features.
    - moderate_missing_values_df (pd.DataFrame): DataFrame of moderate missing values features.
    - low_missing_values_df (pd.DataFrame): DataFrame of low missing values features.
    - high_corr_with_target_df (pd.DataFrame): DataFrame of high correlation with target features.
    - moderate_corr_with_target_df (pd.DataFrame): DataFrame of moderate correlation with target features.
    - low_corr_with_target_df (pd.DataFrame): DataFrame of low correlation with target features.
    - high_missing_corr_with_target_df (pd.DataFrame): DataFrame of high missing values correlation with target.
    - moderate_missing_corr_with_target_df (pd.DataFrame): DataFrame of moderate missing values correlation with target.
    - low_missing_corr_with_target_df (pd.DataFrame): DataFrame of low missing values correlation with target.
    """
    print("Features with High % of Missing Values (>=50%)\n")
    display(HTML(high_missing_values_df.to_html(index=False)))
    print(f"Size of High Missing Values table: {high_missing_values_df.shape}\n")

    print("\nFeatures with Moderate % of Missing Values (>=10% and < 50%)\n")
    display(HTML(moderate_missing_values_df.to_html(index=False)))
    print(f"Size of Moderate Missing Values table: {moderate_missing_values_df.shape}\n")

    print("\nFeatures with Low % of Missing Values (<10%)\n")
    display(HTML(low_missing_values_df.to_html(index=False)))
    print(f"Size of Low Missing Values table: {low_missing_values_df.shape}\n")

    print("\nHigh Correlation with Target (|correlation| > 0.7)\n")
    display(HTML(high_corr_with_target_df.to_html(index=False)))
    print(f"Size of High Correlation with Target table: {high_corr_with_target_df.shape}\n")

    print("\nModerate Correlation with Target (0.3 < |correlation| ≤ 0.7)\n")
    display(HTML(moderate_corr_with_target_df.to_html(index=False)))
    print(f"Size of Moderate Correlation with Target table: {moderate_corr_with_target_df.shape}\n")

    print("\nLow Correlation with Target (|correlation| ≤ 0.3)\n")
    display(HTML(low_corr_with_target_df.to_html(index=False)))
    print(f"Size of Low Correlation with Target table: {low_corr_with_target_df.shape}\n")

    print("\nHigh Missing Values Correlation with Target (|correlation| > 0.7)\n")
    display(HTML(high_missing_corr_with_target_df.to_html(index=False)))
    print(f"Size of High Missing Values Correlation with Target table: {high_missing_corr_with_target_df.shape}\n")

    print("\nModerate Missing Values Correlation with Target (0.3 < |correlation| ≤ 0.7)\n")
    display(HTML(moderate_missing_corr_with_target_df.to_html(index=False)))
    print(f"Size of Moderate Missing Values Correlation with Target table: {moderate_missing_corr_with_target_df.shape}\n")

    print("\nLow Missing Values Correlation with Target (|correlation| ≤ 0.3)\n")
    display(HTML(low_missing_corr_with_target_df.to_html(index=False)))
    print(f"Size of Low Missing Values Correlation with Target table: {low_missing_corr_with_target_df.shape}\n")


def transform_imputation_strategy_to_schema(imputation_df: pd.DataFrame) -> Dict[str, Dict]:
    """
    Transform the imputation strategy DataFrame into a nested dictionary structure consistent with the schema.
    
    Parameters:
    - imputation_df (pd.DataFrame): DataFrame containing imputation strategies.
    
    Returns:
    - dict: Nested dictionary structured according to the schema.
    """
    imputation_strategy_dict: Dict[str, Dict] = {}

    for _, row in imputation_df.iterrows():
        feature_name = row['Feature']
        imputation_type = row['Imputation Strategy']
        comments = row['Comments']

        imputation_strategy_dict.setdefault('features', {}).setdefault(feature_name, {}).setdefault(
            'feature_engineering', {}).setdefault('imputation', {})['type'] = imputation_type
        imputation_strategy_dict['features'][feature_name]['feature_engineering']['imputation']['comments'] = comments

    return imputation_strategy_dict



































# def impute_missing_values(data: pd.DataFrame, metadata: dict, features_to_drop: List[str], imputation_values: dict = None, paths: dict = None, is_train: bool = True) -> pd.DataFrame:
#     data = convert_data_types(data, metadata)
#     data.replace('nan', pd.NA, inplace=True)

#     # Import drop_features and label_missing_values locally to avoid circular import
#     from src.feature_engineering.feature_utils import drop_features, label_missing_values
    
#     for feature in tqdm(metadata['features'].keys(), desc="Imputing missing values"):
#         if feature not in features_to_drop:
#             imputation_strategy = extract_imputation_strategy(metadata, feature)
            
#             if is_train:
#                 if imputation_strategy == 'Mode':
#                     mode_value = data[feature].mode().dropna()
#                     imputation_values[feature] = mode_value[0] if not mode_value.empty else None
#                 elif imputation_strategy == 'Median':
#                     median_value = data[feature].median()
#                     imputation_values[feature] = median_value if pd.notna(median_value) else None
#                 elif imputation_strategy == 'Mean':
#                     mean_value = data[feature].mean()
#                     imputation_values[feature] = mean_value if pd.notna(mean_value) else None

#                 if imputation_values[feature] is not None:
#                     data[feature] = data[feature].fillna(imputation_values[feature])
#                     logging.info(f"Feature '{feature}' imputed with value: {imputation_values[feature]}")
#                 else:
#                     logging.warning(f"Imputation for feature '{feature}' resulted in NaN. No imputation applied.")
#             else:
#                 if feature in imputation_values:
#                     data[feature] = data[feature].fillna(imputation_values[feature])
#                     logging.info(f"Feature '{feature}' imputed in test set with value: {imputation_values[feature]}")
#                 else:
#                     logging.warning(f"No imputation value found for feature '{feature}' in test data. Filling with mode or constant.")
#                     data[feature] = data[feature].fillna(data[feature].mode()[0] if not data[feature].mode().empty else 0)
            
#             missing_indicator = f"{feature}_is_missing"
#             if missing_indicator in data.columns:
#                 if is_train:
#                     imputation_values[missing_indicator] = data[missing_indicator].mode()[0]
#                 else:
#                     data[missing_indicator] = data[missing_indicator].fillna(imputation_values.get(missing_indicator, 0))
                    
#     # Convert numpy types to Python native types before saving
#     imputation_values_converted = {k: convert_numpy_types(v) for k, v in imputation_values.items()}
    
#     # Optionally save imputation values to a file for further inspection
#     imputation_values_path = os.path.join(paths['reports']['analysis_results'], 'imputation_values.json')
#     with open(imputation_values_path, 'w') as f:
#         json.dump(imputation_values_converted, f, indent=4)
#     logging.info(f"Imputation values saved to {imputation_values_path}")
    
#     return data





# def label_missing_values(data: pd.DataFrame, metadata: dict, features_to_drop: List[str]) -> Tuple[pd.DataFrame, List[str]]:
#     essential_missing_indicators = []
    
#     for feature in tqdm(metadata['features'].keys(), desc="Labeling missing values"):
#         if feature not in features_to_drop:
#             missing_percentage = extract_missing_percentage(metadata, feature)
#             if should_label_missing(missing_percentage):
#                 missing_indicator = f'{feature}_is_missing'
                
#                 # Calculate the missing indicator
#                 data[missing_indicator] = data[feature].isnull().astype(int)
#                 essential_missing_indicators.append(missing_indicator)
                
#                 # Log the number of missing values for this feature
#                 missing_count = data[missing_indicator].sum()
#                 logging.info(f"{missing_count} missing values detected for feature '{feature}'")
                
#                 # Validate that the missing indicator was calculated correctly
#                 assert missing_count == data[feature].isnull().sum(), (
#                     f"Mismatch in missing value count for feature '{feature}'."
#                 )
                
#     return data, essential_missing_indicators



# def should_label_missing(missing_percentage: float, threshold: float = 5.0) -> bool:
#     """
#     Determine whether missing values for a feature should be labeled based on the threshold.
    
#     Parameters:
#     missing_percentage (float): The missing percentage for the feature.
#     threshold (float): The threshold percentage to decide if labeling is needed. Default is 5.0.

#     Returns:
#     bool: True if missing values should be labeled, False otherwise.
#     """
#     return missing_percentage is not None and missing_percentage > threshold



# def drop_features(data: pd.DataFrame, metadata: dict) -> Tuple[pd.DataFrame, List[str]]:
#     """
#     Identify and drop features flagged for dropping.

#     Parameters:
#     - data (pd.DataFrame): The DataFrame from which to drop features.
#     - metadata (dict): Metadata containing feature information.

#     Returns:
#     - Tuple[pd.DataFrame, List[str]]: A tuple containing the DataFrame with features dropped and a list of dropped features.
#     """
#     features_to_drop = [
#         feature for feature in metadata['features'].keys() 
#         if extract_imputation_strategy(metadata, feature) == 'Drop'
#     ]
#     data.drop(columns=features_to_drop, inplace=True)
#     return data, features_to_drop



# def label_missing_values(data: pd.DataFrame, metadata: dict, features_to_drop: List[str]) -> Tuple[pd.DataFrame, List[str]]:
#     """
#     Label missing values for reference in future analysis.

#     Parameters:
#     - data (pd.DataFrame): The DataFrame to label missing values.
#     - metadata (dict): Metadata containing feature information.
#     - features_to_drop (List[str]): List of features to drop.

#     Returns:
#     - Tuple[pd.DataFrame, List[str]]: DataFrame with missing values labeled and list of essential missing value indicators.
#     """
#     essential_missing_indicators = []
#     for feature in tqdm(metadata['features'].keys(), desc="Labeling missing values"):
#         if feature not in features_to_drop:
#             missing_percentage = extract_missing_percentage(metadata, feature)
#             if should_label_missing(missing_percentage):
#                 data[f'{feature}_is_missing'] = data[feature].isnull().astype(int)
#                 essential_missing_indicators.append(f'{feature}_is_missing')
#     return data, essential_missing_indicators


# def impute_missing_values(data: pd.DataFrame, metadata: dict, features_to_drop: List[str], imputation_values: dict = None, is_train: bool = True) -> pd.DataFrame:
#     """
#     Impute missing values according to the imputation strategy, after data type conversion.
    
#     Parameters:
#     - data (pd.DataFrame): The DataFrame to impute missing values.
#     - metadata (dict): Metadata containing feature information.
#     - features_to_drop (List[str]): List of features to drop.
#     - imputation_values (dict): A dictionary to store or retrieve imputation values.
#     - is_train (bool): Indicates if the data is training data. Default is True.
    
#     Returns:
#     - pd.DataFrame: DataFrame with missing values imputed.
#     """
#     # Step 1: Convert data types first
#     data = convert_data_types(data, metadata)
#     data.replace('nan', pd.NA, inplace=True)
    
#     # Step 2: Impute missing values after conversion
#     for feature in tqdm(metadata['features'].keys(), desc="Imputing missing values"):
#         if feature not in features_to_drop:
#             imputation_strategy = extract_imputation_strategy(metadata, feature)
            
#             # Handle the imputation of the main feature
#             if is_train:
#                 if imputation_strategy == 'Mode':
#                     mode_value = data[feature].mode().dropna()
#                     imputation_values[feature] = mode_value[0] if not mode_value.empty else None
#                 elif imputation_strategy == 'Median':
#                     median_value = data[feature].median()
#                     imputation_values[feature] = median_value if pd.notna(median_value) else None
#                 elif imputation_strategy == 'Mean':
#                     mean_value = data[feature].mean()
#                     imputation_values[feature] = mean_value if pd.notna(mean_value) else None
                
#                 if imputation_values[feature] is not None:
#                     data[feature] = data[feature].fillna(imputation_values[feature])  # Fills `<NA>` with the imputation value
#                 else:
#                     logging.warning(f"Imputation for feature '{feature}' resulted in NaN. No imputation applied.")
#             else:
#                 # For test data, apply the same imputation strategy as the train data
#                 if feature in imputation_values:
#                     data[feature] = data[feature].fillna(imputation_values[feature])
#                 else:
#                     logging.warning(f"No imputation value found for feature '{feature}' in test data. Filling with mode or constant.")
#                     # Apply a default strategy if no value is found (e.g., mode or a constant)
#                     data[feature] = data[feature].fillna(data[feature].mode()[0] if not data[feature].mode().empty else 0)
            
#             # Handle the imputation of the corresponding `_is_missing` feature if it exists
#             missing_indicator = f"{feature}_is_missing"
#             if missing_indicator in data.columns:
#                 if is_train:
#                     imputation_values[missing_indicator] = data[missing_indicator].mode()[0]
#                 else:
#                     data[missing_indicator] = data[missing_indicator].fillna(imputation_values.get(missing_indicator, 0))
    
#     return data






# def impute_missing_values(data: pd.DataFrame, metadata: dict, features_to_drop: List[str], imputation_values: dict = None, is_train: bool = True) -> pd.DataFrame:
#     """
#     Impute missing values according to the imputation strategy, after data type conversion.
    
#     Parameters:
#     - data (pd.DataFrame): The DataFrame to impute missing values.
#     - metadata (dict): Metadata containing feature information.
#     - features_to_drop (List[str]): List of features to drop.
#     - imputation_values (dict): A dictionary to store or retrieve imputation values.
#     - is_train (bool): Indicates if the data is training data. Default is True.
    
#     Returns:
#     - pd.DataFrame: DataFrame with missing values imputed.
#     """
#     # Step 1: Convert data types first
#     data = convert_data_types(data, metadata)
    
#     # Step 2: Impute missing values after conversion
#     for feature in tqdm(metadata['features'].keys(), desc="Imputing missing values"):
#         if feature not in features_to_drop:
#             imputation_strategy = extract_imputation_strategy(metadata, feature)
            
#             # Handle the imputation of the main feature
#             if is_train:
#                 if imputation_strategy == 'Mode':
#                     mode_value = data[feature].mode().dropna()
#                     imputation_values[feature] = mode_value[0] if not mode_value.empty else None
#                 elif imputation_strategy == 'Median':
#                     median_value = data[feature].median()
#                     imputation_values[feature] = median_value if pd.notna(median_value) else None
#                 elif imputation_strategy == 'Mean':
#                     mean_value = data[feature].mean()
#                     imputation_values[feature] = mean_value if pd.notna(mean_value) else None
                
#                 if imputation_values[feature] is not None:
#                     data[feature] = data[feature].fillna(imputation_values[feature])  # Fills `<NA>` with the imputation value
#                 else:
#                     logging.warning(f"Imputation for feature '{feature}' resulted in NaN. No imputation applied.")
#             else:
#                 data[feature] = data[feature].fillna(imputation_values.get(feature))
            
#             # Handle the imputation of the corresponding `_is_missing` feature if it exists
#             missing_indicator = f"{feature}_is_missing"
#             if missing_indicator in data.columns:
#                 if is_train:
#                     imputation_values[missing_indicator] = data[missing_indicator].mode()[0]
#                 else:
#                     data[missing_indicator] = data[missing_indicator].fillna(imputation_values.get(missing_indicator, 0))
    
#     return data



# # src/feature_engineering/imputation.py

# def impute_missing_values(data: pd.DataFrame, metadata: dict, features_to_drop: List[str], imputation_values: dict = None, is_train: bool = True) -> pd.DataFrame:
#     """
#     Impute missing values according to the imputation strategy.
    
#     Parameters:
#     - data (pd.DataFrame): The DataFrame to impute missing values.
#     - metadata (dict): Metadata containing feature information.
#     - features_to_drop (List[str]): List of features to drop.
#     - imputation_values (dict): A dictionary to store or retrieve imputation values.
#     - is_train (bool): Indicates if the data is training data. Default is True.
    
#     Returns:
#     - pd.DataFrame: DataFrame with missing values imputed.
#     """
#     for feature in tqdm(metadata['features'].keys(), desc="Imputing missing values"):
#         if feature not in features_to_drop:
#             imputation_strategy = extract_imputation_strategy(metadata, feature)
#             # Handle the imputation of the main feature
#             if is_train:
#                 if imputation_strategy == 'Mode':
#                     mode_value = data[feature].mode()[0]
#                     imputation_values[feature] = mode_value
#                 elif imputation_strategy == 'Median':
#                     median_value = data[feature].median()
#                     imputation_values[feature] = median_value
#                 elif imputation_strategy == 'Mean':
#                     mean_value = data[feature].mean()
#                     imputation_values[feature] = mean_value
                
#                 data[feature] = data[feature].fillna(imputation_values[feature])
#             else:
#                 data[feature] = data[feature].fillna(imputation_values.get(feature))
            
#             # Handle the imputation of the corresponding `_is_missing` feature if it exists
#             missing_indicator = f"{feature}_is_missing"
#             if missing_indicator in data.columns:
#                 if is_train:
#                     # Add missing indicators to the imputation values if needed
#                     imputation_values[missing_indicator] = data[missing_indicator].mode()[0]
#                 else:
#                     # Ensure the missing indicator is filled in test data
#                     data[missing_indicator] = data[missing_indicator].fillna(imputation_values.get(missing_indicator, 0))

#     return data





# src/feature_engineering/imputation.py

# def impute_missing_values(data: pd.DataFrame, metadata: dict, features_to_drop: List[str], imputation_values: dict = None, is_train: bool = True) -> pd.DataFrame:
#     """
#     Impute missing values according to the imputation strategy, after data type conversion.
    
#     Parameters:
#     - data (pd.DataFrame): The DataFrame to impute missing values.
#     - metadata (dict): Metadata containing feature information.
#     - features_to_drop (List[str]): List of features to drop.
#     - imputation_values (dict): A dictionary to store or retrieve imputation values.
#     - is_train (bool): Indicates if the data is training data. Default is True.
    
#     Returns:
#     - pd.DataFrame: DataFrame with missing values imputed.
#     """
#     # Step 1: Convert data types first
#     data = convert_data_types(data, metadata)
    
#     # Step 2: Impute missing values after conversion
#     for feature in tqdm(metadata['features'].keys(), desc="Imputing missing values"):
#         if feature not in features_to_drop:
#             imputation_strategy = extract_imputation_strategy(metadata, feature)
            
#             # Handle the imputation of the main feature
#             if is_train:
#                 if imputation_strategy == 'Mode':
#                     mode_value = data[feature].mode().dropna()
#                     imputation_values[feature] = mode_value[0] if not mode_value.empty else None
#                 elif imputation_strategy == 'Median':
#                     median_value = data[feature].median()
#                     imputation_values[feature] = median_value if pd.notna(median_value) else None
#                 elif imputation_strategy == 'Mean':
#                     mean_value = data[feature].mean()
#                     imputation_values[feature] = mean_value if pd.notna(mean_value) else None
                
#                 if imputation_values[feature] is not None:
#                     data[feature] = data[feature].fillna(imputation_values[feature])
#                 else:
#                     logging.warning(f"Imputation for feature '{feature}' resulted in NaN. No imputation applied.")
#             else:
#                 data[feature] = data[feature].fillna(imputation_values.get(feature))
            
#             # Handle the imputation of the corresponding `_is_missing` feature if it exists
#             missing_indicator = f"{feature}_is_missing"
#             if missing_indicator in data.columns:
#                 if is_train:
#                     imputation_values[missing_indicator] = data[missing_indicator].mode()[0]
#                 else:
#                     data[missing_indicator] = data[missing_indicator].fillna(imputation_values.get(missing_indicator, 0))
    
#     return data





