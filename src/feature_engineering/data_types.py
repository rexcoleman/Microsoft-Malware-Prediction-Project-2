# src/feature_engineering/data_types.py

import os
import sys
import pandas as pd
import logging
from IPython.display import display, HTML
from tqdm.notebook import tqdm
from src.utils.file_operations import save_dataframe_with_progress, save_json_file
from src.utils.json_pipeline import save_json_with_pipeline
from src.utils.metadata_operations import extract_classified_data_type
from src.feature_engineering.imputation import (drop_features, label_missing_values, 
                                                impute_missing_values, extract_imputation_data, 
                                                save_essential_missing_value_features)


def save_feature_data_types(data, output_path):
    """
    Save the feature names and updated data types to a JSON file.
    
    Parameters:
    - data (pd.DataFrame): The DataFrame with processed data.
    - output_path (str): Path to save the feature data types JSON file.
    """
    feature_data_types = [
        {'Feature': feature, 'Technical Data Type': str(data[feature].dtype)}
        for feature in data.columns
    ]
    save_json_file(feature_data_types, output_path)


def map_classified_to_technical(classified_type: str) -> str:
    """
    Map classified data types to technical data types.
    
    Parameters:
    classified_type (str): The classified data type (binary, numerical, categorical).
    
    Returns:
    str: The corresponding technical data type (int, float, object).
    """
    if classified_type == 'binary':
        return 'int64'  # Could also use 'bool' if preferred
    elif classified_type == 'numerical':
        return 'float64'  # Assume float for more flexibility
    elif classified_type == 'categorical':
        return 'object'
    else:
        return None


def extract_technical_data_type(metadata: dict, feature: str) -> str:
    """
    Extract the technical data type from the metadata for a given feature.
    
    Parameters:
    metadata (dict): The metadata dictionary.
    feature (str): The feature name to extract metadata for.

    Returns:
    str: The technical data type of the feature.
    """
    return metadata.get('features', {}).get(feature, {}).get('general_attributes', {}).get('technical_data_type', 'Unknown')



def convert_data_types(data, metadata):
    """
    Convert columns to their correct types based on classified data type in metadata.
    
    Parameters:
    - data (pd.DataFrame): The DataFrame with imputed values.
    - metadata (dict): Metadata containing feature information.

    Returns:
    - pd.DataFrame: DataFrame with converted data types.
    """
    for feature in tqdm(metadata['features'].keys(), desc="Converting data types"):
        if feature in data.columns:
            classified_data_type = extract_classified_data_type(metadata, feature)
            technical_data_type = map_classified_to_technical(classified_data_type) or extract_technical_data_type(metadata, feature)
            
            if technical_data_type:
                try:
                    if technical_data_type == 'int64':
                        data[feature] = pd.to_numeric(data[feature], errors='coerce').astype('int64')
                    elif technical_data_type == 'float64':
                        data[feature] = pd.to_numeric(data[feature], errors='coerce').astype('float64')
                    elif technical_data_type == 'object':
                        data[feature] = data[feature].astype(str)
                    elif technical_data_type == 'datetime':
                        data[feature] = pd.to_datetime(data[feature], errors='coerce')
                    logging.info(f"Converted feature '{feature}' to '{technical_data_type}'.")
                except Exception as e:
                    logging.error(f"Error converting feature '{feature}' to '{technical_data_type}': {e}")
            else:
                logging.warning(f"Technical data type not mapped for classified data type '{classified_data_type}' for feature '{feature}'")
        else:
            logging.warning(f"Feature '{feature}' not found in data.")
    return data


def validate_data_types(data, metadata):
    """
    Validate that data types of the DataFrame columns match the metadata specifications.

    Parameters:
    data (pd.DataFrame): The DataFrame to validate.
    metadata (dict): Metadata containing feature information.

    Returns:
    None
    """
    logging.info("Starting data type validation")
    validation_errors = []
    
    for feature in metadata['features'].keys():
        if feature in data.columns:
            expected_dtype = extract_technical_data_type(metadata, feature)
            actual_dtype = str(data[feature].dtype)
            
            if expected_dtype and expected_dtype != actual_dtype:
                error_message = f"Feature '{feature}': Expected dtype technical_data_type '{expected_dtype}', but got dtype '{actual_dtype}'"
                validation_errors.append(error_message)
                logging.error(error_message)
            else:
                logging.info(f"Feature '{feature}': Data type '{actual_dtype}' matches the expected technical_data_type '{expected_dtype}'.")
    
    if not validation_errors:
        logging.info("Data type validation passed with no errors.")
    else:
        logging.warning("Data type validation found discrepancies.")
        for error in validation_errors:
            logging.warning(error)

    logging.info("Data type validation completed.")


def process_data(train_data, metadata, paths):
    """
    Process the data by handling missing values and converting data types.
    Parameters:
    train_data (pd.DataFrame): Training data.
    metadata (dict): Metadata containing feature information.
    paths (dict): Dictionary containing file paths.
    Returns:
    pd.DataFrame: Processed training data.
    """
    # Drop features flagged for dropping
    train_data, features_to_drop = drop_features(train_data, metadata)

    # Extract imputation strategy information
    imputation_df = extract_imputation_data(metadata, features_to_drop)
    display(HTML(imputation_df.to_html(index=False)))

    # Label missing values
    train_data, essential_missing_indicators = label_missing_values(train_data, metadata, features_to_drop)

    # Create JSON for new features
    essential_missing_value_features = [{'Feature': feature, 'Technical Data Type': 'int64'} for feature in essential_missing_indicators]
    essential_missing_value_features_path = os.path.join(paths['reports']['analysis_results'], 'essential_missing_value_features.json')
    save_essential_missing_value_features(essential_missing_value_features, essential_missing_value_features_path)
    logging.info(f"Essential missing value features saved to {essential_missing_value_features_path}")

    # Impute missing values
    train_data = impute_missing_values(train_data, metadata, features_to_drop)

    # Convert data types
    train_data = convert_data_types(train_data, metadata)

    return train_data

def save_processed_data(train_data, metadata, paths):
    """
    Save the processed data to the specified path and validate data types.
    Parameters:
    train_data (pd.DataFrame): Processed training data.
    metadata (dict): Metadata containing feature information.
    paths (dict): Dictionary containing file paths.
    """
    processed_data_path = paths['data']['intermediate_clean_imputation']
    save_dataframe_with_progress(train_data, processed_data_path)
    logging.info(f"Processed data saved to {processed_data_path}")

    # Confirm the saved file
    if os.path.isfile(processed_data_path):
        logging.info(f"Processed data file {processed_data_path} exists.")
        saved_data = pd.read_csv(processed_data_path)
        logging.info(f"Processed data file {processed_data_path} loaded for validation.")
        
        # Ensure correct data types after loading
        saved_data = convert_data_types(saved_data, metadata)
        
        # Validate data types on the saved data
        validate_data_types(saved_data, metadata)
    else:
        logging.error(f"Processed data file {processed_data_path} does not exist.")


def save_feature_data_types(train_data, paths):
    """
    Save the feature data types to a JSON file using save_json_with_pipeline.
    
    Parameters:
    train_data (pd.DataFrame): Training data.
    paths (dict): Dictionary containing file paths.
    """
    # Define the output path for the feature data types JSON file
    feature_data_types_path = os.path.join(paths['reports']['analysis_results'], 'feature_data_types.json')
    
    # Structure the data according to the metadata schema
    feature_data_types = {
        "features": {
            feature: {
                "general_attributes": {
                    "technical_data_type": str(train_data[feature].dtype)
                }
            }
            for feature in train_data.columns
        }
    }
    
    # Save the structured feature data types using the save_json_with_pipeline function
    save_json_with_pipeline(feature_data_types, feature_data_types_path)
    logging.info(f"Feature data types saved to {feature_data_types_path}")





































# def convert_data_types(data, metadata):
#     """
#     Convert columns to their correct types based on classified data type in metadata.
    
#     Parameters:
#     - data (pd.DataFrame): The DataFrame with imputed values.
#     - metadata (dict): Metadata containing feature information.

#     Returns:
#     - pd.DataFrame: DataFrame with converted data types.
#     """
#     for feature, details in tqdm(metadata['features'].items(), desc="Converting data types"):
#         if feature in data.columns:
#             classified_data_type = details.get('classified_data_type', None)
#             technical_data_type = map_classified_to_technical(classified_data_type)
            
#             if technical_data_type:
#                 try:
#                     if technical_data_type == 'int64':
#                         data[feature] = pd.to_numeric(data[feature], errors='coerce').astype('int64')
#                     elif technical_data_type == 'float64':
#                         data[feature] = pd.to_numeric(data[feature], errors='coerce').astype('float64')
#                     elif technical_data_type == 'object':
#                         data[feature] = data[feature].astype(str)
#                     elif technical_data_type == 'datetime':
#                         data[feature] = pd.to_datetime(data[feature], errors='coerce')
#                     logging.info(f"Converted feature '{feature}' to '{technical_data_type}'.")
#                 except Exception as e:
#                     logging.error(f"Error converting feature '{feature}' to '{technical_data_type}': {e}")
#             else:
#                 logging.warning(f"Technical data type not mapped for classified data type '{classified_data_type}' for feature '{feature}'")
#         else:
#             logging.warning(f"Feature '{feature}' not found in data.")
#     return data


# def map_classified_to_technical(classified_type):
#     """
#     Map classified data types to technical data types.
    
#     Parameters:
#     classified_type (str): The classified data type (binary, numerical, categorical).
    
#     Returns:
#     str: The corresponding technical data type (int, float, object).
#     """
#     if classified_type == 'binary':
#         return 'int64'  # Could also use 'bool' if preferred
#     elif classified_type == 'numerical':
#         return 'float64'  # Assume float for more flexibility
#     elif classified_type == 'categorical':
#         return 'object'
#     else:
#         return None


# def validate_data_types(data, metadata):
#     """
#     Validate that data types of the DataFrame columns match the metadata specifications.

#     Parameters:
#     data (pd.DataFrame): The DataFrame to validate.
#     metadata (dict): Metadata containing feature information.

#     Returns:
#     None
#     """
#     logging.info("Starting data type validation")
#     validation_errors = []
    
#     for feature, details in metadata['features'].items():
#         if feature in data.columns:
#             expected_dtype = details.get('technical_data_type', None)
#             actual_dtype = str(data[feature].dtype)
            
#             if expected_dtype and expected_dtype != actual_dtype:
#                 error_message = f"Feature '{feature}': Expected dtype technical_data_type '{expected_dtype}', but got dtype '{actual_dtype}'"
#                 validation_errors.append(error_message)
#                 logging.error(error_message)
#             else:
#                 logging.info(f"Feature '{feature}': Data type dtype '{actual_dtype}' matches the expected technical_data_type type.")
    
#     if not validation_errors:
#         logging.info("Data type validation passed with no errors.")
#     else:
#         logging.warning("Data type validation found discrepancies.")
#         for error in validation_errors:
#             logging.warning(error)

#     logging.info("Data type validation completed.")


