# src/feature_engineering/data_types.py

import os
import sys
import pandas as pd
import logging
from typing import Dict
from IPython.display import display, HTML
from tqdm.notebook import tqdm
from src.utils.metadata_operations import extract_classified_data_type
from src.utils.json_pipeline import save_json_with_pipeline

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def get_type_mapping() -> dict:
    """
    Returns a dictionary that maps classified data types to their corresponding technical data types.

    Returns:
    dict: A dictionary mapping classified data types to technical data types.
    """
    return {
        'categorical': 'object',
        'numerical': 'float64',
        'binary': 'int64'
    }


def convert_data_types(df: pd.DataFrame, metadata: Dict[str, Dict[str, str]]) -> pd.DataFrame:
    """
    Convert the data types of DataFrame columns based on the classified data type in the metadata.
    Parameters:
    df (pd.DataFrame): The input DataFrame.
    metadata (Dict[str, Dict[str, str]]): The metadata dictionary containing the classified data type for each feature.
    Returns:
    pd.DataFrame: The DataFrame with converted data types.
    """
    type_mapping = get_type_mapping()

    try:
        for column in tqdm(df.columns, desc="Converting data types"):
            classified_data_type = extract_classified_data_type(metadata, column)
            if classified_data_type in type_mapping:
                target_dtype = type_mapping[classified_data_type]
                
                # Check for missing values in binary columns before conversion
                if classified_data_type == 'binary' and df[column].isnull().any():
                    target_dtype = 'float64'  # Temporarily convert to float to accommodate NaN
                
                try:
                    df[column] = df[column].astype(target_dtype)
                    logging.info(f"Converted '{column}' to {target_dtype}")
                except Exception as e:
                    logging.error(f"Error converting column '{column}' to {target_dtype}: {e}")
            else:
                logging.warning(f"Classified data type '{classified_data_type}' for column '{column}' not recognized.")
        
        # After imputation, convert any float columns that were temporarily converted from binary back to integer
        for column in df.columns:
            if classified_data_type == 'binary' and df[column].dtype == 'float64':
                try:
                    df[column] = df[column].astype('int64')
                    logging.info(f"Converted '{column}' back to int64 after imputation")
                except Exception as e:
                    logging.error(f"Error converting column '{column}' back to int64: {e}")
        
        return df
    except Exception as e:
        logging.error(f"Error converting data types: {e}")
        raise



# def convert_data_types(df: pd.DataFrame, metadata: Dict[str, Dict[str, str]]) -> pd.DataFrame:
#     """
#     Convert the data types of DataFrame columns based on the classified data type in the metadata.

#     Parameters:
#     df (pd.DataFrame): The input DataFrame.
#     metadata (Dict[str, Dict[str, str]]): The metadata dictionary containing the classified data type for each feature.

#     Returns:
#     pd.DataFrame: The DataFrame with converted data types.
#     """
#     type_mapping = get_type_mapping()

#     try:
#         for column in tqdm(df.columns, desc="Converting data types"):
#             classified_data_type = extract_classified_data_type(metadata, column)
#             if classified_data_type in type_mapping:
#                 target_dtype = type_mapping[classified_data_type]
#                 try:
#                     df[column] = df[column].astype(target_dtype)
#                     logging.info(f"Converted '{column}' to {target_dtype}")
#                 except Exception as e:
#                     logging.error(f"Error converting column '{column}' to {target_dtype}: {e}")
#             else:
#                 logging.warning(f"Classified data type '{classified_data_type}' for column '{column}' not recognized.")
#         return df
#     except Exception as e:
#         logging.error(f"Error converting data types: {e}")
#         raise


def create_feature_data_types_report(df: pd.DataFrame, save_dir: str) -> None:
    """
    Create a JSON report of the data types for each feature in the DataFrame.

    Parameters:
    df (pd.DataFrame): The input DataFrame.
    save_dir (str): The directory where the JSON report should be saved.
    """
    try:
        feature_data_types = {
            "features": {
                column: {
                    "general_attributes": {
                        "technical_data_type": str(df[column].dtype)
                    }
                }
                for column in df.columns
            }
        }

        save_path = os.path.join(save_dir, 'feature_data_types.json')
        save_json_with_pipeline(feature_data_types, save_path)
        logging.info(f"Feature data types report saved to {save_path}")
    except Exception as e:
        logging.error(f"Error creating feature data types report: {e}")
        raise

