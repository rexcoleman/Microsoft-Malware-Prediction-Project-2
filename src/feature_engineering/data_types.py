# src/feature_engineering/data_types.py

import os
import sys
import pandas as pd
import logging
from IPython.display import display, HTML
from tqdm.notebook import tqdm
from src.utils.file_operations import save_dataframe_with_progress, save_json_file
from src.utils.json_pipeline import save_json_with_pipeline
from src.utils.metadata_operations import extract_classified_data_type
from src.feature_engineering.imputation import (drop_features, label_missing_values, 
                                                impute_missing_values, extract_imputation_data, 
                                                save_essential_missing_value_features)


def map_classified_to_technical(classified_type: str) -> str:
    """
    Map classified data types to technical data types.
    
    Parameters:
    classified_type (str): The classified data type (binary, numerical, categorical).
    
    Returns:
    str: The corresponding technical data type (int, float, object).
    """
    if classified_type == 'binary':
        return 'int64'  # Could also use 'bool' if preferred
    elif classified_type == 'numerical':
        return 'float64'  # Assume float for more flexibility
    elif classified_type == 'categorical':
        return 'object'
    else:
        return None


def extract_technical_data_type(metadata: dict, feature: str) -> str:
    """
    Extract the technical data type from the metadata for a given feature.
    
    Parameters:
    metadata (dict): The metadata dictionary.
    feature (str): The feature name to extract metadata for.

    Returns:
    str: The technical data type of the feature.
    """
    return metadata.get('features', {}).get(feature, {}).get('general_attributes', {}).get('technical_data_type', 'Unknown')





def convert_data_types(data, metadata):
    for feature in tqdm(metadata['features'].keys(), desc="Converting data types"):
        if feature in data.columns:
            classified_data_type = extract_classified_data_type(metadata, feature)
            technical_data_type = map_classified_to_technical(classified_data_type) or extract_technical_data_type(metadata, feature)
            
            if technical_data_type:
                try:
                    if technical_data_type == 'int64':
                        data[feature] = pd.to_numeric(data[feature], errors='coerce').astype('int64')
                    elif technical_data_type == 'float64':
                        data[feature] = pd.to_numeric(data[feature], errors='coerce').astype('float64')
                    elif technical_data_type == 'object':
                        data[feature] = data[feature].astype(str)
                    elif technical_data_type == 'datetime':
                        data[feature] = pd.to_datetime(data[feature], errors='coerce')
                    logging.info(f"Converted feature '{feature}' to '{technical_data_type}'.")
                except Exception as e:
                    logging.error(f"Error converting feature '{feature}' to '{technical_data_type}': {e}")
            else:
                logging.warning(f"Technical data type not mapped for classified data type '{classified_data_type}' for feature '{feature}'")
        else:
            logging.warning(f"Feature '{feature}' not found in data.")
    return data

def validate_data_types(data, metadata):
    logging.info("Starting data type validation")
    validation_errors = []
    
    for feature in metadata['features'].keys():
        if feature in data.columns:
            expected_dtype = extract_technical_data_type(metadata, feature)
            actual_dtype = str(data[feature].dtype)
            
            if expected_dtype and expected_dtype != actual_dtype:
                error_message = f"Feature '{feature}': Expected dtype technical_data_type '{expected_dtype}', but got dtype '{actual_dtype}'"
                validation_errors.append(error_message)
                logging.error(error_message)
            else:
                logging.info(f"Feature '{feature}': Data type '{actual_dtype}' matches the expected technical_data_type '{expected_dtype}'.")
    
    if not validation_errors:
        logging.info("Data type validation passed with no errors.")
    else:
        logging.warning("Data type validation found discrepancies.")
        for error in validation_errors:
            logging.warning(error)

    logging.info("Data type validation completed.")



# src/feature_engineering/data_types.py

def process_train_data(train_data, metadata, paths):
    train_data, features_to_drop = drop_features(train_data, metadata)
    imputation_df = extract_imputation_data(metadata, features_to_drop)
    display(HTML(imputation_df.to_html(index=False)))

    # Label missing values and store missing indicators
    train_data, essential_missing_indicators = label_missing_values(train_data, metadata, features_to_drop)
    
    # Save missing value indicator features for later use
    essential_missing_value_features = {
        "features": {
            feature: {
                "general_attributes": {
                    "technical_data_type": "int64"
                }
            } for feature in essential_missing_indicators
        }
    }
    essential_missing_value_features_path = os.path.join(paths['reports']['analysis_results'], 'essential_missing_value_features.json')
    save_essential_missing_value_features(essential_missing_value_features, essential_missing_value_features_path)
    logging.info(f"Essential missing value features saved to {essential_missing_value_features_path}")

    # Store imputation values from train data
    imputation_values = {}
    train_data = impute_missing_values(train_data, metadata, features_to_drop, imputation_values=imputation_values, is_train=True)
    train_data = convert_data_types(train_data, metadata)

    # Return the train data, imputation values, and essential missing indicators
    return train_data, imputation_values, essential_missing_indicators


def process_test_data(test_data, metadata, paths, imputation_values, essential_missing_indicators):
    # Drop features flagged for dropping
    test_data, features_to_drop = drop_features(test_data, metadata)

    # Ensure missing value indicators are added to the test data
    for feature in essential_missing_indicators:
        if feature not in test_data.columns:
            test_data[feature] = 0  # Add the missing indicator column with all zeros if not present

    # Apply the stored imputation strategies from train data, excluding the target variable
    for feature in metadata['features'].keys():
        if feature != 'HasDetections' and feature in test_data.columns:
            test_data[feature] = test_data[feature].fillna(imputation_values.get(feature))
            # Handle the corresponding '_is_missing' feature
            missing_indicator = f"{feature}_is_missing"
            if missing_indicator in test_data.columns:
                test_data[missing_indicator] = test_data[missing_indicator].fillna(imputation_values.get(missing_indicator, 0))

    # Convert data types
    test_data = convert_data_types(test_data, metadata)

    return test_data


def save_processed_data(data, metadata, paths, data_type='train'):
    if data_type == 'train':
        processed_data_path = os.path.join(paths['data']['intermediate'], '01_clean_imputation_and_data_types_train.csv')
    else:
        processed_data_path = os.path.join(paths['data']['intermediate'], '01_clean_imputation_and_data_types_test.csv')
    
    save_dataframe_with_progress(data, processed_data_path)
    logging.info(f"Processed {data_type} data saved to {processed_data_path}")

    if os.path.isfile(processed_data_path):
        logging.info(f"Processed {data_type} data file {processed_data_path} exists.")
        saved_data = pd.read_csv(processed_data_path)
        logging.info(f"Processed {data_type} data file {processed_data_path} loaded for validation.")
        
        saved_data = convert_data_types(saved_data, metadata)
        validate_data_types(saved_data, metadata)
    else:
        logging.error(f"Processed {data_type} data file {processed_data_path} does not exist.")


def save_feature_data_types(train_data, paths):
    feature_data_types_path = os.path.join(paths['reports']['analysis_results'], 'feature_data_types.json')
    feature_data_types = {
        "features": {
            feature: {
                "general_attributes": {
                    "technical_data_type": str(train_data[feature].dtype)
                }
            }
            for feature in train_data.columns if not feature.endswith('_is_missing')
        }
    }
    save_json_with_pipeline(feature_data_types, feature_data_types_path)
    logging.info(f"Feature data types saved to {feature_data_types_path}")







