# src/analysis/data_understanding.py

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import logging
import os
from src.config_loader import load_paths, load_config, update_imputation_strategies
from src.utils.json_utils import save_to_json
from src.utils.common import save_analysis_results, ensure_directories_exist, load_json_file, save_json_file

logger = logging.getLogger(__name__)

def data_overview(train_sample, test_sample, save_dir=None):
    overview = {
        "train_shape": train_sample.shape,
        "test_shape": test_sample.shape,
        "train_columns": list(train_sample.columns),
        "test_columns": list(test_sample.columns)
    }
    if save_dir:
        save_to_json(overview, os.path.join(save_dir, 'data_overview.json'))
    print("\n==== Data Overview ====\n")
    print("Data Loaded: Train Sample Shape:", train_sample.shape, "Test Sample Shape:", test_sample.shape)
    print(f"Number of rows in train sample: {train_sample.shape[0]}")
    print(f"Number of columns in train sample: {train_sample.shape[1]}")
    print(f"Column names in train sample: {list(train_sample.columns)}")
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: The dataset consists of 10,000 rows and 83 columns for the train sample, and 82 columns for the test sample.")
    print("Detailed Technical Insights: The dataset contains a variety of features including numerical, categorical, and binary data.")
    print("Actionable Recommendations: Proceed with initial analysis and understanding of the dataset's structure.")

def data_types_and_formats(dataframe, save_dir=None):
    data_types = dataframe.dtypes.apply(lambda x: x.name).to_dict()
    if save_dir:
        save_to_json(data_types, os.path.join(save_dir, 'data_types.json'))
    print("\n==== Data Types and Formats ====\n")
    print(dataframe.dtypes)
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: The dataset includes multiple data types such as integers, floats, and objects.")
    print("Detailed Technical Insights: Ensuring data types are correctly assigned is crucial for accurate analysis and modeling.")
    print("Actionable Recommendations: Verify and correct data types if necessary before further analysis.")

def statistical_summary(dataframe, save_dir=None):
    summary = dataframe.describe(include='all').to_dict()
    if save_dir:
        save_to_json(summary, os.path.join(save_dir, 'stat_summary.json'))
    print("\n==== Statistical Summary ====\n")
    print(dataframe.describe(include='all'))
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Descriptive statistics provide an overview of the central tendency, dispersion, and shape of the dataset's distribution.")
    print("Detailed Technical Insights: Key statistics include mean, standard deviation, and percentiles for numerical features; frequency counts for categorical features.")
    print("Actionable Recommendations: Use these statistics to identify any anomalies or outliers in the dataset.")

def target_variable_analysis(dataframe, save_dir=None):
    target_summary = dataframe['HasDetections'].value_counts(normalize=True).to_dict()
    if save_dir:
        save_to_json(target_summary, os.path.join(save_dir, 'target_analysis.json'))
    print("\n==== Target Variable Analysis: 'HasDetections' ====\n")
    plt.figure(figsize=(6, 4))
    ax = sns.countplot(x='HasDetections', data=dataframe)
    plt.title('Distribution of HasDetections')
    plt.xlabel('HasDetections Value')
    plt.ylabel('Count')
    total = len(dataframe['HasDetections'].dropna())
    for p in ax.patches:
        height = p.get_height()
        ax.text(p.get_x() + p.get_width() / 2., height + 3, '{:1.2f}%'.format(100 * height/total), ha="center") 
    plt.show()
    detection_counts = dataframe['HasDetections'].value_counts(normalize=True)
    print(f"Distribution of 'HasDetections':\n{detection_counts}")
    print("\n--- Contextual Insights and Recommendations ---\n")
    if detection_counts[1] > 0.6 or detection_counts[0] > 0.6:
        print("High-Level Overview: There is a class imbalance in the 'HasDetections' feature.")
        print("Detailed Technical Insights: The imbalance might lead to model bias favoring the majority class.")
        print("Actionable Recommendations: Consider using SMOTE, class weighting, or other resampling techniques to balance the classes.")
    else:
        print("High-Level Overview: The 'HasDetections' feature is relatively balanced.")
        print("Detailed Technical Insights: The balanced distribution is beneficial for model training as it reduces the risk of bias towards one class.")
        print("Actionable Recommendations: Proceed with model development without additional steps for handling class imbalance at this stage.")

def missing_values_analysis(dataframe, save_dir=None):
    missing_values = dataframe.isnull().sum().to_dict()
    missing_percentage = (dataframe.isnull().sum() / dataframe.shape[0] * 100).to_dict()
    missing_data = {feature: {'count': missing_values[feature], 'percentage': missing_percentage[feature]} for feature in missing_values}
    if save_dir:
        save_to_json(missing_data, os.path.join(save_dir, 'missing_values.json'))
    print("\n==== Missing Values Analysis ====\n")
    missing_data_sorted = pd.DataFrame(missing_data).T.sort_values(by='count', ascending=False)
    plt.figure(figsize=(12, 6))
    sns.barplot(x=missing_data_sorted.index, y=missing_data_sorted['count'])
    plt.xticks(rotation=90)
    plt.title('Missing Values per Column')
    plt.ylabel('Number of Missing Values')
    plt.xlabel('Feature')
    plt.show()
    print("Missing Values in Train Sample:")
    print(missing_data_sorted[missing_data_sorted['count'] > 0])
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Several features have a high percentage of missing values, particularly 'PuaMode' and 'Census_ProcessorClass'.")
    print("Detailed Technical Insights: Features with a high percentage of missing values might need imputation strategies or could be excluded from the analysis if deemed irrelevant.")
    print("Actionable Recommendations: Develop a missing value treatment plan, focusing on imputation strategies or feature exclusion based on the importance and impact on the target variable.")

def feature_balance_analysis(dataframe, save_dir=None):
    feature_balance = {col: {'most_common_value_weight': dataframe[col].value_counts(normalize=True).max() * 100} for col in dataframe.columns}
    if save_dir:
        save_to_json(feature_balance, os.path.join(save_dir, 'feature_balance.json'))
    print("\n==== Feature Balance Analysis ====\n")
    feature_balance_sorted = pd.Series({col: bal['most_common_value_weight'] for col, bal in feature_balance.items()}).sort_values(ascending=False)
    plt.figure(figsize=(12, 6))
    sns.barplot(x=feature_balance_sorted.index, y=feature_balance_sorted.values)
    plt.xticks(rotation=90)
    plt.title('Feature Balance (Percentage of Highest Value)')
    plt.ylabel('Percentage')
    plt.xlabel('Feature')
    plt.show()
    print("Feature Balance in Train Sample:")
    print(feature_balance_sorted)
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Many features show a high percentage of a single value, indicating potential imbalance.")
    print("Detailed Technical Insights: Imbalanced features can impact the performance of machine learning models, particularly in classification tasks.")
    print("Actionable Recommendations: Consider techniques to handle imbalanced features, such as resampling, feature engineering, or model adjustments.")
    
def correlation_analysis(dataframe, save_dir=None, pair_threshold=0.2):
    numerical_columns = dataframe.select_dtypes(include=['int64', 'float64']).columns
    correlation_matrix = dataframe[numerical_columns].corr()
    high_correlation_pairs = correlation_matrix.unstack().sort_values(kind="quicksort", ascending=False).drop_duplicates()
    high_correlation_pairs = high_correlation_pairs[(high_correlation_pairs.abs() > pair_threshold) & (high_correlation_pairs != 1)]
    high_correlation_dict = {str(k): {'correlation': v} for k, v in high_correlation_pairs.to_dict().items()}

    if save_dir:
        save_to_json(correlation_matrix.to_dict(), os.path.join(save_dir, 'correlations.json'))
        save_to_json(high_correlation_dict, os.path.join(save_dir, 'high_correlation_pairs.json'))

    print("\n==== Correlation Analysis ====\n")
    plt.figure(figsize=(15, 10))
    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')
    plt.title('Correlation Matrix')
    plt.show()
    print("\nHigh Correlation Pairs (absolute value > {} or < -{}):\n".format(pair_threshold, pair_threshold))
    print(high_correlation_pairs)
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Several feature pairs show high correlation (absolute value > {} or < -{}).".format(pair_threshold, pair_threshold))
    print("Detailed Technical Insights: Highly correlated features can indicate multicollinearity, which may affect model performance.")
    print("Actionable Recommendations: Consider removing or combining highly correlated features to reduce redundancy and multicollinearity.")

    # Correlation with target variable
    target_corr = correlation_matrix['HasDetections'].drop('HasDetections').sort_values(ascending=False)
    target_corr_dict = target_corr.to_dict()
    
    if save_dir:
        save_to_json(target_corr_dict, os.path.join(save_dir, 'target_correlations.json'))

    print("\n==== Correlation with Target Variable 'HasDetections' ====\n")
    plt.figure(figsize=(12, 8))
    sns.barplot(x=target_corr.values, y=target_corr.index, hue=target_corr.index, dodge=False, palette='coolwarm', legend=False)
    plt.title("Correlation with 'HasDetections'")
    plt.xlabel("Correlation coefficient")
    plt.ylabel("Features")
    plt.show()
    print("\nCorrelation with 'HasDetections':\n", target_corr)
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Some features show strong correlation with the target variable 'HasDetections'.")
    print("Detailed Technical Insights: Identifying these features can help in feature selection and engineering.")
    print("Actionable Recommendations: Consider these features for model development and further analysis.")

    return high_correlation_pairs, target_corr
    
def feature_interactions(dataframe, save_dir=None):
    # Placeholder for future implementation
    if save_dir:
        save_to_json({}, os.path.join(save_dir, 'feature_interactions.json'))
    print("\n==== Feature Interactions ====\n")
    sns.pairplot(dataframe, vars=[
        'Census_OSInstallLanguageIdentifier', 'Census_OSUILocaleIdentifier', 
        'Census_OSBuildNumber', 'OsBuild', 'IsSxsPassiveMode', 'RtpStateBitfield', 
        'Census_InternalPrimaryDisplayResolutionHorizontal', 
        'Census_InternalPrimaryDisplayResolutionVertical'
    ])
    plt.show()
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: The pair plots visualize relationships between highly correlated features.")
    print("Detailed Technical Insights: Observing feature interactions can provide insights into underlying patterns and relationships in the data.")
    print("Actionable Recommendations: Use these insights for feature engineering or model selection, leveraging relationships between features.")

def feature_classification(dataframe, save_dir=None):
    binary_features_manual = [col for col in dataframe.columns if dataframe[col].nunique() == 2]
    categorical_features_manual = [col for col in dataframe.columns if dataframe[col].dtype == 'object' and col not in binary_features_manual]
    numerical_features_manual = [col for col in dataframe.columns if dataframe[col].dtype in ['int64', 'float64'] and col not in binary_features_manual]
    
    binary_features_auto = [col for col in dataframe.columns if dataframe[col].nunique() == 2]
    categorical_features_auto = [col for col in dataframe.columns if dataframe[col].dtype == 'object' and col not in binary_features_auto]
    numerical_features_auto = [col for col in dataframe.columns if dataframe[col].dtype in ['int64', 'float64'] and col not in binary_features_auto]
    
    mismatched_binary = set(binary_features_manual).symmetric_difference(binary_features_auto)
    mismatched_categorical = set(categorical_features_manual).symmetric_difference(categorical_features_auto)
    mismatched_numerical = set(numerical_features_manual).symmetric_difference(numerical_features_auto)
    
    # Save classification results to JSON
    classification_results = {
        'binary_features_auto': binary_features_auto,
        'categorical_features_auto': categorical_features_auto,
        'numerical_features_auto': numerical_features_auto,
        'mismatched_binary': list(mismatched_binary),
        'mismatched_categorical': list(mismatched_categorical),
        'mismatched_numerical': list(mismatched_numerical)
    }
    if save_dir:
        save_to_json(classification_results, os.path.join(save_dir, 'feature_classification.json'))
    
    # Print summary table
    summary_table = pd.DataFrame({
        'Feature Type': ['Binary', 'Categorical', 'Numerical'],
        'Manual Count': [len(binary_features_manual), len(categorical_features_manual), len(numerical_features_manual)],
        'Automated Count': [len(binary_features_auto), len(categorical_features_auto), len(numerical_features_auto)]
    })
    
    print("\n==== Feature Classification ====\n")
    print(summary_table)
    
    print("\n--- Feature Classification Debugging ---\n")
    print(f"Binary Features (Manual): {binary_features_manual}")
    print(f"Binary Features (Automated): {binary_features_auto}")
    print(f"Mismatched Binary Features: {mismatched_binary}")
    
    print(f"Categorical Features (Manual): {categorical_features_manual}")
    print(f"Categorical Features (Automated): {categorical_features_auto}")
    print(f"Mismatched Categorical Features: {mismatched_categorical}")
    
    print(f"Numerical Features (Manual): {numerical_features_manual}")
    print(f"Numerical Features (Automated): {numerical_features_auto}")
    print(f"Mismatched Numerical Features: {mismatched_numerical}")
    
    print("\n--- Further Inspection of Mismatched Features ---\n")
    if mismatched_binary:
        print(f"Mismatched Binary Features: {mismatched_binary}")
        for feature in mismatched_binary:
            print(f"Feature: {feature}")
            print(f"Unique Values: {dataframe[feature].unique()}")
            print(f"Data Type: {dataframe[feature].dtype}")
            print("\n")
    
    if mismatched_categorical:
        print(f"Mismatched Categorical Features: {mismatched_categorical}")
        for feature in mismatched_categorical:
            print(f"Feature: {feature}")
            print(f"Unique Values: {dataframe[feature].unique()}")
            print(f"Data Type: {dataframe[feature].dtype}")
            print("\n")
    
    if mismatched_numerical:
        print(f"Mismatched Numerical Features: {mismatched_numerical}")
        for feature in mismatched_numerical:
            print(f"Feature: {feature}")
            print(f"Unique Values: {dataframe[feature].unique()}")
            print(f"Data Type: {dataframe[feature].dtype}")
            print("\n")
    
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Feature classification is validated by comparing manual and automated classifications.")
    print("Detailed Technical Insights: Mismatched features indicate discrepancies in classification which need further investigation.")
    print("Actionable Recommendations: Investigate and correct mismatched features to ensure accurate classification.")
    
    return binary_features_auto, categorical_features_auto, numerical_features_auto


