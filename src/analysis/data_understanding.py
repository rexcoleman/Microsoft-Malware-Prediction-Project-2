# src/analysis/data_understanding.py

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import logging
from src.config_loader import load_paths, load_config, update_imputation_strategies
from src.utils import save_analysis_results, ensure_directories_exist
from src.visualization.visualize import plot_correlation_heatmap

logger = logging.getLogger(__name__)

def data_overview(train_sample, test_sample):
    print("\n==== Data Overview ====\n")
    print("Data Loaded: Train Sample Shape:", train_sample.shape, "Test Sample Shape:", test_sample.shape)
    print(f"Number of rows in train sample: {train_sample.shape[0]}")
    print(f"Number of columns in train sample: {train_sample.shape[1]}")
    print(f"Column names in train sample: {list(train_sample.columns)}")
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: The dataset consists of 10,000 rows and 83 columns for the train sample, and 82 columns for the test sample.")
    print("Detailed Technical Insights: The dataset contains a variety of features including numerical, categorical, and binary data.")
    print("Actionable Recommendations: Proceed with initial analysis and understanding of the dataset's structure.")

def data_types_and_formats(dataframe):
    print("\n==== Data Types and Formats ====\n")
    print(dataframe.dtypes)
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: The dataset includes multiple data types such as integers, floats, and objects.")
    print("Detailed Technical Insights: Ensuring data types are correctly assigned is crucial for accurate analysis and modeling.")
    print("Actionable Recommendations: Verify and correct data types if necessary before further analysis.")

def statistical_summary(dataframe):
    print("\n==== Statistical Summary ====\n")
    print(dataframe.describe(include='all'))
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Descriptive statistics provide an overview of the central tendency, dispersion, and shape of the dataset's distribution.")
    print("Detailed Technical Insights: Key statistics include mean, standard deviation, and percentiles for numerical features; frequency counts for categorical features.")
    print("Actionable Recommendations: Use these statistics to identify any anomalies or outliers in the dataset.")

def target_variable_analysis(dataframe):
    print("\n==== Target Variable Analysis: 'HasDetections' ====\n")
    plt.figure(figsize=(6, 4))
    ax = sns.countplot(x='HasDetections', data=dataframe)
    plt.title('Distribution of HasDetections')
    plt.xlabel('HasDetections Value')
    plt.ylabel('Count')
    total = len(dataframe['HasDetections'].dropna())
    for p in ax.patches:
        height = p.get_height()
        ax.text(p.get_x() + p.get_width() / 2., height + 3, '{:1.2f}%'.format(100 * height/total), ha="center") 
    plt.show()
    detection_counts = dataframe['HasDetections'].value_counts(normalize=True)
    print(f"Distribution of 'HasDetections':\n{detection_counts}")
    print("\n--- Contextual Insights and Recommendations ---\n")
    if detection_counts[1] > 0.6 or detection_counts[0] > 0.6:
        print("High-Level Overview: There is a class imbalance in the 'HasDetections' feature.")
        print("Detailed Technical Insights: The imbalance might lead to model bias favoring the majority class.")
        print("Actionable Recommendations: Consider using SMOTE, class weighting, or other resampling techniques to balance the classes.")
    else:
        print("High-Level Overview: The 'HasDetections' feature is relatively balanced.")
        print("Detailed Technical Insights: The balanced distribution is beneficial for model training as it reduces the risk of bias towards one class.")
        print("Actionable Recommendations: Proceed with model development without additional steps for handling class imbalance at this stage.")

def missing_values_analysis(dataframe):
    print("\n==== Missing Values Analysis ====\n")
    missing_values = dataframe.isnull().sum()
    missing_percentage = (missing_values / dataframe.shape[0]) * 100
    missing_data = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})
    missing_data_sorted = missing_data.sort_values(by='Missing Values', ascending=False)
    plt.figure(figsize=(12, 6))
    sns.barplot(x=missing_data_sorted.index, y=missing_data_sorted['Missing Values'])
    plt.xticks(rotation=90)
    plt.title('Missing Values per Column')
    plt.ylabel('Number of Missing Values')
    plt.xlabel('Feature')
    plt.show()
    print("Missing Values in Train Sample:")
    print(missing_data_sorted[missing_data_sorted['Missing Values'] > 0])
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Several features have a high percentage of missing values, particularly 'PuaMode' and 'Census_ProcessorClass'.")
    print("Detailed Technical Insights: Features with a high percentage of missing values might need imputation strategies or could be excluded from the analysis if deemed irrelevant.")
    print("Actionable Recommendations: Develop a missing value treatment plan, focusing on imputation strategies or feature exclusion based on the importance and impact on the target variable.")

def feature_balance_analysis(dataframe):
    print("\n==== Feature Balance Analysis ====\n")
    feature_balance = {}
    for col in dataframe.columns:
        value_counts = dataframe[col].value_counts(normalize=True).max()
        feature_balance[col] = value_counts * 100
    feature_balance = pd.Series(feature_balance).sort_values(ascending=False)
    plt.figure(figsize=(12, 6))
    sns.barplot(x=feature_balance.index, y=feature_balance.values)
    plt.xticks(rotation=90)
    plt.title('Feature Balance (Percentage of Highest Value)')
    plt.ylabel('Percentage')
    plt.xlabel('Feature')
    plt.show()
    print("Feature Balance in Train Sample:")
    print(feature_balance)
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Many features show a high percentage of a single value, indicating potential imbalance.")
    print("Detailed Technical Insights: Imbalanced features can impact the performance of machine learning models, particularly in classification tasks.")
    print("Actionable Recommendations: Consider techniques to handle imbalanced features, such as resampling, feature engineering, or model adjustments.")

def correlation_analysis(dataframe):
    print("\n==== Correlation Analysis ====\n")
    numerical_columns = dataframe.select_dtypes(include=['int64', 'float64']).columns
    correlation_matrix = dataframe[numerical_columns].corr()
    plt.figure(figsize=(15, 10))
    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')
    plt.title('Correlation Matrix')
    plt.show()
    high_correlation_pairs = correlation_matrix.abs().unstack().sort_values(kind="quicksort", ascending=False).drop_duplicates()
    print("\nHigh Correlation Pairs (absolute value > 0.8):\n")
    print(high_correlation_pairs[high_correlation_pairs > 0.8])
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Several feature pairs show high correlation (absolute value > 0.8).")
    print("Detailed Technical Insights: Highly correlated features can indicate multicollinearity, which may affect model performance.")
    print("Actionable Recommendations: Consider removing or combining highly correlated features to reduce redundancy and multicollinearity.")

def feature_interactions(dataframe):
    print("\n==== Feature Interactions ====\n")
    sns.pairplot(dataframe, vars=[
        'Census_OSInstallLanguageIdentifier', 'Census_OSUILocaleIdentifier', 
        'Census_OSBuildNumber', 'OsBuild', 'IsSxsPassiveMode', 'RtpStateBitfield', 
        'Census_InternalPrimaryDisplayResolutionHorizontal', 
        'Census_InternalPrimaryDisplayResolutionVertical'
    ])
    plt.show()
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: The pair plots visualize relationships between highly correlated features.")
    print("Detailed Technical Insights: Observing feature interactions can provide insights into underlying patterns and relationships in the data.")
    print("Actionable Recommendations: Use these insights for feature engineering or model selection, leveraging relationships between features.")


def feature_classification(dataframe):
    print("\n==== Feature Classification ====\n")
    binary_features_manual = [col for col in dataframe.columns if dataframe[col].nunique() == 2]
    categorical_features_manual = [col for col in dataframe.columns if dataframe[col].dtype == 'object' and col not in binary_features_manual]
    numerical_features_manual = [col for col in dataframe.columns if dataframe[col].dtype in ['int64', 'float64'] and col not in binary_features_manual]
    
    binary_features_auto = [col for col in dataframe.columns if dataframe[col].nunique() == 2]
    categorical_features_auto = [col for col in dataframe.columns if dataframe[col].dtype == 'object' and col not in binary_features_auto]
    numerical_features_auto = [col for col in dataframe.columns if dataframe[col].dtype in ['int64', 'float64'] and col not in binary_features_auto]
    
    mismatched_binary = set(binary_features_manual).symmetric_difference(binary_features_auto)
    mismatched_categorical = set(categorical_features_manual).symmetric_difference(categorical_features_auto)
    mismatched_numerical = set(numerical_features_manual).symmetric_difference(numerical_features_auto)
    
    summary_table = pd.DataFrame({
        'Feature Type': ['Binary', 'Categorical', 'Numerical'],
        'Manual Count': [len(binary_features_manual), len(categorical_features_manual), len(numerical_features_manual)],
        'Automated Count': [len(binary_features_auto), len(categorical_features_auto), len(numerical_features_auto)]
    })
    
    print(summary_table)
    
    print("\n--- Feature Classification Debugging ---\n")
    print(f"Binary Features (Manual): {binary_features_manual}")
    print(f"Binary Features (Automated): {binary_features_auto}")
    print(f"Mismatched Binary Features: {mismatched_binary}")
    
    print(f"Categorical Features (Manual): {categorical_features_manual}")
    print(f"Categorical Features (Automated): {categorical_features_auto}")
    print(f"Mismatched Categorical Features: {mismatched_categorical}")
    
    print(f"Numerical Features (Manual): {numerical_features_manual}")
    print(f"Numerical Features (Automated): {numerical_features_auto}")
    print(f"Mismatched Numerical Features: {mismatched_numerical}")
    
    print("\n--- Further Inspection of Mismatched Features ---\n")
    if mismatched_binary:
        print(f"Mismatched Binary Features: {mismatched_binary}")
        for feature in mismatched_binary:
            print(f"Feature: {feature}")
            print(f"Unique Values: {dataframe[feature].unique()}")
            print(f"Data Type: {dataframe[feature].dtype}")
            print("\n")
    
    if mismatched_categorical:
        print(f"Mismatched Categorical Features: {mismatched_categorical}")
        for feature in mismatched_categorical:
            print(f"Feature: {feature}")
            print(f"Unique Values: {dataframe[feature].unique()}")
            print(f"Data Type: {dataframe[feature].dtype}")
            print("\n")
    
    if mismatched_numerical:
        print(f"Mismatched Numerical Features: {mismatched_numerical}")
        for feature in mismatched_numerical:
            print(f"Feature: {feature}")
            print(f"Unique Values: {dataframe[feature].unique()}")
            print(f"Data Type: {dataframe[feature].dtype}")
            print("\n")
    
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Feature classification is validated by comparing manual and automated classifications.")
    print("Detailed Technical Insights: Mismatched features indicate discrepancies in classification which need further investigation.")
    print("Actionable Recommendations: Investigate and correct mismatched features to ensure accurate classification.")
    






# def interpret_binary_feature(feature, data):
#     print(f"\n--- Analysis for Binary Feature: {feature} ---\n")
#     if data[feature].dropna().empty:
#         print(f"No data available for feature {feature}.")
#         return
    
#     summary_stats = data[feature].describe()

#     # Plotting the feature with improved visualization
#     plt.figure(figsize=(6, 4))
#     ax = sns.countplot(x=data[feature])
#     plt.title(f'Count Plot of {feature}')
#     plt.xlabel(f'{feature} Value')
#     plt.ylabel('Count')
#     total = len(data[feature].dropna())
    
#     for p in ax.patches:
#         height = p.get_height()
#         ax.text(p.get_x() + p.get_width() / 2., height + 3, '{:1.2f}%'.format(100 * height/total), ha="center") 

#     plt.show()

#     if 'mean' in summary_stats:
#         balance = f"{summary_stats['mean']*100:.2f}% of instances being '1' and {100-summary_stats['mean']*100:.2f}% being '0'."
#         print(f"Summary Statistics for {feature}: {summary_stats}")
        
#         # --- Contextual Insights and Recommendations ---
#         print("\n--- Contextual Insights and Recommendations ---\n")
#         print(f"High-Level Overview: The feature '{feature}' is a binary variable with {balance}")
#         print("Detailed Technical Insights: The count plot indicates the balance of the binary classes.")
#         if abs(summary_stats['mean'] - 0.5) > 0.1:
#             print("Actionable Recommendations: Consider handling class imbalance if necessary, using techniques such as SMOTE, class weighting, or resampling.")
#         else:
#             print("Actionable Recommendations: No significant class imbalance detected. Consider feature engineering if creating interaction terms or new binary features based on domain knowledge.")
#     else:
#         # Handle non-numeric binary features
#         value_counts = data[feature].value_counts(normalize=True)
#         print(f"Value Counts for {feature}:")
#         print(value_counts)
        
#         # --- Contextual Insights and Recommendations ---
#         print("\n--- Contextual Insights and Recommendations ---\n")
#         print(f"High-Level Overview: The feature '{feature}' does not have numerical mean due to its data type.")
#         print("Detailed Technical Insights: The count plot still provides insight into the distribution of the feature values.")
#         print("Actionable Recommendations: Consider encoding techniques such as label encoding, one-hot encoding, or frequency encoding if the feature is to be used in a model.")



# # src/analysis/data_understanding.py

# import pandas as pd
# import seaborn as sns
# import matplotlib.pyplot as plt
# import logging
# from src.config_loader import load_paths, load_config, update_imputation_strategies
# from src.utils import save_analysis_results, ensure_directories_exist
# from src.visualization.visualize import plot_correlation_heatmap

# logger = logging.getLogger(__name__)

# def data_overview(train_sample, test_sample):
#     print("\n==== Data Overview ====\n")
#     print("Data Loaded: Train Sample Shape:", train_sample.shape, "Test Sample Shape:", test_sample.shape)
#     print(f"Number of rows in train sample: {train_sample.shape[0]}")
#     print(f"Number of columns in train sample: {train_sample.shape[1]}")
#     print(f"Column names in train sample: {list(train_sample.columns)}")
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: The dataset consists of 10,000 rows and 83 columns for the train sample, and 82 columns for the test sample.")
#     print("Detailed Technical Insights: The dataset contains a variety of features including numerical, categorical, and binary data.")
#     print("Actionable Recommendations: Proceed with initial analysis and understanding of the dataset's structure.")

# def data_types_and_formats(dataframe):
#     print("\n==== Data Types and Formats ====\n")
#     print(dataframe.dtypes)
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: The dataset includes multiple data types such as integers, floats, and objects.")
#     print("Detailed Technical Insights: Ensuring data types are correctly assigned is crucial for accurate analysis and modeling.")
#     print("Actionable Recommendations: Verify and correct data types if necessary before further analysis.")

# def statistical_summary(dataframe):
#     print("\n==== Statistical Summary ====\n")
#     print(dataframe.describe(include='all'))
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: Descriptive statistics provide an overview of the central tendency, dispersion, and shape of the dataset's distribution.")
#     print("Detailed Technical Insights: Key statistics include mean, standard deviation, and percentiles for numerical features; frequency counts for categorical features.")
#     print("Actionable Recommendations: Use these statistics to identify any anomalies or outliers in the dataset.")

# def target_variable_analysis(dataframe):
#     print("\n==== Target Variable Analysis: 'HasDetections' ====\n")
#     plt.figure(figsize=(6, 4))
#     ax = sns.countplot(x='HasDetections', data=dataframe)
#     plt.title('Distribution of HasDetections')
#     plt.xlabel('HasDetections Value')
#     plt.ylabel('Count')
#     total = len(dataframe['HasDetections'].dropna())
#     for p in ax.patches:
#         height = p.get_height()
#         ax.text(p.get_x() + p.get_width() / 2., height + 3, '{:1.2f}%'.format(100 * height/total), ha="center") 
#     plt.show()
#     detection_counts = dataframe['HasDetections'].value_counts(normalize=True)
#     print(f"Distribution of 'HasDetections':\n{detection_counts}")
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     if detection_counts[1] > 0.6 or detection_counts[0] > 0.6:
#         print("High-Level Overview: There is a class imbalance in the 'HasDetections' feature.")
#         print("Detailed Technical Insights: The imbalance might lead to model bias favoring the majority class.")
#         print("Actionable Recommendations: Consider using SMOTE, class weighting, or other resampling techniques to balance the classes.")
#     else:
#         print("High-Level Overview: The 'HasDetections' feature is relatively balanced.")
#         print("Detailed Technical Insights: The balanced distribution is beneficial for model training as it reduces the risk of bias towards one class.")
#         print("Actionable Recommendations: Proceed with model development without additional steps for handling class imbalance at this stage.")

# def missing_values_analysis(dataframe):
#     print("\n==== Missing Values Analysis ====\n")
#     missing_values = dataframe.isnull().sum()
#     missing_percentage = (missing_values / dataframe.shape[0]) * 100
#     missing_data = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})
#     missing_data_sorted = missing_data.sort_values(by='Missing Values', ascending=False)
#     plt.figure(figsize=(12, 6))
#     sns.barplot(x=missing_data_sorted.index, y=missing_data_sorted['Missing Values'])
#     plt.xticks(rotation=90)
#     plt.title('Missing Values per Column')
#     plt.ylabel('Number of Missing Values')
#     plt.xlabel('Feature')
#     plt.show()
#     print("Missing Values in Train Sample:")
#     print(missing_data_sorted[missing_data_sorted['Missing Values'] > 0])
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: Several features have a high percentage of missing values, particularly 'PuaMode' and 'Census_ProcessorClass'.")
#     print("Detailed Technical Insights: Features with a high percentage of missing values might need imputation strategies or could be excluded from the analysis if deemed irrelevant.")
#     print("Actionable Recommendations: Develop a missing value treatment plan, focusing on imputation strategies or feature exclusion based on the importance and impact on the target variable.")

# def feature_balance_analysis(dataframe):
#     print("\n==== Feature Balance Analysis ====\n")
#     feature_balance = {}
#     for col in dataframe.columns:
#         value_counts = dataframe[col].value_counts(normalize=True).max()
#         feature_balance[col] = value_counts * 100
#     feature_balance = pd.Series(feature_balance).sort_values(ascending=False)
#     plt.figure(figsize=(12, 6))
#     sns.barplot(x=feature_balance.index, y=feature_balance.values)
#     plt.xticks(rotation=90)
#     plt.title('Feature Balance (Percentage of Highest Value)')
#     plt.ylabel('Percentage')
#     plt.xlabel('Feature')
#     plt.show()
#     print("Feature Balance in Train Sample:")
#     print(feature_balance)
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: Many features show a high percentage of a single value, indicating potential imbalance.")
#     print("Detailed Technical Insights: Imbalanced features can impact the performance of machine learning models, particularly in classification tasks.")
#     print("Actionable Recommendations: Consider techniques to handle imbalanced features, such as resampling, feature engineering, or model adjustments.")

# def correlation_analysis(dataframe):
#     print("\n==== Correlation Analysis ====\n")
#     numerical_columns = dataframe.select_dtypes(include=['int64', 'float64']).columns
#     correlation_matrix = dataframe[numerical_columns].corr()
#     plt.figure(figsize=(15, 10))
#     sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')
#     plt.title('Correlation Matrix')
#     plt.show()
#     high_correlation_pairs = correlation_matrix.abs().unstack().sort_values(kind="quicksort", ascending=False).drop_duplicates()
#     print("\nHigh Correlation Pairs (absolute value > 0.8):\n")
#     print(high_correlation_pairs[high_correlation_pairs > 0.8])
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: Several feature pairs show high correlation (absolute value > 0.8).")
#     print("Detailed Technical Insights: Highly correlated features can indicate multicollinearity, which may affect model performance.")
#     print("Actionable Recommendations: Consider removing or combining highly correlated features to reduce redundancy and multicollinearity.")

# def feature_interactions(dataframe):
#     print("\n==== Feature Interactions ====\n")
#     sns.pairplot(dataframe, vars=[
#         'Census_OSInstallLanguageIdentifier', 'Census_OSUILocaleIdentifier', 
#         'Census_OSBuildNumber', 'OsBuild', 'IsSxsPassiveMode', 'RtpStateBitfield', 
#         'Census_InternalPrimaryDisplayResolutionHorizontal', 
#         'Census_InternalPrimaryDisplayResolutionVertical'
#     ])
#     plt.show()
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: The pair plots visualize relationships between highly correlated features.")
#     print("Detailed Technical Insights: Observing feature interactions can provide insights into underlying patterns and relationships in the data.")
#     print("Actionable Recommendations: Use these insights for feature engineering or model selection, leveraging relationships between features.")

# def feature_classification(dataframe):
#     print("\n==== Feature Classification ====\n")
#     binary_features_manual = [col for col in dataframe.columns if dataframe[col].nunique() == 2]
#     categorical_features_manual = [col for col in dataframe.columns if dataframe[col].dtype == 'object' and col not in binary_features_manual]
#     numerical_features_manual = [col for col in dataframe.columns if dataframe[col].dtype in ['int64', 'float64'] and col not in binary_features_manual]
    
#     binary_features_auto = [col for col in dataframe.columns if dataframe[col].nunique() == 2]
#     categorical_features_auto = [col for col in dataframe.columns if dataframe[col].dtype == 'object' and col not in binary_features_auto]
#     numerical_features_auto = [col for col in dataframe.columns if dataframe[col].dtype in ['int64', 'float64'] and col not in binary_features_auto]
    
#     mismatched_binary = set(binary_features_manual).symmetric_difference(binary_features_auto)
#     mismatched_categorical = set(categorical_features_manual).symmetric_difference(categorical_features_auto)
#     mismatched_numerical = set(numerical_features_manual).symmetric_difference(numerical_features_auto)
    
#     summary_table = pd.DataFrame({
#         'Feature Type': ['Binary', 'Categorical', 'Numerical'],
#         'Manual Count': [len(binary_features_manual), len(categorical_features_manual), len(numerical_features_manual)],
#         'Automated Count': [len(binary_features_auto), len(categorical_features_auto), len(numerical_features_auto)]
#     })
    
#     print(summary_table)
    
#     print("\n--- Feature Classification Debugging ---\n")
#     print(f"Binary Features (Manual): {binary_features_manual}")
#     print(f"Binary Features (Automated): {binary_features_auto}")
#     print(f"Mismatched Binary Features: {mismatched_binary}")
    
#     print(f"Categorical Features (Manual): {categorical_features_manual}")
#     print(f"Categorical Features (Automated): {categorical_features_auto}")
#     print(f"Mismatched Categorical Features: {mismatched_categorical}")
    
#     print(f"Numerical Features (Manual): {numerical_features_manual}")
#     print(f"Numerical Features (Automated): {numerical_features_auto}")
#     print(f"Mismatched Numerical Features: {mismatched_numerical}")
    
#     print("\n--- Further Inspection of Mismatched Features ---\n")
#     if mismatched_binary:
#         print(f"Mismatched Binary Features: {mismatched_binary}")
#         for feature in mismatched_binary:
#             print(f"Feature: {feature}")
#             print(f"Unique Values: {dataframe[feature].unique()}")
#             print(f"Data Type: {dataframe[feature].dtype}")
#             print("\n")
    
#     if mismatched_categorical:
#         print(f"Mismatched Categorical Features: {mismatched_categorical}")
#         for feature in mismatched_categorical:
#             print(f"Feature: {feature}")
#             print(f"Unique Values: {dataframe[feature].unique()}")
#             print(f"Data Type: {dataframe[feature].dtype}")
#             print("\n")
    
#     if mismatched_numerical:
#         print(f"Mismatched Numerical Features: {mismatched_numerical}")
#         for feature in mismatched_numerical:
#             print(f"Feature: {feature}")
#             print(f"Unique Values: {dataframe[feature].unique()}")
#             print(f"Data Type: {dataframe[feature].dtype}")
#             print("\n")
    
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: Feature classification is validated by comparing manual and automated classifications.")
#     print("Detailed Technical Insights: Mismatched features indicate discrepancies in classification which need further investigation.")
#     print("Actionable Recommendations: Investigate and correct mismatched features to ensure accurate classification.")















# def generate_summary_statistics(df):
#     summary_stats = df.describe(include='all').transpose()
#     summary_stats['missing_values'] = df.isnull().sum()
#     summary_stats['missing_percentage'] = (df.isnull().sum() / len(df)) * 100
#     return summary_stats

# def save_summary_statistics(summary_stats, report_dir, prefix):
#     tables_dir = os.path.join(report_dir, 'tables/00_initial_analysis')
#     figures_dir = os.path.join(report_dir, 'figures/00_initial_analysis')
    
#     os.makedirs(tables_dir, exist_ok=True)
#     os.makedirs(figures_dir, exist_ok=True)

#     summary_stats.to_csv(os.path.join(tables_dir, f'{prefix}_summary_statistics.csv'))
#     fig, ax = plt.subplots(figsize=(20, 10))
#     sns.heatmap(summary_stats.isnull(), cbar=False, ax=ax, cmap='viridis')
#     plt.xticks(rotation=45, ha='right')
#     plt.tight_layout()
#     plt.savefig(os.path.join(figures_dir, f'{prefix}_summary_statistics.png'))
#     plt.close()

# def initial_data_analysis(train_chunk, test_chunk, config):
#     try:
#         report_dir = config['report_dir']
#         ensure_directories_exist(config)

#         logger.info("Generating summary statistics for training data.")
#         train_summary_stats = generate_summary_statistics(train_chunk)
#         save_summary_statistics(train_summary_stats, report_dir, prefix='train')

#         logger.info("Generating summary statistics for test data.")
#         test_summary_stats = generate_summary_statistics(test_chunk)
#         save_summary_statistics(test_summary_stats, report_dir, prefix='test')

#         logger.info("Train Summary Statistics:")
#         print(train_summary_stats)

#         logger.info("Test Summary Statistics:")
#         print(test_summary_stats)

#         logger.info("Performing correlation analysis.")
#         corr_matrix = plot_correlation_heatmap(train_chunk, os.path.join(report_dir, 'figures/00_initial_analysis'))

#         logger.info("Suggesting imputation strategies.")
#         imputation_strategies = suggest_imputation_strategy(train_summary_stats, corr_matrix)

#         logger.info("Handling missing values.")
#         train_chunk = handle_missing_values(train_chunk, imputation_strategies)
#         test_chunk = handle_missing_values(test_chunk, imputation_strategies)

#         save_analysis_results(train_summary_stats, test_summary_stats, imputation_strategies, report_dir)

#         updated_config = update_imputation_strategies(imputation_strategies, config_path=config['config_paths']['imputation_strategies'])
#         logger.info(f"Updated Imputation Strategies: {updated_config}")

#         logger.info("Initial data analysis completed successfully.")
#         return train_chunk, test_chunk

#     except Exception as e:
#         logger.error(f"An error occurred during initial data analysis: {e}")
#         raise

# def save_cleaned_data(train_chunk, test_chunk, config):
#     train_cleaned_path = config['data_paths']['train_cleaned']
#     test_cleaned_path = config['data_paths']['test_cleaned']

#     train_chunk.to_csv(train_cleaned_path, index=False)
#     test_chunk.to_csv(test_cleaned_path, index=False)

#     logger.info(f"Cleaned training data saved to {train_cleaned_path}")
#     logger.info(f"Cleaned test data saved to {test_cleaned_path}")
















