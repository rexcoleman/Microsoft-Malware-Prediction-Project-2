# src/analysis/data_understanding.py

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import logging
import os
from src.config_loader import load_paths, load_config, update_imputation_strategies
from src.utils.file_operations import save_analysis_results, save_json_file, load_json_file
from src.utils.directory_utils import ensure_directories_exist
# save_to_json
logger = logging.getLogger(__name__)

def data_overview(train_sample, test_sample, save_dir=None):
    """
    Provides an overview of the training and test datasets, including shapes and column names.

    Parameters:
    train_sample (pd.DataFrame): The training sample DataFrame.
    test_sample (pd.DataFrame): The test sample DataFrame.
    save_dir (str, optional): Directory to save the overview JSON file.

    Raises:
    Exception: If there is an error during data overview processing.
    """
    try:
        overview = generate_overview(train_sample, test_sample)
        if save_dir:
            save_overview(overview, save_dir)
        print_overview(overview)
    except Exception as e:
        logger.error(f"Error in data overview: {e}")
        raise

def generate_overview(train_sample, test_sample):
    """
    Generates a summary of the training and test datasets.

    Parameters:
    train_sample (pd.DataFrame): The training sample DataFrame.
    test_sample (pd.DataFrame): The test sample DataFrame.

    Returns:
    dict: A dictionary containing the shapes and column names of the datasets.
    """
    overview = {
        "train_shape": train_sample.shape,
        "test_shape": test_sample.shape,
        "train_columns": list(train_sample.columns),
        "test_columns": list(test_sample.columns)
    }
    return overview

def save_overview(overview, save_dir):
    """
    Saves the overview summary to a JSON file.

    Parameters:
    overview (dict): The overview summary.
    save_dir (str): Directory to save the JSON file.
    """
    try:
        os.makedirs(save_dir, exist_ok=True)
        save_json_file(overview, os.path.join(save_dir, 'data_overview.json'))
    except Exception as e:
        logger.error(f"Error saving data overview: {e}")
        raise

def print_overview(overview):
    """
    Prints the overview summary to the console.

    Parameters:
    overview (dict): The overview summary.
    """
    print("\n==== Data Overview ====\n")
    print("Data Loaded: Train Sample Shape:", overview["train_shape"], "Test Sample Shape:", overview["test_shape"])
    print(f"Number of rows in train sample: {overview['train_shape'][0]}")
    print(f"Number of columns in train sample: {overview['train_shape'][1]}")
    print(f"Column names in train sample: {overview['train_columns']}")
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: The dataset consists of 10,000 rows and 83 columns for the train sample, and 82 columns for the test sample.")
    print("Detailed Technical Insights: The dataset contains a variety of features including numerical, categorical, and binary data.")
    print("Actionable Recommendations: Proceed with initial analysis and understanding of the dataset's structure.")

def data_types_and_formats(dataframe, save_dir=None):
    """
    Analyzes and documents the data types and formats in the dataframe.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame to analyze.
    save_dir (str, optional): Directory to save the data types JSON file.

    Raises:
    Exception: If there is an error during the data types analysis.
    """
    try:
        data_types = analyze_data_types(dataframe)
        if save_dir:
            save_data_types(data_types, save_dir)
        print_data_types(dataframe)
    except Exception as e:
        logger.error(f"Error in data types and formats analysis: {e}")
        raise

def analyze_data_types(dataframe):
    """
    Analyzes the data types of the dataframe columns.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame to analyze.

    Returns:
    dict: A dictionary with column names as keys and data types as values.
    """
    return dataframe.dtypes.apply(lambda x: x.name).to_dict()

def save_data_types(data_types, save_dir):
    """
    Saves the data types information to a JSON file.

    Parameters:
    data_types (dict): The data types information.
    save_dir (str): Directory to save the JSON file.

    Raises:
    Exception: If there is an error saving the JSON file.
    """
    try:
        os.makedirs(save_dir, exist_ok=True)
        save_json_file(data_types, os.path.join(save_dir, 'data_types.json'))
    except Exception as e:
        logger.error(f"Error saving data types: {e}")
        raise

def print_data_types(dataframe):
    """
    Prints the data types of the dataframe columns to the console.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame to analyze.

    Raises:
    Exception: If there is an error printing the data types.
    """
    try:
        print("\n==== Data Types and Formats ====\n")
        print(dataframe.dtypes)
        print("\n--- Contextual Insights and Recommendations ---\n")
        print("High-Level Overview: The dataset includes multiple data types such as integers, floats, and objects.")
        print("Detailed Technical Insights: Ensuring data types are correctly assigned is crucial for accurate analysis and modeling.")
        print("Actionable Recommendations: Verify and correct data types if necessary before further analysis.")
    except Exception as e:
        logger.error(f"Error printing data types: {e}")
        raise

def statistical_summary(dataframe, save_dir=None):
    """
    Generates and saves a statistical summary of the dataframe.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame to analyze.
    save_dir (str, optional): Directory to save the statistical summary JSON file.

    Raises:
    Exception: If there is an error during the statistical summary generation or saving.
    """
    try:
        summary = generate_statistical_summary(dataframe)
        if save_dir:
            save_statistical_summary(summary, save_dir)
        print_statistical_summary(dataframe)
    except Exception as e:
        logger.error(f"Error in statistical summary: {e}")
        raise

def generate_statistical_summary(dataframe):
    """
    Generates descriptive statistics for the dataframe.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame to analyze.

    Returns:
    dict: A dictionary containing the descriptive statistics.
    """
    return dataframe.describe(include='all').to_dict()

def save_statistical_summary(summary, save_dir):
    """
    Saves the statistical summary to a JSON file.

    Parameters:
    summary (dict): The statistical summary.
    save_dir (str): Directory to save the JSON file.

    Raises:
    Exception: If there is an error saving the JSON file.
    """
    try:
        os.makedirs(save_dir, exist_ok=True)
        save_json_file(summary, os.path.join(save_dir, 'stat_summary.json'))
    except Exception as e:
        logger.error(f"Error saving statistical summary: {e}")
        raise

def print_statistical_summary(dataframe):
    """
    Prints the statistical summary of the dataframe to the console.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame to analyze.

    Raises:
    Exception: If there is an error printing the statistical summary.
    """
    try:
        print("\n==== Statistical Summary ====\n")
        print(dataframe.describe(include='all'))
        print("\n--- Contextual Insights and Recommendations ---\n")
        print("High-Level Overview: Descriptive statistics provide an overview of the central tendency, dispersion, and shape of the dataset's distribution.")
        print("Detailed Technical Insights: Key statistics include mean, standard deviation, and percentiles for numerical features; frequency counts for categorical features.")
        print("Actionable Recommendations: Use these statistics to identify any anomalies or outliers in the dataset.")
    except Exception as e:
        logger.error(f"Error printing statistical summary: {e}")
        raise

def target_variable_analysis(dataframe, target_column='HasDetections', save_dir=None):
    """
    Perform analysis on the target variable and save the results.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame containing the target variable.
    target_column (str): The name of the target variable column.
    save_dir (str, optional): Directory to save the analysis results.

    Raises:
    Exception: If there is an error during the analysis.
    """
    try:
        target_summary = get_target_summary(dataframe, target_column)
        if save_dir:
            save_target_summary(target_summary, save_dir)
        print_target_analysis(dataframe, target_column, target_summary)
    except Exception as e:
        logger.error(f"Error in target variable analysis: {e}")
        raise

def get_target_summary(dataframe, target_column):
    """
    Get the summary of the target variable.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame containing the target variable.
    target_column (str): The name of the target variable column.

    Returns:
    dict: A dictionary containing the normalized value counts of the target variable.
    """
    return dataframe[target_column].value_counts(normalize=True).to_dict()

def save_target_summary(target_summary, save_dir):
    """
    Save the target variable summary to a JSON file.

    Parameters:
    target_summary (dict): The summary of the target variable.
    save_dir (str): Directory to save the JSON file.

    Raises:
    Exception: If there is an error saving the JSON file.
    """
    try:
        os.makedirs(save_dir, exist_ok=True)
        save_json_file(target_summary, os.path.join(save_dir, 'target_analysis.json'))
    except Exception as e:
        logger.error(f"Error saving target summary: {e}")
        raise

def print_target_analysis(dataframe, target_column, target_summary):
    """
    Print the target variable analysis and plot the distribution.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame containing the target variable.
    target_column (str): The name of the target variable column.
    target_summary (dict): The summary of the target variable.

    Raises:
    Exception: If there is an error printing the analysis.
    """
    try:
        print(f"\n==== Target Variable Analysis: '{target_column}' ====\n")
        plot_target_distribution(dataframe, target_column)
        detection_counts = dataframe[target_column].value_counts(normalize=True)
        print(f"Distribution of '{target_column}':\n{detection_counts}")
        print("\n--- Contextual Insights and Recommendations ---\n")
        if detection_counts[1] > 0.6 or detection_counts[0] > 0.6:
            print("High-Level Overview: There is a class imbalance in the target feature.")
            print("Detailed Technical Insights: The imbalance might lead to model bias favoring the majority class.")
            print("Actionable Recommendations: Consider using SMOTE, class weighting, or other resampling techniques to balance the classes.")
        else:
            print("High-Level Overview: The target feature is relatively balanced.")
            print("Detailed Technical Insights: The balanced distribution is beneficial for model training as it reduces the risk of bias towards one class.")
            print("Actionable Recommendations: Proceed with model development without additional steps for handling class imbalance at this stage.")
    except Exception as e:
        logger.error(f"Error printing target analysis: {e}")
        raise

def plot_target_distribution(dataframe, target_column):
    """
    Plot the distribution of the target variable.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame containing the target variable.
    target_column (str): The name of the target variable column.

    Raises:
    Exception: If there is an error plotting the distribution.
    """
    try:
        plt.figure(figsize=(6, 4))
        ax = sns.countplot(x=target_column, data=dataframe)
        plt.title('Distribution of Target Variable')
        plt.xlabel(f'{target_column} Value')
        plt.ylabel('Count')
        total = len(dataframe[target_column].dropna())
        for p in ax.patches:
            height = p.get_height()
            ax.text(p.get_x() + p.get_width() / 2., height + 3, '{:1.2f}%'.format(100 * height / total), ha="center") 
        plt.show()
    except Exception as e:
        logger.error(f"Error plotting target distribution: {e}")
        raise

def missing_values_analysis(dataframe, save_dir=None):
    """
    Perform analysis on missing values in the dataframe and save the results.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame to analyze for missing values.
    save_dir (str, optional): Directory to save the analysis results.

    Raises:
    Exception: If there is an error during the analysis.
    """
    try:
        missing_data = get_missing_values(dataframe)
        if save_dir:
            save_missing_values(missing_data, save_dir)
        print("\n==== Missing Values Analysis ====\n")
        plot_missing_values(missing_data)
        print_missing_values_analysis(missing_data)
    except Exception as e:
        logger.error(f"Error in missing values analysis: {e}")
        raise

def get_missing_values(dataframe):
    """
    Get the count and percentage of missing values for each feature in the dataframe.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.

    Returns:
    dict: A dictionary with feature names as keys and dictionaries with 'count' and 'percentage' of missing values as values.
    """
    missing_values = dataframe.isnull().sum().to_dict()
    missing_percentage = (dataframe.isnull().sum() / dataframe.shape[0] * 100).to_dict()
    missing_data = {feature: {'count': missing_values[feature], 'percentage': missing_percentage[feature]} for feature in missing_values}
    return missing_data

def save_missing_values(missing_data, save_dir):
    """
    Save the missing values analysis results to a JSON file.

    Parameters:
    missing_data (dict): The missing values analysis results.
    save_dir (str): Directory to save the JSON file.

    Raises:
    Exception: If there is an error saving the JSON file.
    """
    try:
        os.makedirs(save_dir, exist_ok=True)
        save_json_file(missing_data, os.path.join(save_dir, 'missing_values.json'))
    except Exception as e:
        logger.error(f"Error saving missing values analysis: {e}")
        raise

def print_missing_values_analysis(missing_data):
    """
    Print the missing values analysis.

    Parameters:
    missing_data (dict): The missing values analysis results.

    Raises:
    Exception: If there is an error printing the analysis.
    """
    try:
        missing_data_sorted = pd.DataFrame(missing_data).T.sort_values(by='count', ascending=False)
        print("Missing Values in Train Sample:")
        print(missing_data_sorted[missing_data_sorted['count'] > 0])
        print("\n--- Contextual Insights and Recommendations ---\n")
        print("High-Level Overview: Several features have a high percentage of missing values, particularly 'PuaMode' and 'Census_ProcessorClass'.")
        print("Detailed Technical Insights: Features with a high percentage of missing values might need imputation strategies or could be excluded from the analysis if deemed irrelevant.")
        print("Actionable Recommendations: Develop a missing value treatment plan, focusing on imputation strategies or feature exclusion based on the importance and impact on the target variable.")
    except Exception as e:
        logger.error(f"Error printing missing values analysis: {e}")
        raise

def plot_missing_values(missing_data):
    """
    Plot the missing values analysis results.

    Parameters:
    missing_data (dict): The missing values analysis results.

    Raises:
    Exception: If there is an error plotting the analysis.
    """
    try:
        missing_data_sorted = pd.DataFrame(missing_data).T.sort_values(by='count', ascending=False)
        plt.figure(figsize=(12, 6))
        sns.barplot(x=missing_data_sorted.index, y=missing_data_sorted['count'])
        plt.xticks(rotation=90)
        plt.title('Missing Values per Column')
        plt.ylabel('Number of Missing Values')
        plt.xlabel('Feature')
        plt.show()
    except Exception as e:
        logger.error(f"Error plotting missing values: {e}")
        raise

def feature_balance_analysis(dataframe, save_dir=None):
    """
    Perform feature balance analysis on the given dataframe and save the results.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    save_dir (str, optional): Directory to save the analysis results.

    Raises:
    Exception: If there is an error during the analysis.
    """
    try:
        feature_balance = calculate_feature_balance(dataframe)
        if save_dir:
            save_feature_balance(feature_balance, save_dir)
        print_feature_balance(feature_balance)
    except Exception as e:
        logger.error(f"Error in feature balance analysis: {e}")
        raise

def calculate_feature_balance(dataframe):
    """
    Calculate the feature balance for each column in the dataframe.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.

    Returns:
    dict: A dictionary with the feature balance for each column.
    """
    return {col: {'most_common_value_weight': dataframe[col].value_counts(normalize=True).max() * 100} for col in dataframe.columns}

def save_feature_balance(feature_balance, save_dir):
    """
    Save the feature balance analysis results to a JSON file.

    Parameters:
    feature_balance (dict): The feature balance analysis results.
    save_dir (str): Directory to save the JSON file.

    Raises:
    Exception: If there is an error saving the JSON file.
    """
    try:
        os.makedirs(save_dir, exist_ok=True)
        save_json_file(feature_balance, os.path.join(save_dir, 'feature_balance.json'))
    except Exception as e:
        logger.error(f"Error saving feature balance: {e}")
        raise

def print_feature_balance(feature_balance):
    """
    Print the feature balance analysis results and plot the balance.

    Parameters:
    feature_balance (dict): The feature balance analysis results.
    """
    feature_balance_sorted = pd.Series({col: bal['most_common_value_weight'] for col, bal in feature_balance.items()}).sort_values(ascending=False)
    print("\n==== Feature Balance Analysis ====\n")
    
    # Plot the feature balance
    plt.figure(figsize=(12, 6))
    sns.barplot(x=feature_balance_sorted.index, y=feature_balance_sorted.values)
    plt.xticks(rotation=90)
    plt.title('Feature Balance (Percentage of Highest Value)')
    plt.ylabel('Percentage')
    plt.xlabel('Feature')
    plt.show()
    
    # Print the feature balance table
    print("Feature Balance in Train Sample:")
    print(feature_balance_sorted)
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Many features show a high percentage of a single value, indicating potential imbalance.")
    print("Detailed Technical Insights: Imbalanced features can impact the performance of machine learning models, particularly in classification tasks.")
    print("Actionable Recommendations: Consider techniques to handle imbalanced features, such as resampling, feature engineering, or model adjustments.")

def correlation_analysis(dataframe, save_dir=None, pair_threshold=0.2):
    """
    Perform correlation analysis on the given dataframe and save the results.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    save_dir (str, optional): Directory to save the analysis results.
    pair_threshold (float, optional): Threshold for high correlation pairs.

    Returns:
    tuple: High correlation pairs and target correlation with 'HasDetections'.
    """
    try:
        numerical_columns = dataframe.select_dtypes(include=['int64', 'float64']).columns
        correlation_matrix = dataframe[numerical_columns].corr()

        high_correlation_pairs = calculate_high_correlation_pairs(correlation_matrix, pair_threshold)
        target_corr = calculate_target_correlation(correlation_matrix, 'HasDetections')

        if save_dir:
            save_correlation_results(correlation_matrix, high_correlation_pairs, target_corr, save_dir)

        print_correlation_analysis(correlation_matrix, high_correlation_pairs, pair_threshold)
        print_target_correlation_analysis(target_corr)

        return high_correlation_pairs, target_corr

    except Exception as e:
        logger.error(f"Error in correlation analysis: {e}")
        raise

def calculate_high_correlation_pairs(correlation_matrix, threshold):
    """
    Calculate high correlation pairs based on the given threshold.

    Parameters:
    correlation_matrix (pd.DataFrame): Correlation matrix.
    threshold (float): Threshold for high correlation pairs.

    Returns:
    pd.Series: High correlation pairs.
    """
    high_correlation_pairs = correlation_matrix.unstack().sort_values(kind="quicksort", ascending=False).drop_duplicates()
    high_correlation_pairs = high_correlation_pairs[(high_correlation_pairs.abs() > threshold) & (high_correlation_pairs != 1)]
    return high_correlation_pairs

def calculate_target_correlation(correlation_matrix, target_column):
    """
    Calculate correlation with the target variable.

    Parameters:
    correlation_matrix (pd.DataFrame): Correlation matrix.
    target_column (str): Target column name.

    Returns:
    pd.Series: Correlation with the target variable.
    """
    return correlation_matrix[target_column].drop(target_column).sort_values(ascending=False)

def save_correlation_results(correlation_matrix, high_correlation_pairs, target_corr, save_dir):
    """
    Save the correlation analysis results to JSON files.

    Parameters:
    correlation_matrix (pd.DataFrame): Correlation matrix.
    high_correlation_pairs (pd.Series): High correlation pairs.
    target_corr (pd.Series): Correlation with the target variable.
    save_dir (str): Directory to save the JSON files.

    Raises:
    Exception: If there is an error saving the JSON files.
    """
    try:
        save_json_file(correlation_matrix.to_dict(), os.path.join(save_dir, 'correlations.json'))
        save_json_file({str(k): {'correlation': v} for k, v in high_correlation_pairs.to_dict().items()}, os.path.join(save_dir, 'high_correlation_pairs.json'))
        save_json_file(target_corr.to_dict(), os.path.join(save_dir, 'target_correlations.json'))
    except Exception as e:
        logger.error(f"Error saving correlation results: {e}")
        raise

def print_correlation_analysis(correlation_matrix, high_correlation_pairs, pair_threshold):
    """
    Print the correlation analysis results.

    Parameters:
    correlation_matrix (pd.DataFrame): Correlation matrix.
    high_correlation_pairs (pd.Series): High correlation pairs.
    pair_threshold (float): Threshold for high correlation pairs.
    """
    print("\n==== Correlation Analysis ====\n")
    plt.figure(figsize=(15, 10))
    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')
    plt.title('Correlation Matrix')
    plt.show()

    print("\nHigh Correlation Pairs (absolute value > {} or < -{}):\n".format(pair_threshold, pair_threshold))
    print(high_correlation_pairs)
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Several feature pairs show high correlation (absolute value > {} or < -{}).".format(pair_threshold, pair_threshold))
    print("Detailed Technical Insights: Highly correlated features can indicate multicollinearity, which may affect model performance.")
    print("Actionable Recommendations: Consider removing or combining highly correlated features to reduce redundancy and multicollinearity.")

def print_target_correlation_analysis(target_corr):
    """
    Print the target correlation analysis results.

    Parameters:
    target_corr (pd.Series): Correlation with the target variable.
    """
    print("\n==== Correlation with Target Variable 'HasDetections' ====\n")
    plt.figure(figsize=(12, 8))
    sns.barplot(x=target_corr.values, y=target_corr.index, hue=target_corr.index, dodge=False, palette='coolwarm', legend=False)
    plt.title("Correlation with 'HasDetections'")
    plt.xlabel("Correlation coefficient")
    plt.ylabel("Features")
    plt.show()

    print("\nCorrelation with 'HasDetections':\n", target_corr)
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Some features show strong correlation with the target variable 'HasDetections'.")
    print("Detailed Technical Insights: Identifying these features can help in feature selection and engineering.")
    print("Actionable Recommendations: Consider these features for model development and further analysis.")

def feature_interactions(dataframe, save_dir=None, features=None):
    """
    Analyze feature interactions in the given dataframe and save the results.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    save_dir (str, optional): Directory to save the analysis results.
    features (list, optional): List of features to analyze interactions.

    Returns:
    None
    """
    if features is None:
        features = [
            'Census_OSInstallLanguageIdentifier', 'Census_OSUILocaleIdentifier', 
            'Census_OSBuildNumber', 'OsBuild', 'IsSxsPassiveMode', 'RtpStateBitfield', 
            'Census_InternalPrimaryDisplayResolutionHorizontal', 
            'Census_InternalPrimaryDisplayResolutionVertical'
        ]

    try:
        plot_feature_interactions(dataframe, features)
        if save_dir:
            save_json_file({}, os.path.join(save_dir, 'feature_interactions.json'))
    except Exception as e:
        logger.error(f"Error in feature interactions analysis: {e}")
        raise

def plot_feature_interactions(dataframe, features):
    """
    Plot feature interactions for the given features.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    features (list): List of features to analyze interactions.

    Returns:
    None
    """
    print("\n==== Feature Interactions ====\n")
    sns.pairplot(dataframe, vars=features)
    plt.show()
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: The pair plots visualize relationships between highly correlated features.")
    print("Detailed Technical Insights: Observing feature interactions can provide insights into underlying patterns and relationships in the data.")
    print("Actionable Recommendations: Use these insights for feature engineering or model selection, leveraging relationships between features.")




# src/analysis/data_understanding.py

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import logging
import os
# from src.utils.file_operations import save_to_json

logger = logging.getLogger(__name__)

def feature_classification(dataframe, save_dir=None):
    """
    Classify features in the dataframe into binary, categorical, and numerical types.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    save_dir (str, optional): Directory to save the classification results.

    Returns:
    tuple: A tuple containing lists of binary, categorical, and numerical features classified automatically.
    """
    try:
        binary_features_manual, categorical_features_manual, numerical_features_manual = manual_feature_classification(dataframe)
        binary_features_auto, categorical_features_auto, numerical_features_auto = auto_feature_classification(dataframe)
        
        mismatched_binary, mismatched_categorical, mismatched_numerical = find_mismatched_features(
            binary_features_manual, binary_features_auto,
            categorical_features_manual, categorical_features_auto,
            numerical_features_manual, numerical_features_auto
        )
        
        classification_results = compile_classification_results(
            binary_features_auto, categorical_features_auto, numerical_features_auto,
            mismatched_binary, mismatched_categorical, mismatched_numerical
        )
        
        if save_dir:
            save_json_file(classification_results, os.path.join(save_dir, 'feature_classification.json'))
        
        print_feature_classification_summary(
            dataframe,
            binary_features_manual, categorical_features_manual, numerical_features_manual,
            binary_features_auto, categorical_features_auto, numerical_features_auto,
            mismatched_binary, mismatched_categorical, mismatched_numerical
        )
        
        return binary_features_auto, categorical_features_auto, numerical_features_auto
    except Exception as e:
        logger.error(f"Error in feature classification: {e}")
        raise

def manual_feature_classification(dataframe):
    """
    Manually classify features in the dataframe.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.

    Returns:
    tuple: A tuple containing lists of manually classified binary, categorical, and numerical features.
    """
    binary_features_manual = [col for col in dataframe.columns if dataframe[col].nunique() == 2]
    categorical_features_manual = [col for col in dataframe.columns if dataframe[col].dtype == 'object' and col not in binary_features_manual]
    numerical_features_manual = [col for col in dataframe.columns if dataframe[col].dtype in ['int64', 'float64'] and col not in binary_features_manual]
    return binary_features_manual, categorical_features_manual, numerical_features_manual

def auto_feature_classification(dataframe):
    """
    Automatically classify features in the dataframe.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.

    Returns:
    tuple: A tuple containing lists of automatically classified binary, categorical, and numerical features.
    """
    binary_features_auto = [col for col in dataframe.columns if dataframe[col].nunique() == 2]
    categorical_features_auto = [col for col in dataframe.columns if dataframe[col].dtype == 'object' and col not in binary_features_auto]
    numerical_features_auto = [col for col in dataframe.columns if dataframe[col].dtype in ['int64', 'float64'] and col not in binary_features_auto]
    return binary_features_auto, categorical_features_auto, numerical_features_auto

def find_mismatched_features(binary_manual, binary_auto, categorical_manual, categorical_auto, numerical_manual, numerical_auto):
    """
    Find mismatched features between manual and automated classifications.

    Parameters:
    binary_manual (list): List of manually classified binary features.
    binary_auto (list): List of automatically classified binary features.
    categorical_manual (list): List of manually classified categorical features.
    categorical_auto (list): List of automatically classified categorical features.
    numerical_manual (list): List of manually classified numerical features.
    numerical_auto (list): List of automatically classified numerical features.

    Returns:
    tuple: A tuple containing sets of mismatched binary, categorical, and numerical features.
    """
    mismatched_binary = set(binary_manual).symmetric_difference(binary_auto)
    mismatched_categorical = set(categorical_manual).symmetric_difference(categorical_auto)
    mismatched_numerical = set(numerical_manual).symmetric_difference(numerical_auto)
    return mismatched_binary, mismatched_categorical, mismatched_numerical

def compile_classification_results(binary_auto, categorical_auto, numerical_auto, mismatched_binary, mismatched_categorical, mismatched_numerical):
    """
    Compile classification results into a dictionary.

    Parameters:
    binary_auto (list): List of automatically classified binary features.
    categorical_auto (list): List of automatically classified categorical features.
    numerical_auto (list): List of automatically classified numerical features.
    mismatched_binary (set): Set of mismatched binary features.
    mismatched_categorical (set): Set of mismatched categorical features.
    mismatched_numerical (set): Set of mismatched numerical features.

    Returns:
    dict: Dictionary containing classification results.
    """
    return {
        'binary_features_auto': binary_auto,
        'categorical_features_auto': categorical_auto,
        'numerical_features_auto': numerical_auto,
        'mismatched_binary': list(mismatched_binary),
        'mismatched_categorical': list(mismatched_categorical),
        'mismatched_numerical': list(mismatched_numerical)
    }

def print_feature_classification_summary(dataframe, binary_manual, categorical_manual, numerical_manual, binary_auto, categorical_auto, numerical_auto, mismatched_binary, mismatched_categorical, mismatched_numerical):
    """
    Print the feature classification summary.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    binary_manual (list): List of manually classified binary features.
    categorical_manual (list): List of manually classified categorical features.
    numerical_manual (list): List of manually classified numerical features.
    binary_auto (list): List of automatically classified binary features.
    categorical_auto (list): List of automatically classified categorical features.
    numerical_auto (list): List of automatically classified numerical features.
    mismatched_binary (set): Set of mismatched binary features.
    mismatched_categorical (set): Set of mismatched categorical features.
    mismatched_numerical (set): Set of mismatched numerical features.

    Returns:
    None
    """
    summary_table = pd.DataFrame({
        'Feature Type': ['Binary', 'Categorical', 'Numerical'],
        'Manual Count': [len(binary_manual), len(categorical_manual), len(numerical_manual)],
        'Automated Count': [len(binary_auto), len(categorical_auto), len(numerical_auto)]
    })

    print("\n==== Feature Classification ====\n")
    print(summary_table)

    print("\n--- Feature Classification Debugging ---\n")
    print(f"Binary Features (Manual): {binary_manual}")
    print(f"Binary Features (Automated): {binary_auto}")
    print(f"Mismatched Binary Features: {mismatched_binary}")

    print(f"Categorical Features (Manual): {categorical_manual}")
    print(f"Categorical Features (Automated): {categorical_auto}")
    print(f"Mismatched Categorical Features: {mismatched_categorical}")

    print(f"Numerical Features (Manual): {numerical_manual}")
    print(f"Numerical Features (Automated): {numerical_auto}")
    print(f"Mismatched Numerical Features: {mismatched_numerical}")

    print("\n--- Further Inspection of Mismatched Features ---\n")
    inspect_mismatched_features(dataframe, mismatched_binary, 'Binary')
    inspect_mismatched_features(dataframe, mismatched_categorical, 'Categorical')
    inspect_mismatched_features(dataframe, mismatched_numerical, 'Numerical')

    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Feature classification is validated by comparing manual and automated classifications.")
    print("Detailed Technical Insights: Mismatched features indicate discrepancies in classification which need further investigation.")
    print("Actionable Recommendations: Investigate and correct mismatched features to ensure accurate classification.")

def inspect_mismatched_features(dataframe, mismatched_features, feature_type):
    """
    Inspect and print details of mismatched features.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    mismatched_features (set): Set of mismatched features.
    feature_type (str): Type of features (Binary, Categorical, Numerical).

    Returns:
    None
    """
    if mismatched_features:
        print(f"Mismatched {feature_type} Features: {mismatched_features}")
        for feature in mismatched_features:
            print(f"Feature: {feature}")
            print(f"Unique Values: {dataframe[feature].unique()}")
            print(f"Data Type: {dataframe[feature].dtype}")
            print("\n")








    
    


# def feature_classification(dataframe, save_dir=None):
#     binary_features_manual = [col for col in dataframe.columns if dataframe[col].nunique() == 2]
#     categorical_features_manual = [col for col in dataframe.columns if dataframe[col].dtype == 'object' and col not in binary_features_manual]
#     numerical_features_manual = [col for col in dataframe.columns if dataframe[col].dtype in ['int64', 'float64'] and col not in binary_features_manual]
    
#     binary_features_auto = [col for col in dataframe.columns if dataframe[col].nunique() == 2]
#     categorical_features_auto = [col for col in dataframe.columns if dataframe[col].dtype == 'object' and col not in binary_features_auto]
#     numerical_features_auto = [col for col in dataframe.columns if dataframe[col].dtype in ['int64', 'float64'] and col not in binary_features_auto]
    
#     mismatched_binary = set(binary_features_manual).symmetric_difference(binary_features_auto)
#     mismatched_categorical = set(categorical_features_manual).symmetric_difference(categorical_features_auto)
#     mismatched_numerical = set(numerical_features_manual).symmetric_difference(numerical_features_auto)
    
#     # Save classification results to JSON
#     classification_results = {
#         'binary_features_auto': binary_features_auto,
#         'categorical_features_auto': categorical_features_auto,
#         'numerical_features_auto': numerical_features_auto,
#         'mismatched_binary': list(mismatched_binary),
#         'mismatched_categorical': list(mismatched_categorical),
#         'mismatched_numerical': list(mismatched_numerical)
#     }
#     if save_dir:
#         save_to_json(classification_results, os.path.join(save_dir, 'feature_classification.json'))
    
#     # Print summary table
#     summary_table = pd.DataFrame({
#         'Feature Type': ['Binary', 'Categorical', 'Numerical'],
#         'Manual Count': [len(binary_features_manual), len(categorical_features_manual), len(numerical_features_manual)],
#         'Automated Count': [len(binary_features_auto), len(categorical_features_auto), len(numerical_features_auto)]
#     })
    
#     print("\n==== Feature Classification ====\n")
#     print(summary_table)
    
#     print("\n--- Feature Classification Debugging ---\n")
#     print(f"Binary Features (Manual): {binary_features_manual}")
#     print(f"Binary Features (Automated): {binary_features_auto}")
#     print(f"Mismatched Binary Features: {mismatched_binary}")
    
#     print(f"Categorical Features (Manual): {categorical_features_manual}")
#     print(f"Categorical Features (Automated): {categorical_features_auto}")
#     print(f"Mismatched Categorical Features: {mismatched_categorical}")
    
#     print(f"Numerical Features (Manual): {numerical_features_manual}")
#     print(f"Numerical Features (Automated): {numerical_features_auto}")
#     print(f"Mismatched Numerical Features: {mismatched_numerical}")
    
#     print("\n--- Further Inspection of Mismatched Features ---\n")
#     if mismatched_binary:
#         print(f"Mismatched Binary Features: {mismatched_binary}")
#         for feature in mismatched_binary:
#             print(f"Feature: {feature}")
#             print(f"Unique Values: {dataframe[feature].unique()}")
#             print(f"Data Type: {dataframe[feature].dtype}")
#             print("\n")
    
#     if mismatched_categorical:
#         print(f"Mismatched Categorical Features: {mismatched_categorical}")
#         for feature in mismatched_categorical:
#             print(f"Feature: {feature}")
#             print(f"Unique Values: {dataframe[feature].unique()}")
#             print(f"Data Type: {dataframe[feature].dtype}")
#             print("\n")
    
#     if mismatched_numerical:
#         print(f"Mismatched Numerical Features: {mismatched_numerical}")
#         for feature in mismatched_numerical:
#             print(f"Feature: {feature}")
#             print(f"Unique Values: {dataframe[feature].unique()}")
#             print(f"Data Type: {dataframe[feature].dtype}")
#             print("\n")
    
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: Feature classification is validated by comparing manual and automated classifications.")
#     print("Detailed Technical Insights: Mismatched features indicate discrepancies in classification which need further investigation.")
#     print("Actionable Recommendations: Investigate and correct mismatched features to ensure accurate classification.")
    
#     return binary_features_auto, categorical_features_auto, numerical_features_auto













# def data_overview(train_sample, test_sample, save_dir=None):
#     overview = {
#         "train_shape": train_sample.shape,
#         "test_shape": test_sample.shape,
#         "train_columns": list(train_sample.columns),
#         "test_columns": list(test_sample.columns)
#     }
#     if save_dir:
#         save_to_json(overview, os.path.join(save_dir, 'data_overview.json'))
#     print("\n==== Data Overview ====\n")
#     print("Data Loaded: Train Sample Shape:", train_sample.shape, "Test Sample Shape:", test_sample.shape)
#     print(f"Number of rows in train sample: {train_sample.shape[0]}")
#     print(f"Number of columns in train sample: {train_sample.shape[1]}")
#     print(f"Column names in train sample: {list(train_sample.columns)}")
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: The dataset consists of 10,000 rows and 83 columns for the train sample, and 82 columns for the test sample.")
#     print("Detailed Technical Insights: The dataset contains a variety of features including numerical, categorical, and binary data.")
#     print("Actionable Recommendations: Proceed with initial analysis and understanding of the dataset's structure.")



# def data_types_and_formats(dataframe, save_dir=None):
#     data_types = dataframe.dtypes.apply(lambda x: x.name).to_dict()
#     if save_dir:
#         save_to_json(data_types, os.path.join(save_dir, 'data_types.json'))
#     print("\n==== Data Types and Formats ====\n")
#     print(dataframe.dtypes)
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: The dataset includes multiple data types such as integers, floats, and objects.")
#     print("Detailed Technical Insights: Ensuring data types are correctly assigned is crucial for accurate analysis and modeling.")
#     print("Actionable Recommendations: Verify and correct data types if necessary before further analysis.")



# def statistical_summary(dataframe, save_dir=None):
#     summary = dataframe.describe(include='all').to_dict()
#     if save_dir:
#         save_to_json(summary, os.path.join(save_dir, 'stat_summary.json'))
#     print("\n==== Statistical Summary ====\n")
#     print(dataframe.describe(include='all'))
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: Descriptive statistics provide an overview of the central tendency, dispersion, and shape of the dataset's distribution.")
#     print("Detailed Technical Insights: Key statistics include mean, standard deviation, and percentiles for numerical features; frequency counts for categorical features.")
#     print("Actionable Recommendations: Use these statistics to identify any anomalies or outliers in the dataset.")

# def target_variable_analysis(dataframe, save_dir=None):
#     target_summary = dataframe['HasDetections'].value_counts(normalize=True).to_dict()
#     if save_dir:
#         save_to_json(target_summary, os.path.join(save_dir, 'target_analysis.json'))
#     print("\n==== Target Variable Analysis: 'HasDetections' ====\n")
#     plt.figure(figsize=(6, 4))
#     ax = sns.countplot(x='HasDetections', data=dataframe)
#     plt.title('Distribution of HasDetections')
#     plt.xlabel('HasDetections Value')
#     plt.ylabel('Count')
#     total = len(dataframe['HasDetections'].dropna())
#     for p in ax.patches:
#         height = p.get_height()
#         ax.text(p.get_x() + p.get_width() / 2., height + 3, '{:1.2f}%'.format(100 * height/total), ha="center") 
#     plt.show()
#     detection_counts = dataframe['HasDetections'].value_counts(normalize=True)
#     print(f"Distribution of 'HasDetections':\n{detection_counts}")
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     if detection_counts[1] > 0.6 or detection_counts[0] > 0.6:
#         print("High-Level Overview: There is a class imbalance in the 'HasDetections' feature.")
#         print("Detailed Technical Insights: The imbalance might lead to model bias favoring the majority class.")
#         print("Actionable Recommendations: Consider using SMOTE, class weighting, or other resampling techniques to balance the classes.")
#     else:
#         print("High-Level Overview: The 'HasDetections' feature is relatively balanced.")
#         print("Detailed Technical Insights: The balanced distribution is beneficial for model training as it reduces the risk of bias towards one class.")
#         print("Actionable Recommendations: Proceed with model development without additional steps for handling class imbalance at this stage.")




# def missing_values_analysis(dataframe, save_dir=None):
#     missing_values = dataframe.isnull().sum().to_dict()
#     missing_percentage = (dataframe.isnull().sum() / dataframe.shape[0] * 100).to_dict()
#     missing_data = {feature: {'count': missing_values[feature], 'percentage': missing_percentage[feature]} for feature in missing_values}
#     if save_dir:
#         save_to_json(missing_data, os.path.join(save_dir, 'missing_values.json'))
#     print("\n==== Missing Values Analysis ====\n")
#     missing_data_sorted = pd.DataFrame(missing_data).T.sort_values(by='count', ascending=False)
#     plt.figure(figsize=(12, 6))
#     sns.barplot(x=missing_data_sorted.index, y=missing_data_sorted['count'])
#     plt.xticks(rotation=90)
#     plt.title('Missing Values per Column')
#     plt.ylabel('Number of Missing Values')
#     plt.xlabel('Feature')
#     plt.show()
#     print("Missing Values in Train Sample:")
#     print(missing_data_sorted[missing_data_sorted['count'] > 0])
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: Several features have a high percentage of missing values, particularly 'PuaMode' and 'Census_ProcessorClass'.")
#     print("Detailed Technical Insights: Features with a high percentage of missing values might need imputation strategies or could be excluded from the analysis if deemed irrelevant.")
#     print("Actionable Recommendations: Develop a missing value treatment plan, focusing on imputation strategies or feature exclusion based on the importance and impact on the target variable.")

# def feature_balance_analysis(dataframe, save_dir=None):
#     feature_balance = {col: {'most_common_value_weight': dataframe[col].value_counts(normalize=True).max() * 100} for col in dataframe.columns}
#     if save_dir:
#         save_to_json(feature_balance, os.path.join(save_dir, 'feature_balance.json'))
#     print("\n==== Feature Balance Analysis ====\n")
#     feature_balance_sorted = pd.Series({col: bal['most_common_value_weight'] for col, bal in feature_balance.items()}).sort_values(ascending=False)
#     plt.figure(figsize=(12, 6))
#     sns.barplot(x=feature_balance_sorted.index, y=feature_balance_sorted.values)
#     plt.xticks(rotation=90)
#     plt.title('Feature Balance (Percentage of Highest Value)')
#     plt.ylabel('Percentage')
#     plt.xlabel('Feature')
#     plt.show()
#     print("Feature Balance in Train Sample:")
#     print(feature_balance_sorted)
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: Many features show a high percentage of a single value, indicating potential imbalance.")
#     print("Detailed Technical Insights: Imbalanced features can impact the performance of machine learning models, particularly in classification tasks.")
#     print("Actionable Recommendations: Consider techniques to handle imbalanced features, such as resampling, feature engineering, or model adjustments.")

# def correlation_analysis(dataframe, save_dir=None, pair_threshold=0.2):
#     numerical_columns = dataframe.select_dtypes(include=['int64', 'float64']).columns
#     correlation_matrix = dataframe[numerical_columns].corr()
#     high_correlation_pairs = correlation_matrix.unstack().sort_values(kind="quicksort", ascending=False).drop_duplicates()
#     high_correlation_pairs = high_correlation_pairs[(high_correlation_pairs.abs() > pair_threshold) & (high_correlation_pairs != 1)]
#     high_correlation_dict = {str(k): {'correlation': v} for k, v in high_correlation_pairs.to_dict().items()}

#     if save_dir:
#         save_to_json(correlation_matrix.to_dict(), os.path.join(save_dir, 'correlations.json'))
#         save_to_json(high_correlation_dict, os.path.join(save_dir, 'high_correlation_pairs.json'))

#     print("\n==== Correlation Analysis ====\n")
#     plt.figure(figsize=(15, 10))
#     sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')
#     plt.title('Correlation Matrix')
#     plt.show()
#     print("\nHigh Correlation Pairs (absolute value > {} or < -{}):\n".format(pair_threshold, pair_threshold))
#     print(high_correlation_pairs)
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: Several feature pairs show high correlation (absolute value > {} or < -{}).".format(pair_threshold, pair_threshold))
#     print("Detailed Technical Insights: Highly correlated features can indicate multicollinearity, which may affect model performance.")
#     print("Actionable Recommendations: Consider removing or combining highly correlated features to reduce redundancy and multicollinearity.")

#     # Correlation with target variable
#     target_corr = correlation_matrix['HasDetections'].drop('HasDetections').sort_values(ascending=False)
#     target_corr_dict = target_corr.to_dict()
    
#     if save_dir:
#         save_to_json(target_corr_dict, os.path.join(save_dir, 'target_correlations.json'))

#     print("\n==== Correlation with Target Variable 'HasDetections' ====\n")
#     plt.figure(figsize=(12, 8))
#     sns.barplot(x=target_corr.values, y=target_corr.index, hue=target_corr.index, dodge=False, palette='coolwarm', legend=False)
#     plt.title("Correlation with 'HasDetections'")
#     plt.xlabel("Correlation coefficient")
#     plt.ylabel("Features")
#     plt.show()
#     print("\nCorrelation with 'HasDetections':\n", target_corr)
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: Some features show strong correlation with the target variable 'HasDetections'.")
#     print("Detailed Technical Insights: Identifying these features can help in feature selection and engineering.")
#     print("Actionable Recommendations: Consider these features for model development and further analysis.")

#     return high_correlation_pairs, target_corr


# def feature_interactions(dataframe, save_dir=None):
#     # Placeholder for future implementation
#     if save_dir:
#         save_to_json({}, os.path.join(save_dir, 'feature_interactions.json'))
#     print("\n==== Feature Interactions ====\n")
#     sns.pairplot(dataframe, vars=[
#         'Census_OSInstallLanguageIdentifier', 'Census_OSUILocaleIdentifier', 
#         'Census_OSBuildNumber', 'OsBuild', 'IsSxsPassiveMode', 'RtpStateBitfield', 
#         'Census_InternalPrimaryDisplayResolutionHorizontal', 
#         'Census_InternalPrimaryDisplayResolutionVertical'
#     ])
#     plt.show()
#     print("\n--- Contextual Insights and Recommendations ---\n")
#     print("High-Level Overview: The pair plots visualize relationships between highly correlated features.")
#     print("Detailed Technical Insights: Observing feature interactions can provide insights into underlying patterns and relationships in the data.")
#     print("Actionable Recommendations: Use these insights for feature engineering or model selection, leveraging relationships between features.")

