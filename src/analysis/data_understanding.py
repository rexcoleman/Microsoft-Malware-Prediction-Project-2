# src/analysis/data_understanding.py

import pandas as pd
import os
import logging
import seaborn as sns
import matplotlib.pyplot as plt
from config_loader import load_config, update_imputation_strategies
from utils import save_analysis_results
from feature_engineering.imputation import suggest_imputation_strategy, handle_missing_values

logger = logging.getLogger(__name__)

def generate_summary_statistics(df):
    """
    Generate summary statistics for the given dataframe.

    Parameters:
    - df (DataFrame): The dataframe to analyze.

    Returns:
    - DataFrame: Summary statistics.
    """
    summary_stats = df.describe(include='all').transpose()
    summary_stats['missing_values'] = df.isnull().sum()
    summary_stats['missing_percentage'] = (df.isnull().sum() / len(df)) * 100
    return summary_stats

def save_summary_statistics(summary_stats, save_path):
    """
    Save summary statistics as a CSV file and an image.

    Parameters:
    - summary_stats (DataFrame): The summary statistics to save.
    - save_path (str): The path to save the image.
    """
    summary_stats.to_csv(save_path.replace('.png', '.csv'))
    fig, ax = plt.subplots(figsize=(20, 10))  # Adjust size as necessary
    sns.heatmap(summary_stats.isnull(), cbar=False, ax=ax, cmap='viridis')
    plt.savefig(save_path)
    plt.close()

def plot_correlation_heatmap(df, save_path=None):
    """
    Plot a correlation heatmap for the dataframe.

    Parameters:
    - df (DataFrame): The dataframe to analyze.
    - save_path (str): The path to save the heatmap image.

    Returns:
    - DataFrame: Correlation matrix.
    """
    plt.figure(figsize=(20, 16))
    
    # Select only numeric columns for correlation analysis
    numeric_df = df.select_dtypes(include=['float64', 'int64', 'float32', 'int32'])
    
    # Calculate the correlation matrix
    corr_matrix = numeric_df.corr()
    
    # Plot the heatmap
    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')
    plt.title('Correlation Matrix')
    
    if save_path:
        plt.savefig(save_path)
    
    plt.show()
    return corr_matrix

def initial_data_analysis(train_chunk, test_chunk, config):
    try:
        report_dir = config['report_dir']
        if not os.path.exists(report_dir):
            os.makedirs(report_dir)

        # Generate summary statistics for train and test samples
        logger.info("Generating summary statistics for training data.")
        train_summary_stats = generate_summary_statistics(train_chunk)
        save_summary_statistics(train_summary_stats, os.path.join(report_dir, 'train_summary_statistics.png'))

        logger.info("Generating summary statistics for test data.")
        test_summary_stats = generate_summary_statistics(test_chunk)
        save_summary_statistics(test_summary_stats, os.path.join(report_dir, 'test_summary_statistics.png'))

        # Print summary statistics
        logger.info("Train Summary Statistics:")
        print(train_summary_stats)

        logger.info("Test Summary Statistics:")
        print(test_summary_stats)

        # Correlation Analysis
        logger.info("Performing correlation analysis.")
        corr_matrix = plot_correlation_heatmap(train_chunk, save_path=os.path.join(report_dir, 'correlation_heatmap.png'))

        # Suggest imputation strategies
        logger.info("Suggesting imputation strategies.")
        imputation_strategies = suggest_imputation_strategy(train_summary_stats, corr_matrix)

        # Handle missing values based on suggested imputation strategies
        logger.info("Handling missing values.")
        train_chunk = handle_missing_values(train_chunk, imputation_strategies)
        test_chunk = handle_missing_values(test_chunk, imputation_strategies)

        # Save analysis results
        save_analysis_results(train_summary_stats, test_summary_stats, imputation_strategies, report_dir)

        # Update and save imputation strategies to config file
        updated_config = update_imputation_strategies(imputation_strategies, config_path=config['config_paths']['imputation_strategies'])
        logger.info(f"Updated Imputation Strategies: {updated_config}")

        logger.info("Initial data analysis completed successfully.")
        return train_chunk, test_chunk

    except Exception as e:
        logger.error(f"An error occurred during initial data analysis: {e}")
        raise

def save_cleaned_data(train_chunk, test_chunk, config):
    train_cleaned_path = config['data_paths']['train_cleaned']
    test_cleaned_path = config['data_paths']['test_cleaned']

    train_chunk.to_csv(train_cleaned_path, index=False)
    test_chunk.to_csv(test_cleaned_path, index=False)

    logger.info(f"Cleaned training data saved to {train_cleaned_path}")
    logger.info(f"Cleaned test data saved to {test_cleaned_path}")

def ensure_directories_exist(config):
    report_dir = config['report_dir']
    figures_dir = os.path.join(report_dir, 'figures')
    if not os.path.exists(report_dir):
        os.makedirs(report_dir)
    if not os.path.exists(figures_dir):
        os.makedirs(figures_dir)








# # src/analysis/data_understanding.py

# import pandas as pd
# import os
# import logging
# from config_loader import load_config, update_imputation_strategies
# from utils import save_analysis_results
# from feature_engineering.imputation import suggest_imputation_strategy, handle_missing_values
# from visualization.visualize import plot_correlation_heatmap, plot_missing_values
# import matplotlib.pyplot as plt
# import seaborn as sns

# logger = logging.getLogger(__name__)

# def generate_summary_statistics(df):
#     """
#     Generate summary statistics for the given dataframe.

#     Parameters:
#     - df (DataFrame): The dataframe to analyze.

#     Returns:
#     - DataFrame: Summary statistics.
#     """
#     summary_stats = df.describe(include='all').transpose()
#     summary_stats['missing_values'] = df.isnull().sum()
#     summary_stats['missing_percentage'] = (df.isnull().sum() / len(df)) * 100
#     return summary_stats

# def save_summary_statistics(summary_stats, save_path):
#     """
#     Save summary statistics as a CSV file.

#     Parameters:
#     - summary_stats (DataFrame): The summary statistics to save.
#     - save_path (str): The path to save the CSV file.
#     """
#     summary_stats.to_csv(save_path.replace('.png', '.csv'))

# def plot_correlation_heatmap(df, target_col='HasDetections', save_path=None):
#     """
#     Plot a correlation heatmap for the dataframe.

#     Parameters:
#     - df (DataFrame): The dataframe to analyze.
#     - target_col (str): The target column for correlation analysis.
#     - save_path (str): The path to save the heatmap image.

#     Returns:
#     - DataFrame: Correlation matrix.
#     """
#     plt.figure(figsize=(12, 8))
    
#     # Select only numeric columns for correlation analysis
#     numeric_df = df.select_dtypes(include=['float64', 'int64', 'float32', 'int32'])
    
#     # Calculate the correlation matrix
#     corr_matrix = numeric_df.corr()
    
#     # Plot the heatmap
#     sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')
#     plt.title('Correlation Matrix')
    
#     if save_path:
#         plt.savefig(save_path)
    
#     plt.show()
#     return corr_matrix

# def plot_missing_values(df, save_path):
#     """
#     Plot a heatmap of missing values for the dataframe.

#     Parameters:
#     - df (DataFrame): The dataframe to analyze.
#     - save_path (str): The path to save the heatmap image.
#     """
#     plt.figure(figsize=(12, 8))
#     sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
#     plt.title('Missing Values Heatmap')
#     plt.savefig(save_path)
#     plt.close()

# def initial_data_analysis(train_chunk, test_chunk, config):
#     try:
#         ensure_directories_exist(config)
#         report_dir = config['report_dir']

#         # Generate and save summary statistics for train and test samples
#         logger.info("Generating summary statistics for training data.")
#         train_summary_stats = generate_summary_statistics(train_chunk)
#         save_summary_statistics(train_summary_stats, os.path.join(report_dir, 'train_summary_statistics.csv'))

#         logger.info("Generating summary statistics for test data.")
#         test_summary_stats = generate_summary_statistics(test_chunk)
#         save_summary_statistics(test_summary_stats, os.path.join(report_dir, 'test_summary_statistics.csv'))

#         logger.info("Train Summary Statistics:")
#         print(train_summary_stats)

#         logger.info("Test Summary Statistics:")
#         print(test_summary_stats)

#         # Correlation Analysis
#         logger.info("Performing correlation analysis.")
#         corr_matrix = plot_correlation_heatmap(train_chunk, save_path=os.path.join(report_dir, 'correlation_heatmap.png'))

#         # Missing Values Analysis
#         logger.info("Performing missing values analysis.")
#         plot_missing_values(train_chunk, os.path.join(report_dir, 'train_missing_values.png'))
#         plot_missing_values(test_chunk, os.path.join(report_dir, 'test_missing_values.png'))

#         # Suggest imputation strategies
#         logger.info("Suggesting imputation strategies.")
#         imputation_strategies = suggest_imputation_strategy(train_summary_stats, corr_matrix)

#         # Handle missing values based on suggested imputation strategies
#         logger.info("Handling missing values.")
#         train_chunk = handle_missing_values(train_chunk, imputation_strategies)
#         test_chunk = handle_missing_values(test_chunk, imputation_strategies)

#         # Save analysis results
#         save_analysis_results(train_summary_stats, test_summary_stats, imputation_strategies, report_dir)

#         # Update and save imputation strategies to config file
#         updated_config = update_imputation_strategies(imputation_strategies, config_path=config['config_paths']['imputation_strategies'])
#         logger.info(f"Updated Imputation Strategies: {updated_config}")

#         logger.info("Initial data analysis completed successfully.")
#         return train_chunk, test_chunk

#     except Exception as e:
#         logger.error(f"An error occurred during initial data analysis: {e}")
#         raise

# def save_cleaned_data(train_chunk, test_chunk, config):
#     train_cleaned_path = config['data_paths']['train_cleaned']
#     test_cleaned_path = config['data_paths']['test_cleaned']

#     train_chunk.to_csv(train_cleaned_path, index=False)
#     test_chunk.to_csv(test_cleaned_path, index=False)

#     logger.info(f"Cleaned training data saved to {train_cleaned_path}")
#     logger.info(f"Cleaned test data saved to {test_cleaned_path}")

# def ensure_directories_exist(config):
#     report_dir = config['report_dir']
#     figures_dir = os.path.join(report_dir, 'figures')
#     if not os.path.exists(report_dir):
#         os.makedirs(report_dir)
#     if not os.path.exists(figures_dir):
#         os.makedirs(figures_dir)








# # # src/analysis/data_understanding.py

# # import pandas as pd
# # import os
# # import logging
# # import seaborn as sns
# # import matplotlib.pyplot as plt
# # from config_loader import load_config, update_imputation_strategies
# # from utils import save_analysis_results
# # from feature_engineering.imputation import suggest_imputation_strategy, handle_missing_values
# # from visualization.visualize import plot_correlation_heatmap

# # logger = logging.getLogger(__name__)

# # def generate_summary_statistics(df):
# #     """
# #     Generate summary statistics for the given dataframe.

# #     Parameters:
# #     - df (DataFrame): The dataframe to analyze.

# #     Returns:
# #     - DataFrame: Summary statistics.
# #     """
# #     summary_stats = df.describe(include='all').transpose()
# #     summary_stats['missing_values'] = df.isnull().sum()
# #     summary_stats['missing_percentage'] = (df.isnull().sum() / len(df)) * 100
# #     return summary_stats

# # def save_summary_statistics(summary_stats, save_path):
# #     """
# #     Save summary statistics as a CSV file and an image.

# #     Parameters:
# #     - summary_stats (DataFrame): The summary statistics to save.
# #     - save_path (str): The path to save the image.
# #     """
# #     summary_stats.to_csv(save_path.replace('.png', '.csv'))
# #     fig, ax = plt.subplots(figsize=(10, 8))
# #     sns.heatmap(summary_stats.isnull(), cbar=False, ax=ax)
# #     plt.savefig(save_path)
# #     plt.close()

# # def initial_data_analysis(train_chunk, test_chunk, config):
# #     try:
# #         # Ensure directories exist
# #         ensure_directories_exist(config)

# #         report_dir = config['report_dir']

# #         # Generate summary statistics for train and test samples
# #         logger.info("Generating summary statistics for training data.")
# #         train_summary_stats = generate_summary_statistics(train_chunk)
# #         save_summary_statistics(train_summary_stats, os.path.join(report_dir, 'train_summary_statistics.png'))

# #         logger.info("Generating summary statistics for test data.")
# #         test_summary_stats = generate_summary_statistics(test_chunk)
# #         save_summary_statistics(test_summary_stats, os.path.join(report_dir, 'test_summary_statistics.png'))

# #         # Print summary statistics
# #         logger.info("Train Summary Statistics:")
# #         print(train_summary_stats)

# #         logger.info("Test Summary Statistics:")
# #         print(test_summary_stats)

# #         # Correlation Analysis
# #         logger.info("Performing correlation analysis.")
# #         corr_matrix = plot_correlation_heatmap(train_chunk, save_path=os.path.join(report_dir, 'correlation_heatmap.png'))

# #         # Suggest imputation strategies
# #         logger.info("Suggesting imputation strategies.")
# #         imputation_strategies = suggest_imputation_strategy(train_summary_stats, corr_matrix)

# #         # Handle missing values based on suggested imputation strategies
# #         logger.info("Handling missing values.")
# #         train_chunk = handle_missing_values(train_chunk, imputation_strategies)
# #         test_chunk = handle_missing_values(test_chunk, imputation_strategies)

# #         # Save analysis results
# #         save_analysis_results(train_summary_stats, test_summary_stats, imputation_strategies, report_dir)

# #         # Update and save imputation strategies to config file
# #         updated_config = update_imputation_strategies(imputation_strategies, config_path=config['config_paths']['imputation_strategies'])
# #         logger.info(f"Updated Imputation Strategies: {updated_config}")

# #         logger.info("Initial data analysis completed successfully.")
# #         return train_chunk, test_chunk

# #     except Exception as e:
# #         logger.error(f"An error occurred during initial data analysis: {e}")
# #         raise

# # def save_cleaned_data(train_chunk, test_chunk, config):
# #     train_cleaned_path = config['data_paths']['train_cleaned']
# #     test_cleaned_path = config['data_paths']['test_cleaned']

# #     train_chunk.to_csv(train_cleaned_path, index=False)
# #     test_chunk.to_csv(test_cleaned_path, index=False)

# #     logger.info(f"Cleaned training data saved to {train_cleaned_path}")
# #     logger.info(f"Cleaned test data saved to {test_cleaned_path}")

# # def ensure_directories_exist(config):
# #     report_dir = config['report_dir']
# #     figures_dir = os.path.join(report_dir, 'figures')
# #     if not os.path.exists(report_dir):
# #         os.makedirs(report_dir)
# #     if not os.path.exists(figures_dir):
# #         os.makedirs(figures_dir)














# # import pandas as pd
# # import os
# # import logging
# # from config_loader import load_config, update_imputation_strategies
# # from utils import save_analysis_results
# # from feature_engineering.imputation import suggest_imputation_strategy, handle_missing_values
# # from visualization.visualize import plot_correlation_heatmap

# # logger = logging.getLogger(__name__)

# # def generate_summary_statistics(df):
# #     """
# #     Generate summary statistics for the dataframe.
# #     """
# #     summary_stats = df.describe(include='all').transpose()
# #     summary_stats['missing_values'] = df.isnull().sum()
# #     summary_stats['missing_percentage'] = (summary_stats['missing_values'] / len(df)) * 100
# #     summary_stats['dtype'] = df.dtypes
# #     return summary_stats

# # def save_summary_statistics(summary_stats, filename):
# #     summary_stats.to_csv(filename)

# # def save_missing_values_table(summary_stats, filename):
# #     missing_values_table = summary_stats[['missing_values', 'missing_percentage']]
# #     missing_values_table.to_csv(filename)

# # def save_imputation_strategies(imputation_strategies, filename):
# #     imputation_strategies_df = pd.DataFrame(list(imputation_strategies['imputation_strategies'].items()), columns=['Feature', 'Imputation Strategy'])
# #     imputation_strategies_df.to_csv(filename, index=False)

# # def initial_data_analysis(train_chunk, test_chunk, config):
# #     try:
# #         # Generate summary statistics for train and test samples
# #         logger.info("Generating summary statistics for training data.")
# #         train_summary_stats = generate_summary_statistics(train_chunk)
# #         logger.info("Generating summary statistics for test data.")
# #         test_summary_stats = generate_summary_statistics(test_chunk)

# #         # Save summary statistics
# #         save_summary_statistics(train_summary_stats, '../reports/figures/train_summary_statistics.csv')
# #         save_summary_statistics(test_summary_stats, '../reports/figures/test_summary_statistics.csv')

# #         # Print summary statistics
# #         logger.info("Train Summary Statistics:")
# #         print(train_summary_stats)

# #         logger.info("Test Summary Statistics:")
# #         print(test_summary_stats)

# #         # Correlation Analysis
# #         logger.info("Performing correlation analysis.")
# #         corr_matrix = plot_correlation_heatmap(train_chunk, target_col='HasDetections')

# #         # Suggest imputation strategies
# #         logger.info("Suggesting imputation strategies.")
# #         imputation_strategies = suggest_imputation_strategy(train_summary_stats, corr_matrix, target_col='HasDetections')

# #         # Handle missing values based on suggested imputation strategies
# #         logger.info("Handling missing values.")
# #         train_chunk = handle_missing_values(train_chunk, imputation_strategies)
# #         test_chunk = handle_missing_values(test_chunk, imputation_strategies)

# #         # Save missing values table
# #         save_missing_values_table(train_summary_stats, '../reports/figures/train_missing_values.csv')
# #         save_missing_values_table(test_summary_stats, '../reports/figures/test_missing_values.csv')

# #         # Save imputation strategies table
# #         save_imputation_strategies(imputation_strategies, '../reports/figures/imputation_strategies.csv')

# #         # Save analysis results
# #         report_dir = config['report_dir']
# #         if not os.path.exists(report_dir):
# #             os.makedirs(report_dir)
# #         save_analysis_results(train_summary_stats, test_summary_stats, imputation_strategies, report_dir)

# #         # Update and save imputation strategies to config file
# #         updated_config = update_imputation_strategies(imputation_strategies, config_path=config['config_paths']['imputation_strategies'])
# #         logger.info(f"Updated Imputation Strategies: {updated_config}")

# #         logger.info("Initial data analysis completed successfully.")

# #         return train_chunk, test_chunk

# #     except Exception as e:
# #         logger.error(f"An error occurred during initial data analysis: {e}")
# #         raise

# # def save_cleaned_data(train_chunk, test_chunk, config):
# #     train_cleaned_path = config['data_paths']['train_cleaned']
# #     test_cleaned_path = config['data_paths']['test_cleaned']
    
# #     train_chunk.to_csv(train_cleaned_path, index=False)
# #     test_chunk.to_csv(test_cleaned_path, index=False)
    
# #     logger.info(f"Cleaned training data saved to {train_cleaned_path}")
# #     logger.info(f"Cleaned test data saved to {test_cleaned_path}")












