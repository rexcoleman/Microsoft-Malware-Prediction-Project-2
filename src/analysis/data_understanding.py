# src/analysis/data_understanding.py

import os
import json
from IPython.display import display, HTML
from typing import Dict, Optional
import logging
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from src.visualization.plot_utils import plot_bar, plot_heatmap, plot_correlation 
from src.utils.directory_utils import ensure_directories_exist
from src.utils.file_operations import save_analysis_results, save_json_file
from src.utils.json_pipeline import save_json_with_pipeline
from src.config_loader import load_paths, load_config, update_imputation_strategies
from src.utils.display_utils import display_dataframe_as_html, display_side_by_side, display_width_compact_dataframe

logger = logging.getLogger(__name__)


def data_overview(train_sample: pd.DataFrame, test_sample: pd.DataFrame, save_dir: str = None) -> None:
    """
    Provides an overview of the training and test datasets, including shapes and column names.

    Parameters:
    train_sample (pd.DataFrame): The training sample DataFrame.
    test_sample (pd.DataFrame): The test sample DataFrame.
    save_dir (str, optional): Directory to save the overview JSON file.

    Raises:
    Exception: If there is an error during data overview processing.
    """
    try:
        overview = generate_overview(train_sample, test_sample)
        if save_dir:
            save_overview(overview, save_dir)
        display_overview(overview)
    except Exception as e:
        logger.error("Error in data overview: %s", e)
        raise

def generate_overview(train_sample: pd.DataFrame, test_sample: pd.DataFrame) -> dict:
    """
    Generates a summary of the training and test datasets.

    Parameters:
    train_sample (pd.DataFrame): The training sample DataFrame.
    test_sample (pd.DataFrame): The test sample DataFrame.

    Returns:
    dict: A dictionary containing the shapes and column names of the datasets.
    """
    return {
        "data_overview": {
            "train_shape": list(train_sample.shape),
            "test_shape": list(test_sample.shape),
            "train_columns": list(train_sample.columns),
            "test_columns": list(test_sample.columns),
        }
    }

def save_overview(overview: dict, save_dir: str) -> None:
    """
    Saves the overview summary to a JSON file using the JSON pipeline.

    Parameters:
    overview (dict): The overview summary.
    save_dir (str): Directory to save the JSON file.
    """
    try:
        os.makedirs(save_dir, exist_ok=True)
        # Use the pipeline to save the JSON file with all preprocessing steps applied
        save_json_with_pipeline(overview, os.path.join(save_dir, 'data_overview.json'))
    except Exception as e:
        logger.error("Error saving data overview: %s", e)
        raise

def display_overview(overview: dict) -> None:
    """
    Displays the overview summary in a clean table format.

    Parameters:
    overview (dict): The overview summary.
    """
    try:
        train_overview_df = pd.DataFrame(
            overview["data_overview"]["train_columns"], columns=["Train Columns"]
        ).reset_index(drop=True)
        test_overview_df = pd.DataFrame(
            overview["data_overview"]["test_columns"], columns=["Test Columns"]
        ).reset_index(drop=True)

        display_side_by_side(
            train_overview_df,
            test_overview_df,
            title1="Train Data Overview",
            title2="Test Data Overview",
            description1=f"Train Data Shape: {overview['data_overview']['train_shape']}",
            description2=f"Test Data Shape: {overview['data_overview']['test_shape']}",
            width="45%"
        )
    except KeyError as e:
        logger.error("KeyError in display_overview: %s", e)
        raise
    except Exception as e:
        logger.error("Error in display_overview: %s", e)
        raise


def data_types_and_formats(dataframe: pd.DataFrame, save_dir: str = None, table_width: str = "70%") -> None:
    """
    Analyzes and documents the data types and formats in the dataframe.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame to analyze.
    save_dir (str, optional): Directory to save the data types JSON file.
    table_width (str): Width of the table to display.

    Raises:
    Exception: If there is an error during the data types analysis.
    """
    try:
        data_types = analyze_data_types(dataframe)
        if save_dir:
            save_data_types(data_types, save_dir)
        display_data_types(data_types, table_width)
    except Exception as e:
        logger.error("Error in data types and formats analysis: %s", e)
        raise


def analyze_data_types(dataframe: pd.DataFrame) -> dict:
    """
    Analyzes the data types of the dataframe columns.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame to analyze.

    Returns:
    dict: A dictionary structured according to the feature_metadata_complete_schema.json for data types.
    """
    data_types = dataframe.dtypes.apply(lambda x: x.name).to_dict()

    return {
        "features": {
            column: {
                "general_attributes": {
                    "technical_data_type": dtype
                }
            }
            for column, dtype in data_types.items()
        }
    }


def save_data_types(data_types: dict, save_dir: str) -> None:
    """
    Saves the data types information to a JSON file.

    Parameters:
    data_types (dict): The data types information.
    save_dir (str): Directory to save the JSON file.

    Raises:
    Exception: If there is an error saving the JSON file.
    """
    try:
        os.makedirs(save_dir, exist_ok=True)
        save_json_with_pipeline(data_types, os.path.join(save_dir, 'data_types.json'))
    except Exception as e:
        logger.error("Error saving data types: %s", e)
        raise


def display_data_types(data_types: dict, table_width: str) -> None:
    """
    Displays the data types of the dataframe columns in a clean table format.

    Parameters:
    data_types (dict): The data types information.
    table_width (str): Width of the table to display.

    Raises:
    Exception: If there is an error displaying the data types.
    """
    try:
        flat_data_types = {
            feature: attrs["general_attributes"]["technical_data_type"]
            for feature, attrs in data_types["features"].items()
        }

        data_types_df = pd.DataFrame(
            list(flat_data_types.items()), columns=['Feature', 'Data Type']
        ).reset_index(drop=True)
        display_dataframe_as_html(data_types_df, title="Data Types and Formats", description="")

        print("\n--- Contextual Insights and Recommendations ---\n")
        print("High-Level Overview: The dataset includes multiple data types such as integers, floats, and objects.")
        print("Detailed Technical Insights: Ensuring data types are correctly assigned is crucial for accurate analysis and modeling.")
        print("Actionable Recommendations: Verify and correct data types if necessary before further analysis.")
    except Exception as e:
        logger.error("Error displaying data types: %s", e)
        raise


def statistical_summary(dataframe: pd.DataFrame, save_dir: str = None) -> None:
    """
    Generates and saves a statistical summary of the dataframe aligned with the schema.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame to analyze.
    save_dir (str, optional): Directory to save the statistical summary JSON file.

    Raises:
    Exception: If there is an error during the statistical summary generation or saving.
    """
    try:
        summary_dict = generate_statistical_summary_dict(dataframe)
        summary_df = generate_statistical_summary_df(dataframe)

        if save_dir:
            save_statistical_summary(summary_dict, save_dir)

        display_statistical_summary(summary_df)
    except Exception as e:
        logger.error("Error in statistical summary: %s", e)
        raise


def generate_statistical_summary_dict(dataframe: pd.DataFrame) -> dict:
    """
    Generates descriptive statistics for the dataframe in dictionary format, consistent with the schema.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame to analyze.

    Returns:
    dict: A dictionary containing the descriptive statistics, structured according to the schema.
    """
    stats = dataframe.describe(include='all').to_dict()
    summary_dict = {
        "features": {}
    }

    for column, stat in stats.items():
        summary_dict["features"][column] = {
            "additional_attributes": {
                "summary_statistics": {
                    "count": stat.get("count", None),
                    "mean": stat.get("mean", None),
                    "std": stat.get("std", None),
                    "min": stat.get("min", None),
                    "25%": stat.get("25%", None),
                    "50%": stat.get("50%", None),
                    "75%": stat.get("75%", None),
                    "max": stat.get("max", None),
                }
            }
        }

    return summary_dict


def generate_statistical_summary_df(dataframe: pd.DataFrame) -> pd.DataFrame:
    """
    Generates descriptive statistics for the dataframe in DataFrame format.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame to analyze.

    Returns:
    pd.DataFrame: A DataFrame containing the descriptive statistics.
    """
    return dataframe.describe(include='all').transpose().reset_index().rename(columns={'index': 'Feature'})


def save_statistical_summary(summary: dict, save_dir: str) -> None:
    """
    Saves the statistical summary to a JSON file.

    Parameters:
    summary (dict): The statistical summary.
    save_dir (str): Directory to save the JSON file.

    Raises:
    Exception: If there is an error saving the JSON file.
    """
    try:
        os.makedirs(save_dir, exist_ok=True)
        # Use the pipeline to save the JSON file with all preprocessing steps applied
        save_json_with_pipeline(summary, os.path.join(save_dir, 'stat_summary.json'))
    except Exception as e:
        logger.error("Error saving statistical summary: %s", e)
        raise


def display_statistical_summary(summary_df: pd.DataFrame) -> None:
    """
    Displays the statistical summary of the dataframe.

    Parameters:
    summary_df (pd.DataFrame): The statistical summary DataFrame.

    Raises:
    Exception: If there is an error displaying the statistical summary.
    """
    display_dataframe_as_html(
        summary_df, title="Statistical Summary", description="Descriptive statistics of the dataset."
    )


def target_variable_analysis(
    dataframe: pd.DataFrame, 
    target_column: str = 'HasDetections', 
    save_dir: Optional[str] = None
) -> None:
    """
    Perform analysis on the target variable and save the results.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame containing the target variable.
    target_column (str): The name of the target variable column.
    save_dir (Optional[str]): Directory to save the analysis results.

    Raises:
    Exception: If there is an error during the analysis.
    """
    try:
        target_summary = get_target_summary(dataframe, target_column)
        if save_dir:
            save_target_summary(target_summary, save_dir)
        print_target_analysis(dataframe, target_column, target_summary)
    except Exception as e:
        logger.error(f"Error in target variable analysis: {e}")
        raise


def get_target_summary(dataframe: pd.DataFrame, target_column: str) -> Dict[str, float]:
    """
    Get the summary of the target variable.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame containing the target variable.
    target_column (str): The name of the target variable column.

    Returns:
    Dict[str, float]: A dictionary containing the normalized value counts of the target variable.
    """
    return dataframe[target_column].value_counts(normalize=True).to_dict()


def save_target_summary(target_summary: Dict[str, float], save_dir: str) -> None:
    """
    Save the target variable summary to a JSON file.

    Parameters:
    target_summary (Dict[str, float]): The summary of the target variable.
    save_dir (str): Directory to save the JSON file.

    Raises:
    Exception: If there is an error saving the JSON file.
    """
    try:
        structured_summary = {
            "data_overview": {
                "target_variable_analysis": target_summary
            }
        }
        os.makedirs(save_dir, exist_ok=True)
        save_json_with_pipeline(structured_summary, os.path.join(save_dir, 'target_analysis.json'))
    except Exception as e:
        logger.error(f"Error saving target summary: {e}")
        raise


def print_target_analysis(
    dataframe: pd.DataFrame, 
    target_column: str, 
    target_summary: Dict[str, float]
) -> None:
    """
    Print the target variable analysis and plot the distribution.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame containing the target variable.
    target_column (str): The name of the target variable column.
    target_summary (Dict[str, float]): The summary of the target variable.

    Raises:
    Exception: If there is an error printing the analysis.
    """
    try:
        print(f"\n==== Target Variable Analysis: '{target_column}' ====\n")
        plot_target_distribution(dataframe, target_column)
        detection_counts = dataframe[target_column].value_counts(normalize=True)
        print(f"Distribution of '{target_column}':\n{detection_counts}")
        print("\n--- Contextual Insights and Recommendations ---\n")
        if detection_counts[1] > 0.6 or detection_counts[0] > 0.6:
            print("High-Level Overview: There is a class imbalance in the target feature.")
            print("Detailed Technical Insights: The imbalance might lead to model bias favoring the majority class.")
            print("Actionable Recommendations: Consider using SMOTE, class weighting, or other resampling techniques to balance the classes.")
        else:
            print("High-Level Overview: The target feature is relatively balanced.")
            print("Detailed Technical Insights: The balanced distribution is beneficial for model training as it reduces the risk of bias towards one class.")
            print("Actionable Recommendations: Proceed with model development without additional steps for handling class imbalance at this stage.")
    except Exception as e:
        logger.error(f"Error printing target analysis: {e}")
        raise


def plot_target_distribution(dataframe: pd.DataFrame, target_column: str) -> None:
    """
    Plot the distribution of the target variable.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame containing the target variable.
    target_column (str): The name of the target variable column.

    Raises:
    Exception: If there is an error plotting the distribution.
    """
    try:
        plt.figure(figsize=(6, 4))
        ax = sns.countplot(x=target_column, data=dataframe)
        plt.title('Distribution of Target Variable')
        plt.xlabel(f'{target_column} Value')
        plt.ylabel('Count')
        total = len(dataframe[target_column].dropna())
        for p in ax.patches:
            height = p.get_height()
            ax.text(p.get_x() + p.get_width() / 2., height + 3, '{:1.2f}%'.format(100 * height / total), ha="center") 
        plt.show()
    except Exception as e:
        logger.error(f"Error plotting target distribution: {e}")
        raise


def missing_values_analysis(
    dataframe: pd.DataFrame, 
    save_dir: Optional[str] = None
) -> None:
    """
    Perform analysis on missing values in the dataframe and save the results.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame to analyze for missing values.
    save_dir (Optional[str]): Directory to save the analysis results.

    Raises:
    Exception: If there is an error during the analysis.
    """
    try:
        missing_data = get_missing_values(dataframe)
        if save_dir:
            save_missing_values(missing_data, save_dir)
        display_missing_values_analysis(missing_data)
        plot_missing_values(missing_data, save_dir)
        print("\n--- Contextual Insights and Recommendations ---\n")
        print(
            "High-Level Overview: Several features have a high percentage of "
            "missing values, particularly 'PuaMode' and 'Census_ProcessorClass'."
        )
        print(
            "Detailed Technical Insights: Features with a high percentage of missing "
            "values might need imputation strategies or could be excluded from the "
            "analysis if deemed irrelevant."
        )
        print(
            "Actionable Recommendations: Develop a missing value treatment plan, "
            "focusing on imputation strategies or feature exclusion based on the "
            "importance and impact on the target variable."
        )
    except Exception as e:
        logger.error(f"Error in missing values analysis: {e}")
        raise

def get_missing_values(dataframe: pd.DataFrame) -> Dict[str, Dict[str, float]]:
    """
    Get the count and percentage of missing values for each feature in the dataframe.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.

    Returns:
    Dict[str, Dict[str, float]]: A dictionary with feature names as keys and 
    dictionaries with 'count' and 'percentage' of missing values as values.
    """
    missing_values = dataframe.isnull().sum().to_dict()
    missing_percentage = (dataframe.isnull().sum() / dataframe.shape[0] * 100).to_dict()
    missing_data = {
        feature: {
            'count': int(missing_values[feature]),
            'percentage': float(missing_percentage[feature])
        } 
        for feature in missing_values
    }
    return missing_data


def save_missing_values(missing_data: Dict[str, Dict[str, float]], save_dir: str) -> None:
    """
    Save the missing values analysis results to a JSON file.

    Parameters:
    missing_data (Dict[str, Dict[str, float]]): The missing values analysis results.
    save_dir (str): Directory to save the JSON file.

    Raises:
    Exception: If there is an error saving the JSON file.
    """
    try:
        metadata = {
            "features": {
                feature: {
                    "missing_values": {
                        "count": values['count'],
                        "percentage": values['percentage']
                    }
                }
                for feature, values in missing_data.items()
            }
        }

        filepath = os.path.join(save_dir, 'missing_values_analysis.json')
        # Use the pipeline to save the JSON file with all preprocessing steps applied
        save_json_with_pipeline(metadata, filepath)
    except Exception as e:
        logger.error(f"Error saving missing values analysis: {e}")
        raise


def display_missing_values_analysis(
    missing_data: Dict[str, Dict[str, float]]
) -> None:
    """
    Display the missing values analysis.

    Parameters:
    missing_data (Dict[str, Dict[str, float]]): The missing values analysis results.

    Raises:
    Exception: If there is an error displaying the analysis.
    """
    try:
        missing_data_df = (
            pd.DataFrame(missing_data)
            .T
            .sort_values(by='count', ascending=False)
            .reset_index()
        )
        missing_data_df = missing_data_df.rename(columns={'index': 'Feature'})
        display_dataframe_as_html(
            missing_data_df, 
            title="Missing Values Analysis", 
            description="Overview of missing values in the dataset."
        )
    except Exception as e:
        logger.error(f"Error displaying missing values analysis: {e}")
        raise

def plot_missing_values(
    missing_data: Dict[str, Dict[str, float]], 
    save_dir: Optional[str]
) -> None:
    """
    Plot the missing values analysis results.

    Parameters:
    missing_data (Dict[str, Dict[str, float]]): The missing values analysis results.
    save_dir (Optional[str]): Directory to save the plot.

    Raises:
    Exception: If there is an error plotting the missing values.
    """
    try:
        missing_data_df = (
            pd.DataFrame(missing_data)
            .T
            .sort_values(by='count', ascending=False)
            .reset_index()
        )
        missing_data_df = missing_data_df.rename(columns={'index': 'Feature'})

        plot_path = os.path.join(save_dir, 'missing_values_plot.png') if save_dir else 'missing_values_plot.png'
        plot_bar(
            data=missing_data_df,
            x='Feature',
            y='count',
            title='Missing Values per Column',
            xlabel='Feature',
            ylabel='Number of Missing Values',
            save_path=plot_path,
            rotation=90
        )
    except Exception as e:
        logger.error(f"Error plotting missing values: {e}")
        raise


def feature_balance_analysis(dataframe: pd.DataFrame, save_dir: Optional[str] = None) -> None:
    """
    Perform feature balance analysis on the given dataframe and save the results.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    save_dir (Optional[str]): Directory to save the analysis results.

    Raises:
    Exception: If there is an error during the analysis.
    """
    try:
        feature_balance = calculate_feature_balance(dataframe)
        if save_dir:
            save_feature_balance(feature_balance, save_dir)
        display_feature_balance(feature_balance)
        feature_balance_sorted = pd.Series({
            col: bal['balance']['most_common_value_weight']
            for col, bal in feature_balance["features"].items()
        }).sort_values(ascending=False)
        plot_feature_balance(feature_balance_sorted, save_dir)
        
        print("\n--- Contextual Insights and Recommendations ---\n")
        print("High-Level Overview: Many features show a high percentage of a single value, indicating potential imbalance.")
        print("Detailed Technical Insights: Imbalanced features can impact the performance of machine learning models, particularly in classification tasks.")
        print("Actionable Recommendations: Consider techniques to handle imbalanced features, such as resampling, feature engineering, or model adjustments.")
    except Exception as e:
        logger.error(f"Error in feature balance analysis: {e}")
        raise


def calculate_feature_balance(dataframe: pd.DataFrame) -> Dict[str, Dict[str, Dict[str, float]]]:
    """
    Calculate the feature balance for each column in the dataframe.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.

    Returns:
    Dict[str, Dict[str, Dict[str, float]]]: A dictionary with the feature balance for each column.
    """
    return {
        "features": {
            col: {
                'balance': {
                    'most_common_value_weight': dataframe[col].value_counts(normalize=True).max() * 100
                }
            }
            for col in dataframe.columns
        }
    }


def save_feature_balance(feature_balance: Dict[str, Dict[str, Dict[str, float]]], save_dir: str) -> None:
    """
    Save the feature balance analysis results to a JSON file using the JSON pipeline.

    Parameters:
    feature_balance (Dict[str, Dict[str, Dict[str, float]]]): The feature balance analysis results.
    save_dir (str): Directory to save the JSON file.

    Raises:
    Exception: If there is an error saving the JSON file.
    """
    try:
        os.makedirs(save_dir, exist_ok=True)
        filepath = os.path.join(save_dir, 'feature_balance.json')
        save_json_with_pipeline(feature_balance, filepath)
    except Exception as e:
        logger.error(f"Error saving feature balance: {e}")
        raise


def display_feature_balance(feature_balance: Dict[str, Dict[str, Dict[str, float]]]) -> None:
    """
    Display the feature balance analysis results.

    Parameters:
    feature_balance (Dict[str, Dict[str, Dict[str, float]]]): The feature balance analysis results.

    Raises:
    Exception: If there is an error displaying the analysis.
    """
    try:
        feature_balance_sorted = pd.Series({
            col: bal['balance']['most_common_value_weight']
            for col, bal in feature_balance["features"].items()
        }).sort_values(ascending=False)
        feature_balance_df = feature_balance_sorted.reset_index()
        feature_balance_df.columns = ['Feature', 'Most Common Value Weight']
        
        display_dataframe_as_html(
            feature_balance_df, 
            title="Feature Balance Analysis", 
            description="Overview of feature balance in the dataset."
        )
    except Exception as e:
        logger.error(f"Error displaying feature balance analysis: {e}")
        raise


def plot_feature_balance(
    feature_balance_sorted: pd.Series, 
    save_dir: Optional[str]
) -> None:
    """
    Plot the feature balance analysis results.

    Parameters:
    feature_balance_sorted (pd.Series): The sorted feature balance analysis results.
    save_dir (Optional[str]): Directory to save the plot.

    Raises:
    Exception: If there is an error plotting the feature balance.
    """
    try:
        feature_balance_df = feature_balance_sorted.reset_index()
        feature_balance_df.columns = ['Feature', 'Most Common Value Weight']

        plot_path = os.path.join(save_dir, 'feature_balance_plot.png') if save_dir else 'feature_balance_plot.png'
        plot_bar(
            data=feature_balance_df,
            x='Feature',
            y='Most Common Value Weight',
            title='Feature Balance (Percentage of Highest Value)',
            xlabel='Feature',
            ylabel='Percentage',
            save_path=plot_path,
            rotation=90
        )
    except Exception as e:
        logger.error(f"Error plotting feature balance: {e}")
        raise


# src/analysis/data_understanding.py

import os
import logging
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, Optional, Tuple
from IPython.display import display, HTML
from src.visualization.plot_utils import plot_heatmap, plot_correlation
from src.utils.display_utils import display_dataframe_as_html
from src.utils.json_pipeline import save_json_with_pipeline

logger = logging.getLogger(__name__)

def correlation_analysis(dataframe: pd.DataFrame, save_dir: Optional[str] = None, pair_threshold: float = 0.2) -> Tuple[pd.Series, pd.Series]:
    """
    Perform correlation analysis on the given dataframe and save the results.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    save_dir (Optional[str]): Directory to save the analysis results.
    pair_threshold (float, optional): Threshold for high correlation pairs.

    Returns:
    Tuple[pd.Series, pd.Series]: High correlation pairs and target correlation with 'HasDetections'.
    """
    try:
        numerical_columns = dataframe.select_dtypes(include=['int64', 'float64']).columns
        correlation_matrix = dataframe[numerical_columns].corr()

        high_correlation_pairs = calculate_high_correlation_pairs(correlation_matrix, pair_threshold)
        target_corr = calculate_target_correlation(correlation_matrix, 'HasDetections')

        if save_dir:
            # Save results using the preprocessing pipeline
            save_correlation_results(correlation_matrix, high_correlation_pairs, target_corr, save_dir)
            plot_heatmap(correlation_matrix, 'Correlation Matrix', os.path.join(save_dir, 'correlation_matrix.png'))
            target_corr_df = target_corr.reset_index()
            target_corr_df.columns = ['Feature', 'Correlation']
            plot_correlation(target_corr_df, "Correlation with 'HasDetections'", os.path.join(save_dir, 'target_correlation.png'))

        display_correlation_analysis(correlation_matrix, high_correlation_pairs, pair_threshold)
        display_target_correlation_analysis(target_corr)

        return high_correlation_pairs, target_corr

    except Exception as e:
        logging.error(f"Error in correlation analysis: {e}")
        raise

def calculate_high_correlation_pairs(correlation_matrix: pd.DataFrame, threshold: float) -> pd.Series:
    """
    Calculate high correlation pairs based on the given threshold.

    Parameters:
    correlation_matrix (pd.DataFrame): Correlation matrix.
    threshold (float): Threshold for high correlation pairs.

    Returns:
    pd.Series: High correlation pairs.
    """
    high_correlation_pairs = correlation_matrix.unstack().sort_values(kind="quicksort", ascending=False).drop_duplicates()
    high_correlation_pairs = high_correlation_pairs[(high_correlation_pairs.abs() > threshold) & (high_correlation_pairs != 1)]
    return high_correlation_pairs

def calculate_target_correlation(correlation_matrix: pd.DataFrame, target_column: str) -> pd.Series:
    """
    Calculate correlation with the target variable.

    Parameters:
    correlation_matrix (pd.DataFrame): Correlation matrix.
    target_column (str): Target column name.

    Returns:
    pd.Series: Correlation with the target variable.
    """
    return correlation_matrix[target_column].drop(target_column).sort_values(ascending=False)

def save_correlation_results(correlation_matrix: pd.DataFrame, high_correlation_pairs: pd.Series, target_corr: pd.Series, save_dir: str) -> None:
    """
    Save the correlation analysis results to JSON files.

    Parameters:
    correlation_matrix (pd.DataFrame): Correlation matrix.
    high_correlation_pairs (pd.Series): High correlation pairs.
    target_corr (pd.Series): Correlation with the target variable.
    save_dir (str): Directory to save the JSON files.

    Raises:
    Exception: If there is an error saving the JSON files.
    """
    try:
        os.makedirs(save_dir, exist_ok=True)

        correlation_dict = {
            "features": {
                col: {
                    "correlations": {
                        "feature_correlation_with_other_features": correlation_matrix[col].drop(col).to_dict()
                    }
                }
                for col in correlation_matrix.columns
            }
        }

        high_corr_pairs_dict = {
            "features": {}
        }

        for pair, value in high_correlation_pairs.items():
            feature1, feature2 = pair
            if feature1 not in high_corr_pairs_dict["features"]:
                high_corr_pairs_dict["features"][feature1] = {"correlations": {"feature_correlation_with_other_features": {}}}
            if feature2 not in high_corr_pairs_dict["features"]:
                high_corr_pairs_dict["features"][feature2] = {"correlations": {"feature_correlation_with_other_features": {}}}
            high_corr_pairs_dict["features"][feature1]["correlations"]["feature_correlation_with_other_features"][feature2] = value
            high_corr_pairs_dict["features"][feature2]["correlations"]["feature_correlation_with_other_features"][feature1] = value

        target_corr_dict = {
            "features": {
                feature: {
                    "correlations": {
                        "feature_correlation_with_target": value
                    }
                }
                for feature, value in target_corr.items()
            }
        }

        # Use the pipeline to save the JSON files with all preprocessing steps applied
        save_json_with_pipeline(correlation_dict, os.path.join(save_dir, 'correlations.json'))
        save_json_with_pipeline(high_corr_pairs_dict, os.path.join(save_dir, 'high_correlation_pairs.json'))
        save_json_with_pipeline(target_corr_dict, os.path.join(save_dir, 'target_correlations.json'))

    except Exception as e:
        logging.error(f"Error saving correlation results: {e}")
        raise

def display_correlation_analysis(correlation_matrix: pd.DataFrame, high_correlation_pairs: pd.Series, pair_threshold: float) -> None:
    """
    Display the correlation analysis results.

    Parameters:
    correlation_matrix (pd.DataFrame): Correlation matrix.
    high_correlation_pairs (pd.Series): High correlation pairs.
    pair_threshold (float): Threshold for high correlation pairs.
    """
    try:
        high_correlation_df = high_correlation_pairs.reset_index()
        high_correlation_df.columns = ['Feature Pair 1', 'Feature Pair 2', 'Correlation']
        high_correlation_df.reset_index(drop=True, inplace=True)
        display_dataframe_as_html(high_correlation_df, title="High Correlation Pairs", description=f"High correlation pairs (absolute value > {pair_threshold})")
        
        plot_heatmap(correlation_matrix, 'Correlation Matrix', 'correlation_matrix.png')

        print("\n--- Contextual Insights and Recommendations ---\n")
        print(f"High-Level Overview: Several feature pairs show high correlation (absolute value > {pair_threshold}).")
        print("Detailed Technical Insights: Highly correlated features can indicate multicollinearity, which may affect model performance.")
        print("Actionable Recommendations: Consider removing or combining highly correlated features to reduce redundancy and multicollinearity.")
    except Exception as e:
        logging.error(f"Error displaying correlation analysis: {e}")
        raise

def display_target_correlation_analysis(target_corr: pd.Series) -> None:
    """
    Display the target correlation analysis results.

    Parameters:
    target_corr (pd.Series): Correlation with the target variable.
    """
    try:
        target_corr_df = target_corr.reset_index()
        target_corr_df.columns = ['Feature', 'Correlation']
        target_corr_df.reset_index(drop=True, inplace=True)

        display_dataframe_as_html(target_corr_df, title="Correlation with Target Variable", description="Correlation with target variable 'HasDetections'")

        plot_correlation(target_corr_df, "Correlation with 'HasDetections'", 'target_correlation.png')

        print("\n--- Contextual Insights and Recommendations ---\n")
        print("High-Level Overview: Some features show strong correlation with the target variable 'HasDetections'.")
        print("Detailed Technical Insights: Identifying these features can help in feature selection and engineering.")
        print("Actionable Recommendations: Consider these features for model development and further analysis.")
    except Exception as e:
        logging.error(f"Error displaying target correlation analysis: {e}")
        raise


def feature_interactions(dataframe, save_dir=None, features=None):
    """
    Analyze feature interactions in the given dataframe and save the results.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    save_dir (str, optional): Directory to save the analysis results.
    features (list, optional): List of features to analyze interactions.

    Returns:
    None
    """
    if features is None:
        features = [
            'Census_OSInstallLanguageIdentifier', 'Census_OSUILocaleIdentifier', 
            'Census_OSBuildNumber', 'OsBuild', 'IsSxsPassiveMode', 'RtpStateBitfield', 
            'Census_InternalPrimaryDisplayResolutionHorizontal', 
            'Census_InternalPrimaryDisplayResolutionVertical'
        ]

    try:
        plot_feature_interactions(dataframe, features)
        if save_dir:
            save_json_file({}, os.path.join(save_dir, 'feature_interactions.json'))
    except Exception as e:
        logger.error(f"Error in feature interactions analysis: {e}")
        raise


def plot_feature_interactions(dataframe, features):
    """
    Plot feature interactions for the given features.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    features (list): List of features to analyze interactions.

    Returns:
    None
    """
    print("\n==== Feature Interactions ====\n")
    sns.pairplot(dataframe, vars=features)
    plt.show()
    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: The pair plots visualize relationships between highly correlated features.")
    print("Detailed Technical Insights: Observing feature interactions can provide insights into underlying patterns and relationships in the data.")
    print("Actionable Recommendations: Use these insights for feature engineering or model selection, leveraging relationships between features.")


def classify_feature_types(dataframe: pd.DataFrame, schema_path: str, save_dir: str = None) -> dict:
    """
    Classify feature types based on the DataFrame and update classified_data_type field in JSON format.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame to analyze.
    schema_path (str): Path to the schema file to ensure the JSON structure.
    save_dir (str, optional): Directory to save the classification results.

    Returns:
    dict: A dictionary structured according to the feature_metadata_complete_schema.json for classified_data_type.
    """
    try:
        # Load the schema to ensure correct structure
        schema = load_config(schema_path)
        classified_types = analyze_classified_types(dataframe)

        # Ensure the structure is consistent with the schema
        structured_classified_types = {
            "features": {
                column: {
                    "general_attributes": {
                        "classified_data_type": dtype
                    }
                }
                for column, dtype in classified_types.items()
            }
        }

        # Validate against the schema (optional step, could be added)
        # validate_json_against_schema(structured_classified_types, schema) 

        if save_dir:
            save_classified_types(structured_classified_types, save_dir)

        return structured_classified_types
    except Exception as e:
        logger.error(f"Error in classifying feature types: {e}")
        raise

def analyze_classified_types(dataframe: pd.DataFrame) -> dict:
    """
    Analyzes the classified data types of the dataframe columns.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame to analyze.

    Returns:
    dict: A dictionary of classified data types.
    """
    classified_types = {}
    for column in dataframe.columns:
        if dataframe[column].nunique() == 2:
            classified_types[column] = "binary"
        elif dataframe[column].dtype == 'object' or dataframe[column].nunique() < 10:
            classified_types[column] = "categorical"
        else:
            classified_types[column] = "numerical"
    return classified_types


def save_classified_types(classified_types: dict, save_dir: str) -> None:
    """
    Saves the classified data types information to a JSON file.

    Parameters:
    classified_types (dict): The classified data types information.
    save_dir (str): Directory to save the JSON file.

    Raises:
    Exception: If there is an error saving the JSON file.
    """
    try:
        os.makedirs(save_dir, exist_ok=True)
        save_json_with_pipeline(classified_types, os.path.join(save_dir, 'classified_data_types.json'))
    except Exception as e:
        logger.error(f"Error saving classified types: {e}")
        raise


def manual_feature_classification(dataframe):
    """
    Manually classify features in the dataframe.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.

    Returns:
    tuple: A tuple containing lists of manually classified binary, categorical, and numerical features.
    """
    binary_features_manual = [col for col in dataframe.columns if dataframe[col].nunique() == 2]
    categorical_features_manual = [col for col in dataframe.columns if dataframe[col].dtype == 'object' and col not in binary_features_manual]
    numerical_features_manual = [col for col in dataframe.columns if dataframe[col].dtype in ['int64', 'float64'] and col not in binary_features_manual]
    return binary_features_manual, categorical_features_manual, numerical_features_manual


def auto_feature_classification(dataframe):
    """
    Automatically classify features in the dataframe.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.

    Returns:
    tuple: A tuple containing lists of automatically classified binary, categorical, and numerical features.
    """
    binary_features_auto = [col for col in dataframe.columns if dataframe[col].nunique() == 2]
    categorical_features_auto = [col for col in dataframe.columns if dataframe[col].dtype == 'object' and col not in binary_features_auto]
    numerical_features_auto = [col for col in dataframe.columns if dataframe[col].dtype in ['int64', 'float64'] and col not in binary_features_auto]
    return binary_features_auto, categorical_features_auto, numerical_features_auto


def find_mismatched_features(binary_manual, binary_auto, categorical_manual, categorical_auto, numerical_manual, numerical_auto):
    """
    Find mismatched features between manual and automated classifications.

    Parameters:
    binary_manual (list): List of manually classified binary features.
    binary_auto (list): List of automatically classified binary features.
    categorical_manual (list): List of manually classified categorical features.
    categorical_auto (list): List of automatically classified categorical features.
    numerical_manual (list): List of manually classified numerical features.
    numerical_auto (list): List of automatically classified numerical features.

    Returns:
    tuple: A tuple containing sets of mismatched binary, categorical, and numerical features.
    """
    mismatched_binary = set(binary_manual).symmetric_difference(binary_auto)
    mismatched_categorical = set(categorical_manual).symmetric_difference(categorical_auto)
    mismatched_numerical = set(numerical_manual).symmetric_difference(numerical_auto)
    return mismatched_binary, mismatched_categorical, mismatched_numerical


def compile_classification_results(binary_auto, categorical_auto, numerical_auto, mismatched_binary, mismatched_categorical, mismatched_numerical):
    """
    Compile classification results into a dictionary.

    Parameters:
    binary_auto (list): List of automatically classified binary features.
    categorical_auto (list): List of automatically classified categorical features.
    numerical_auto (list): List of automatically classified numerical features.
    mismatched_binary (set): Set of mismatched binary features.
    mismatched_categorical (set): Set of mismatched categorical features.
    mismatched_numerical (set): Set of mismatched numerical features.

    Returns:
    dict: Dictionary containing classification results.
    """
    return {
        'binary_features_auto': binary_auto,
        'categorical_features_auto': categorical_auto,
        'numerical_features_auto': numerical_auto,
        'mismatched_binary': list(mismatched_binary),
        'mismatched_categorical': list(mismatched_categorical),
        'mismatched_numerical': list(mismatched_numerical)
    }


def print_feature_classification_summary(dataframe, binary_manual, categorical_manual, numerical_manual, binary_auto, categorical_auto, numerical_auto, mismatched_binary, mismatched_categorical, mismatched_numerical):
    """
    Print the feature classification summary.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    binary_manual (list): List of manually classified binary features.
    categorical_manual (list): List of manually classified categorical features.
    numerical_manual (list): List of manually classified numerical features.
    binary_auto (list): List of automatically classified binary features.
    categorical_auto (list): List of automatically classified categorical features.
    numerical_auto (list): List of automatically classified numerical features.
    mismatched_binary (set): Set of mismatched binary features.
    mismatched_categorical (set): Set of mismatched categorical features.
    mismatched_numerical (set): Set of mismatched numerical features.

    Returns:
    None
    """
    summary_table = pd.DataFrame({
        'Feature Type': ['Binary', 'Categorical', 'Numerical'],
        'Manual Count': [len(binary_manual), len(categorical_manual), len(numerical_manual)],
        'Automated Count': [len(binary_auto), len(categorical_auto), len(numerical_auto)]
    })

    print("\n==== Feature Classification ====\n")
    print(summary_table)

    print("\n--- Feature Classification Debugging ---\n")
    print(f"Binary Features (Manual): {binary_manual}")
    print(f"Binary Features (Automated): {binary_auto}")
    print(f"Mismatched Binary Features: {mismatched_binary}")

    print(f"Categorical Features (Manual): {categorical_manual}")
    print(f"Categorical Features (Automated): {categorical_auto}")
    print(f"Mismatched Categorical Features: {mismatched_categorical}")

    print(f"Numerical Features (Manual): {numerical_manual}")
    print(f"Numerical Features (Automated): {numerical_auto}")
    print(f"Mismatched Numerical Features: {mismatched_numerical}")

    print("\n--- Further Inspection of Mismatched Features ---\n")
    inspect_mismatched_features(dataframe, mismatched_binary, 'Binary')
    inspect_mismatched_features(dataframe, mismatched_categorical, 'Categorical')
    inspect_mismatched_features(dataframe, mismatched_numerical, 'Numerical')

    print("\n--- Contextual Insights and Recommendations ---\n")
    print("High-Level Overview: Feature classification is validated by comparing manual and automated classifications.")
    print("Detailed Technical Insights: Mismatched features indicate discrepancies in classification which need further investigation.")
    print("Actionable Recommendations: Investigate and correct mismatched features to ensure accurate classification.")


def inspect_mismatched_features(dataframe, mismatched_features, feature_type):
    """
    Inspect and print details of mismatched features.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    mismatched_features (set): Set of mismatched features.
    feature_type (str): Type of features (Binary, Categorical, Numerical).

    Returns:
    None
    """
    if mismatched_features:
        print(f"Mismatched {feature_type} Features: {mismatched_features}")
        for feature in mismatched_features:
            print(f"Feature: {feature}")
            print(f"Unique Values: {dataframe[feature].unique()}")
            print(f"Data Type: {dataframe[feature].dtype}")
            print("\n")



