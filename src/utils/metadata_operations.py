# src/utils/metadata_operations.py

import os
import json
import logging
from typing import Dict, Any
from src.utils.file_operations import load_yaml, save_yaml, load_json_file, save_json_file
from src.data.data_preparation import convert_to_native_types

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

logger = logging.getLogger(__name__)


def save_updated_metadata(metadata: dict, metadata_path: str) -> None:
    """
    Save the updated metadata to a JSON file.

    Parameters:
    metadata (dict): The metadata dictionary to save.
    metadata_path (str): The path to the metadata JSON file.

    Raises:
    Exception: If there is an error saving the metadata.
    """
    try:
        save_json_file(metadata, metadata_path)
        logging.info(f"Metadata saved to {metadata_path}")
    except Exception as e:
        logging.error(f"Error saving updated metadata: {e}")
        raise

def update_metadata_from_json(metadata: dict, json_files: list) -> dict:
    """
    Update the metadata based on a list of JSON files.

    Parameters:
    metadata (dict): The metadata dictionary to update.
    json_files (list): List of file paths to JSON files to use for updates.

    Returns:
    dict: The updated metadata.
    """
    try:
        for json_file in json_files:
            update_data = load_json_file(json_file)
            logging.info(f"Processing update data from: {json_file}")
            if "balance" in update_data:
                _merge_balance_data(metadata, update_data)
            else:
                _merge_dicts(metadata, update_data)
            logging.info(f"Updated metadata with data from: {json_file}")
        return metadata
    except Exception as e:
        logging.error(f"Error updating metadata: {e}")
        raise

def _merge_balance_data(metadata: dict, balance_data: dict) -> None:
    """
    Merge balance data into the metadata structure.

    Parameters:
    metadata (dict): The original metadata dictionary.
    balance_data (dict): The balance data to merge into the metadata.
    """
    try:
        for feature, balance_info in balance_data.items():
            if feature in metadata["features"]:
                if "balance" not in metadata["features"][feature]:
                    metadata["features"][feature]["balance"] = {}
                metadata["features"][feature]["balance"].update(balance_info["balance"])
                logging.info(f"Balance data updated for feature: {feature}")
            else:
                logging.warning(f"Feature {feature} not found in metadata, skipping balance update.")
    except Exception as e:
        logging.error(f"Error merging balance data: {e}")
        raise

def _merge_dicts(original: dict, updates: dict) -> None:
    """
    Recursively merge two dictionaries.

    Parameters:
    original (dict): The original dictionary to update.
    updates (dict): The updates to apply to the original dictionary.
    """
    for key, value in updates.items():
        if isinstance(value, dict) and key in original:
            _merge_dicts(original[key], value)
        else:
            original[key] = value



# src/utils/metadata_operations.py

def dynamic_metadata_pipeline(metadata_path: str, json_files: list, schema_path: str) -> None:
    """
    The dynamic pipeline for loading, updating, validating, and saving metadata.

    Parameters:
    metadata_path (str): Path to the metadata JSON file.
    json_files (list): List of JSON files to update the metadata.
    schema_path (str): Path to the schema JSON file for validation.
    """
    try:
        # Load existing metadata
        metadata = load_json_file(metadata_path)
        
        # Load the schema
        schema = load_json_file(schema_path)
        
        # Insert logging here to validate schema type and content
        logging.info(f"Loaded schema type: {type(schema)}")
        logging.info(f"Loaded schema content: {schema}")
        
        # Update metadata dynamically based on JSON files
        updated_metadata = update_metadata_from_json(metadata, json_files)
        
        # Validate the updated metadata
        validate_metadata(updated_metadata, schema)  # Ensure schema is a dictionary
        
        # Save the updated metadata back to the file
        save_updated_metadata(updated_metadata, metadata_path)
        
        logging.info("Dynamic metadata pipeline completed successfully.")
    except Exception as e:
        logging.error(f"Error in dynamic metadata pipeline: {e}")
        raise





def validate_metadata(metadata: dict, schema: dict) -> bool:
    """
    Validate the metadata against the provided schema.

    Parameters:
    metadata (dict): The metadata dictionary to validate.
    schema (dict): The schema dictionary to validate against.

    Returns:
    bool: True if validation is successful, otherwise raises ValidationError.
    """
    try:
        from jsonschema import validate, ValidationError  # Importing here to ensure module availability
        validate(instance=metadata, schema=schema)
        logging.info("Metadata validation successful.")
        return True
    except ValidationError as e:
        logging.error(f"Metadata validation error: {e}")
        raise



# Old function adapted for JSON
def determine_feature_type(metadata: Dict[str, Any], feature_name: str) -> str:
    """
    Determine the type of a feature.

    Parameters:
    metadata (Dict[str, Any]): The metadata dictionary.
    feature_name (str): The name of the feature to determine the type.

    Returns:
    str: Determined feature type.
    """
    classified_type = metadata['features'][feature_name].get('classified_data_type', 'unknown')
    if classified_type in ['binary', 'categorical']:
        return classified_type
    
    example_values = metadata['features'][feature_name].get('example_values', [])
    if all(isinstance(val, (int, float)) for val in example_values):
        if example_values and isinstance(example_values[0], int):
            return 'categorical'
    return 'numerical'


def update_example_values(metadata, results, paths, train_sample):
    """
    Update metadata with example values for features.

    Parameters:
    metadata (dict): Existing metadata.
    results (dict): Analysis results.
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.

    Returns:
    dict: Updated metadata.
    """
    def select_example_values(feature, dtype, dataframe):
        if dtype in ['int64', 'float64']:
            return [
                dataframe[feature].min(),
                dataframe[feature].max(),
                dataframe[feature].mean(),
                dataframe[feature].median(),
                dataframe[feature].std()
            ]
        elif dtype == 'object':
            return dataframe[feature].value_counts().index.tolist()[:5]
        else:
            return dataframe[feature].unique().tolist()

    try:
        if 'data_types' in results:
            for feature, dtype in results['data_types'].items():
                example_values = select_example_values(feature, dtype, train_sample)
                metadata = update_feature_metadata(metadata, feature, {'example_values': example_values})
        return metadata
    except Exception as e:
        logging.error(f"Error updating example values: {e}", exc_info=True)
        raise




    


# def determine_feature_type(metadata: Dict[str, Any], feature_name: str) -> str:
#     """
#     Determine the feature type based on the `technical_data_type` in the metadata.

#     Parameters:
#     metadata (Dict[str, Any]): The metadata dictionary loaded from `config/feature_metadata.json`.
#     feature_name (str): The name of the feature to determine the type.

#     Returns:
#     str: The `technical_data_type` of the feature if found, otherwise 'unknown'.
#     """
#     try:
#         print(f"metadata type: {type(metadata)}")  # Debugging
#         print(f"metadata content: {metadata}")  # Debugging
#         print(f"feature_name: {feature_name}")  # Debugging

#         # Access the feature's general attributes in the metadata
#         feature_info = metadata.get("features", {}).get(feature_name, {})
#         print(f"feature_info: {feature_info}")  # Debugging

#         general_attributes = feature_info.get("general_attributes", {})
#         print(f"general_attributes: {general_attributes}")  # Debugging
        
#         # Retrieve the technical data type
#         feature_type = general_attributes.get("technical_data_type", "unknown")
        
#         if feature_type == "unknown":
#             logger.warning(f"Feature type for {feature_name} is unknown.")
        
#         return feature_type
#     except KeyError as e:
#         logger.error(f"KeyError: {e} while determining the feature type for {feature_name}.")
#         return "unknown"
#     except Exception as e:
#         logger.error(f"Error in determine_feature_type: {e}")
#         return "unknown"


        
# def dynamic_metadata_pipeline(metadata_path: str, json_files: list, schema_path: str) -> None:
#     """
#     The dynamic pipeline for loading, updating, validating, and saving metadata.

#     Parameters:
#     metadata_path (str): Path to the metadata JSON file.
#     json_files (list): List of JSON files to update the metadata.
#     schema_path (str): Path to the schema JSON file for validation.
#     """
#     try:
#         logging.info("Starting dynamic metadata pipeline.")
        
#         # Load existing metadata
#         metadata = load_json_file(metadata_path)
#         logging.info("Loaded existing metadata.")
        
#         # Load the schema
#         schema = load_json_file(schema_path)
#         logging.info("Loaded schema.")
#         logging.info(f"Loaded schema type: {type(schema)}")
#         logging.info(f"Loaded schema content: {schema}")
        
#         # Update metadata dynamically based on JSON files
#         updated_metadata = update_metadata_from_json(metadata, json_files)
#         logging.info("Metadata updated.")
        
#         # Validate the updated metadata
#         if validate_metadata(updated_metadata, schema):
#             logging.info("Updated metadata validated successfully.")
        
#         # Save the updated metadata back to the file
#         save_updated_metadata(updated_metadata, metadata_path)
#         logging.info("Dynamic metadata pipeline completed successfully.")
        
#     except Exception as e:
#         logging.error(f"Error in dynamic metadata pipeline: {e}")
#         raise








# """
# Module for handling dynamic metadata operations, including loading, updating, 
# and saving metadata based on various JSON files.
# """

# import os
# import json
# import logging
# from src.utils.file_operations import load_yaml, save_yaml, load_json_file, save_json_file
# from src.data.data_preparation import convert_to_native_types


# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# def save_updated_metadata(metadata: dict, metadata_path: str) -> None:
#     """
#     Save the updated metadata to a JSON file.

#     Parameters:
#     metadata (dict): The metadata dictionary to save.
#     metadata_path (str): The path to the metadata JSON file.

#     Raises:
#     Exception: If there is an error saving the metadata.
#     """
#     try:
#         save_json_file(metadata, metadata_path)
#         logging.info(f"Metadata saved to {metadata_path}")
#     except Exception as e:
#         logging.error(f"Error saving updated metadata: {e}")
#         raise

# def update_metadata_from_json(metadata: dict, json_files: list) -> dict:
#     """
#     Update the metadata based on a list of JSON files.

#     Parameters:
#     metadata (dict): The metadata dictionary to update.
#     json_files (list): List of file paths to JSON files to use for updates.

#     Returns:
#     dict: The updated metadata.
#     """
#     try:
#         for json_file in json_files:
#             update_data = load_json_file(json_file)
#             if "balance" in update_data:
#                 _merge_balance_data(metadata, update_data)
#             else:
#                 _merge_dicts(metadata, update_data)
#             logging.info(f"Updated metadata with data from: {json_file}")
#         return metadata
#     except Exception as e:
#         logging.error(f"Error updating metadata: {e}")
#         raise

# def _merge_balance_data(metadata: dict, balance_data: dict) -> None:
#     """
#     Merge balance data into the metadata structure.

#     Parameters:
#     metadata (dict): The original metadata dictionary.
#     balance_data (dict): The balance data to merge into the metadata.
#     """
#     for feature, balance_info in balance_data.items():
#         if feature in metadata["features"]:
#             if "balance" not in metadata["features"][feature]:
#                 metadata["features"][feature]["balance"] = {}
#             metadata["features"][feature]["balance"].update(balance_info["balance"])
#         else:
#             logging.warning(f"Feature {feature} not found in metadata, skipping balance update.")

# def _merge_dicts(original: dict, updates: dict) -> None:
#     """
#     Recursively merge two dictionaries.

#     Parameters:
#     original (dict): The original dictionary to update.
#     updates (dict): The updates to apply to the original dictionary.
#     """
#     for key, value in updates.items():
#         if isinstance(value, dict) and key in original:
#             _merge_dicts(original[key], value)
#         else:
#             original[key] = value

# def validate_metadata(metadata: dict, schema: dict) -> bool:
#     """
#     Validate the metadata against the provided schema.

#     Parameters:
#     metadata (dict): The metadata dictionary to validate.
#     schema (dict): The schema dictionary to validate against.

#     Returns:
#     bool: True if validation is successful, otherwise raises ValidationError.
#     """
#     try:
#         from jsonschema import validate, ValidationError  # Importing here to ensure module availability
#         validate(instance=metadata, schema=schema)
#         logging.info("Metadata validation successful.")
#         return True
#     except ValidationError as e:
#         logging.error(f"Metadata validation error: {e}")
#         raise

# def dynamic_metadata_pipeline(metadata_path: str, json_files: list, schema_path: str) -> None:
#     """
#     The dynamic pipeline for loading, updating, validating, and saving metadata.

#     Parameters:
#     metadata_path (str): Path to the metadata JSON file.
#     json_files (list): List of JSON files to update the metadata.
#     schema_path (str): Path to the schema JSON file for validation.
#     """
#     try:
#         # Load existing metadata
#         metadata = load_json_file(metadata_path)
        
#         # Load the schema
#         schema = load_json_file(schema_path)
        
#         # Update metadata dynamically based on JSON files
#         updated_metadata = update_metadata_from_json(metadata, json_files)
        
#         # Validate the updated metadata
#         validate_metadata(updated_metadata, schema)
        
#         # Save the updated metadata back to the file
#         save_updated_metadata(updated_metadata, metadata_path)
        
#         logging.info("Dynamic metadata pipeline completed successfully.")
#     except Exception as e:
#         logging.error(f"Error in dynamic metadata pipeline: {e}")
#         raise




# # src/utils/metadata_operations.py

# """
# Module for handling dynamic metadata operations, including loading, updating, 
# and saving metadata based on various JSON files.
# """

# import os
# import json
# import logging
# from src.utils.file_operations import load_yaml, save_yaml, load_json_file, save_json_file
# from src.data.data_preparation import convert_to_native_types


# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# def save_updated_metadata(metadata, metadata_path):
#     """
#     Save the updated metadata to a JSON file.

#     Parameters:
#     metadata (dict): The metadata dictionary to save.
#     metadata_path (str): The path to the metadata JSON file.

#     Raises:
#     Exception: If there is an error saving the metadata.
#     """
#     try:
#         save_json_file(metadata, metadata_path)
#         logging.info(f"Metadata saved to {metadata_path}")
#     except Exception as e:
#         logging.error(f"Error saving updated metadata: {e}")
#         raise




# def update_metadata_from_json(metadata, json_files):
#     """
#     Update the metadata based on a list of JSON files.

#     Parameters:
#     metadata (dict): The metadata dictionary to update.
#     json_files (list): List of file paths to JSON files to use for updates.

#     Returns:
#     dict: The updated metadata.
#     """
#     try:
#         for json_file in json_files:
#             update_data = load_json_file(json_file)
#             _merge_dicts(metadata, update_data)
#             logging.info(f"Updated metadata with data from: {json_file}")
#         return metadata
#     except Exception as e:
#         logging.error(f"Error updating metadata: {e}")
#         raise

# def _merge_dicts(original, updates):
#     """
#     Recursively merge two dictionaries.

#     Parameters:
#     original (dict): The original dictionary to update.
#     updates (dict): The updates to apply to the original dictionary.
#     """
#     for key, value in updates.items():
#         if isinstance(value, dict) and key in original:
#             _merge_dicts(original[key], value)
#         else:
#             original[key] = value

# def validate_metadata(metadata, schema):
#     """
#     Validate the metadata against the provided schema.

#     Parameters:
#     metadata (dict): The metadata dictionary to validate.
#     schema (dict): The schema dictionary to validate against.

#     Returns:
#     bool: True if validation is successful, otherwise raises ValidationError.
#     """
#     try:
#         from jsonschema import validate, ValidationError  # Importing here to ensure module availability
#         validate(instance=metadata, schema=schema)
#         logging.info("Metadata validation successful.")
#         return True
#     except ValidationError as e:
#         logging.error(f"Metadata validation error: {e}")
#         raise

# def dynamic_metadata_pipeline(metadata_path, json_files, schema_path):
#     """
#     The dynamic pipeline for loading, updating, validating, and saving metadata.

#     Parameters:
#     metadata_path (str): Path to the metadata JSON file.
#     json_files (list): List of JSON files to update the metadata.
#     schema_path (str): Path to the schema JSON file for validation.
#     """
#     try:
#         # Load existing metadata
#         metadata = load_json_file(metadata_path)
        
#         # Load the schema
#         schema = load_json_file(schema_path)
        
#         # Update metadata dynamically based on JSON files
#         updated_metadata = update_metadata_from_json(metadata, json_files)
        
#         # Validate the updated metadata
#         validate_metadata(updated_metadata, schema)
        
#         # Save the updated metadata back to the file
#         save_json_file(updated_metadata, metadata_path)
        
#         logging.info("Dynamic metadata pipeline completed successfully.")
#     except Exception as e:
#         logging.error(f"Error in dynamic metadata pipeline: {e}")
#         raise



        


        

# def execute_metadata_update_pipeline(train_sample, paths, analysis_results_dir, json_files, fields_to_update=None):
#     """
#     Execute the metadata update pipeline with various steps.

#     Parameters:
#     train_sample (pd.DataFrame): The training sample DataFrame.
#     paths (dict): Dictionary containing file paths.
#     analysis_results_dir (str): Directory for saving analysis results.
#     json_files (list): List of JSON files to initialize.
#     fields_to_update (list): List of fields to update in the metadata.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         results = load_analysis_results(analysis_results_dir, json_files)
#         logging.debug(f"Loaded analysis results: {results.keys()}")
#         metadata_path = paths['config']['feature_metadata']
#         metadata = load_metadata(metadata_path)
#         logging.debug(f"Loaded metadata: {metadata}")
#         schema_path = paths['config']['feature_metadata_schema']
#         schema = load_yaml(schema_path)
        
#         backup_metadata(paths)
        
#         metadata = apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update)

#         save_updated_metadata(metadata, paths, schema)
        
#         logging.debug("Metadata before displaying:")
#         display_metadata(metadata)
        
#         return metadata
#     except Exception as e:
#         logging.error(f"Error in metadata update pipeline: {e}", exc_info=True)
#         raise

# def load_analysis_results(analysis_results_dir, json_files):
#     """
#     Load analysis results from JSON files.

#     Parameters:
#     analysis_results_dir (str): Directory containing analysis result files.
#     json_files (list): List of JSON files to load.

#     Returns:
#     dict: Loaded analysis results.
#     """
#     results = {}
#     for file in json_files:
#         file_path = os.path.join(analysis_results_dir, file)
#         if os.path.isfile(file_path):
#             with open(file_path, 'r') as f:
#                 results[file.split('.')[0]] = json.load(f)
#             logging.debug(f"Loaded {file} from {file_path}")
#         else:
#             logging.warning(f"File {file} not found in {analysis_results_dir}")
#     return results

# def load_metadata(metadata_path):



# def apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update=None):
#     """
#     Update metadata with analysis results.

#     Parameters:
#     metadata (dict): Existing metadata.
#     schema (dict): Schema to validate against.
#     results (dict): Analysis results.
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.
#     fields_to_update (list): List of fields to update in the metadata.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         update_functions = {
#             'data_overview': update_data_overview,
#             'data_types': update_data_types_initial,
#             'stat_summary': update_stat_summary,
#             'missing_values': update_missing_values,
#             'feature_balance': update_feature_balance,
#             'target_correlations': update_target_correlations,
#             'target_analysis': update_target_analysis,
#             'descriptions': update_metadata_with_descriptions,
#             'classifications_initial': update_feature_classifications_initial,
#             'classifications_manual': update_feature_classifications_manual,
#             'example_values': update_example_values,
#             'high_correlation_pairs': update_high_correlation_pairs,
#             'missing_values_target_correlations': update_missing_values_target_correlations,
#             'missing_values_pair_correlations': update_missing_values_pair_correlations,
#             'imputation_strategy': update_imputation_strategy,
#             'data_types_after_initial_cleaning': update_data_types_after_initial_cleaning 
#         }

#         fields_to_update = fields_to_update or update_functions.keys()

#         for field in fields_to_update:
#             if field in update_functions:
#                 update_functions[field](metadata, results, paths, train_sample)

#         return metadata 
#     except Exception as e:
#         logging.error(f"Error updating metadata with results: {e}", exc_info=True)
#         raise



# def update_feature_metadata(metadata, feature, updates):



# def update_data_overview(metadata, results, paths, train_sample):







# def update_missing_values(metadata, results, paths, train_sample):
#     """
#     Update metadata with missing values analysis results.

#     Parameters:
#     metadata (dict): Existing metadata.
#     results (dict): Analysis results.
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.
#     """
#     missing_values = results.get('missing_values', {})
#     for feature, data in missing_values.items():
#         if feature in metadata['features']:
#             metadata['features'][feature]['missing_values'] = {
#                 'count': data['count'],
#                 'percentage': data['percentage']
#             }
#             logging.debug(f"Updated missing values for feature: {feature}")
#         else:
#             logging.warning(f"Feature {feature} not found in metadata.")


# # def update_missing_values_target_correlations(metadata, results, paths, train_sample):
# #     """
# #     Update metadata with missing values' correlation with the target variable.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results containing missing values correlations.
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     if 'missing_values_correlations_with_target_variable' in results:
# #         correlations = results['missing_values_correlations_with_target_variable']
# #         logging.debug(f"Correlations found: {correlations}")
# #         for item in correlations:
# #             feature = item['Feature']
# #             correlation = item['Correlation']
# #             if feature in metadata['features']:
# #                 if 'missing_values' not in metadata['features'][feature]:
# #                     metadata['features'][feature]['missing_values'] = {}
# #                 metadata['features'][feature]['missing_values']['missing_value_correlation_with_target'] = correlation
# #                 logging.debug(f"Updated missing_values.missing_value_correlation_with_target for feature: {feature} to {correlation}")
# #             else:
# #                 logging.warning(f"Feature {feature} not found in metadata.")
# #     else:
# #         logging.warning("No missing values correlations with target found in the results.")
# #     return metadata


# # def update_missing_values_pair_correlations(metadata, results, paths, train_sample):
# #     """
# #     Update metadata with missing values' pair correlations.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results containing pair correlations.
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     if 'missing_value_pair_correlations' in results:
# #         pair_correlations = results['missing_value_pair_correlations']
# #         logging.debug(f"Pair correlations found: {pair_correlations}")
# #         for item in pair_correlations:
# #             feature1 = item['Feature1']
# #             feature2 = item['Feature2']
# #             correlation = item['Correlation']
# #             if feature1 in metadata['features']:
# #                 if 'missing_values' not in metadata['features'][feature1]:
# #                     metadata['features'][feature1]['missing_values'] = {}
# #                 if 'missing_value_correlation_with_other_features' not in metadata['features'][feature1]['missing_values']:
# #                     metadata['features'][feature1]['missing_values']['missing_value_correlation_with_other_features'] = {}
# #                 metadata['features'][feature1]['missing_values']['missing_value_correlation_with_other_features'][feature2] = correlation
# #                 logging.debug(f"Updated missing_values.missing_value_correlation_with_other_features for feature: {feature1} to {correlation} with {feature2}")
# #             else:
# #                 logging.warning(f"Feature {feature1} not found in metadata.")
# #     else:
# #         logging.warning("No missing values pair correlations found in the results.")
# #     return metadata


# # def update_data_overview(metadata, results, paths, train_sample):
# #     """
# #     Update metadata with data overview analysis results.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results.
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.
# #     """
# #     if 'data_overview' in results:
# #         if 'data_overview' not in metadata:
# #             metadata['data_overview'] = {}
# #         if isinstance(metadata['data_overview'], dict) and isinstance(results['data_overview'], dict):
# #             metadata['data_overview'].update(results['data_overview'])
# #         else:
# #             logging.error("Type mismatch in data_overview update.")
# #         logging.debug("Updated data_overview.")





        

# # def update_data_types_initial(metadata, results, paths, train_sample):
# #     if 'data_types' in results:
# #         for feature, dtype in results['data_types'].items():
# #             metadata = update_feature_metadata(metadata, feature, {'technical_data_type': dtype})
# #             logging.debug(f"Updated technical_data_type for feature: {feature} to {dtype}")


# # def update_stat_summary(metadata, results, paths, train_sample):
# #     if 'stat_summary' in results:
# #         for feature, stats in results['stat_summary'].items():
# #             metadata = update_feature_metadata(metadata, feature, {'summary_statistics': stats})
# #             logging.debug(f"Updated summary_statistics for feature: {feature} to {stats}")


# # def update_feature_balance(metadata, results, paths, train_sample):
# #     if 'feature_balance' in results:
# #         for feature, balance_data in results['feature_balance'].items():
# #             metadata = update_feature_metadata(metadata, feature, {'balance': {'most_common_value_weight': balance_data['most_common_value_weight']}})
# #             logging.debug(f"Updated balance for feature: {feature} to {balance_data['most_common_value_weight']}")


# # def update_correlations(metadata, results, paths, train_sample):
# #     if 'correlations' in results:
# #         for feature, correlation_data in results['correlations'].items():
# #             metadata = update_feature_metadata(metadata, feature, {'correlations': {'feature_correlation_with_other_features': correlation_data}})
# #             logging.debug(f"Updated correlation_with_other_features for feature: {feature} to {correlation_data}")


# # def update_target_correlations(metadata, results, paths, train_sample):
# #     if 'target_correlations' in results:
# #         for feature, correlation in results['target_correlations'].items():
# #             metadata = update_feature_metadata(metadata, feature, {'correlations': {'feature_correlation_with_target': correlation}})
# #             logging.debug(f"Updated correlation_with_target for feature: {feature} to {correlation}")


# # def update_target_analysis(metadata, results, paths, train_sample):
# #     if 'target_analysis' in results:
# #         metadata['data_overview']['target_variable_analysis'] = results['target_analysis']
# #         logging.debug("Updated target_variable_analysis.")


# # def update_metadata_with_descriptions(metadata, results, paths, train_sample):
# #     """
# #     Update metadata with feature descriptions.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results (not used in this function).
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     try:
# #         descriptions_data_path = paths['config']['feature_descriptions']
# #         descriptions_data = load_yaml(descriptions_data_path)

# #         for feature, details in descriptions_data['features'].items():
# #             metadata = update_feature_metadata(metadata, feature, {
# #                 'description': details['description'],
# #                 'security_context': details['security_context']
# #             })
# #         return metadata
# #     except Exception as e:
# #         logging.error(f"Error updating metadata with descriptions: {e}", exc_info=True)
# #         raise


# # def update_feature_classifications_initial(metadata, results, paths, train_sample):
# #     """
# #     Classify and update feature types in metadata.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results (not used in this function).
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     try:
# #         from src.analysis.data_understanding import feature_classification

# #         binary_features_auto, categorical_features_auto, numerical_features_auto = feature_classification(train_sample)

# #         for feature in binary_features_auto:
# #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'binary'})

# #         for feature in categorical_features_auto:
# #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'categorical'})

# #         for feature in numerical_features_auto:
# #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'numerical'})

# #         return metadata
# #     except Exception as e:
# #         logging.error(f"Error classifying and updating features: {e}")
# #         raise


# # def update_feature_classifications_manual(metadata, results, paths, train_sample):
# #     """
# #     Update metadata with manual feature classifications.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results (not used in this function).
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     try:
# #         manual_classifications_path = paths['config']['manual_feature_classification_update']
# #         manual_classifications = load_yaml(manual_classifications_path)

# #         for feature, details in manual_classifications['manual_feature_classification_updates'].items():
# #             classified_data_type = details['Manual Review and Update']
# #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': classified_data_type})
# #         return metadata
# #     except Exception as e:
# #         logging.error(f"Error updating manual feature classifications: {e}", exc_info=True)
# #         raise





# # def update_high_correlation_pairs(metadata, results, paths, train_sample):
# #     """
# #     Update metadata with high correlation pairs.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results.
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     try:
# #         if 'high_correlation_pairs' in results:
# #             high_corr_pairs = results['high_correlation_pairs']
# #             for pair_str, corr_data in high_corr_pairs.items():
# #                 feature1, feature2 = eval(pair_str)
# #                 metadata = update_feature_metadata(metadata, feature1, {
# #                     'correlations': {'feature_correlation_with_other_features': {feature2: corr_data['correlation']}}
# #                 })
# #                 metadata = update_feature_metadata(metadata, feature2, {
# #                     'correlations': {'feature_correlation_with_other_features': {feature1: corr_data['correlation']}}
# #                 })
# #         return metadata
# #     except Exception as e:
# #         logging.error(f"Error updating high correlation pairs: {e}", exc_info=True)
# #         raise


# # def update_imputation_strategy(metadata, results, paths, train_sample):
# #     """
# #     Update metadata with imputation strategies.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results.
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     if 'imputation_strategy' in results:
# #         imputation_data = results['imputation_strategy']
# #         logging.debug(f"Imputation strategies found: {imputation_data}")
# #         for item in imputation_data:
# #             feature = item['Feature']
# #             strategy = item['Imputation Strategy']
# #             if feature in metadata['features']:
# #                 if 'feature_engineering' not in metadata['features'][feature]:
# #                     metadata['features'][feature]['feature_engineering'] = {}
# #                 if 'imputation' not in metadata['features'][feature]['feature_engineering']:
# #                     metadata['features'][feature]['feature_engineering']['imputation'] = {}
# #                 metadata['features'][feature]['feature_engineering']['imputation']['type'] = strategy
# #                 logging.debug(f"Updated feature_engineering.imputation.type for feature: {feature} to {strategy}")
# #             else:
# #                 logging.warning(f"Feature {feature} not found in metadata.")
# #     else:
# #         logging.warning("No imputation strategies found in the results.")
# #     return metadata


# # def update_data_types_after_initial_cleaning(metadata, results, paths, train_sample):
# #     """
# #     Update the technical_data_type field in the metadata based on the analysis results after initial cleaning.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results containing feature data types.
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     data_types_path = os.path.join(paths['reports']['analysis_results'], 'feature_data_types.json')
# #     if os.path.isfile(data_types_path):
# #         with open(data_types_path, 'r') as f:
# #             data_types = json.load(f)
# #         for item in data_types:
# #             feature = item['Feature']
# #             technical_data_type = item['Technical Data Type']
# #             if feature in metadata['features']:
# #                 previous_data_type = metadata['features'][feature].get('technical_data_type', 'Not Set')
# #                 metadata['features'][feature]['technical_data_type'] = technical_data_type
# #                 logging.info(f"Updated technical_data_type for feature: {feature} from {previous_data_type} to {technical_data_type}")
# #             else:
# #                 logging.warning(f"Feature {feature} not found in metadata.")
# #     else:
# #         logging.warning(f"File {data_types_path} not found.")
# #     return metadata





# # # def execute_metadata_update_pipeline(train_sample, paths, analysis_results_dir, json_files, fields_to_update=None):
# # #     """
# # #     Execute the metadata update pipeline with various steps.

# # #     Parameters:
# # #     train_sample (pd.DataFrame): The training sample DataFrame.
# # #     paths (dict): Dictionary containing file paths.
# # #     analysis_results_dir (str): Directory for saving analysis results.
# # #     json_files (list): List of JSON files to initialize.
# # #     fields_to_update (list): List of fields to update in the metadata.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     try:
# # #         results = load_analysis_results(analysis_results_dir, json_files)
# # #         logging.debug(f"Loaded analysis results: {results.keys()}")
# # #         metadata_path = paths['config']['feature_metadata']
# # #         metadata = load_metadata(metadata_path)
# # #         schema_path = paths['config']['feature_metadata_schema']
# # #         schema = load_yaml(schema_path)
        
# # #         backup_metadata(paths)
        
# # #         metadata = apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update)

# # #         save_updated_metadata(metadata, paths, schema)
        
# # #         logging.debug("Metadata before displaying:")
# # #         display_metadata(metadata)
        
# # #         return metadata
# # #     except Exception as e:
# # #         logging.error(f"Error in metadata update pipeline: {e}", exc_info=True)
# # #         raise


# # # def load_analysis_results(analysis_results_dir, json_files):
# # #     """
# # #     Load analysis results from JSON files.

# # #     Parameters:
# # #     analysis_results_dir (str): Directory containing analysis result files.
# # #     json_files (list): List of JSON files to load.

# # #     Returns:
# # #     dict: Loaded analysis results.
# # #     """
# # #     results = {}
# # #     for file in json_files:
# # #         file_path = os.path.join(analysis_results_dir, file)
# # #         if os.path.isfile(file_path):
# # #             with open(file_path, 'r') as f:
# # #                 results[file.split('.')[0]] = json.load(f)
# # #             logging.debug(f"Loaded {file} from {file_path}")
# # #         else:
# # #             logging.warning(f"File {file} not found in {analysis_results_dir}")
# # #     return results


# # # def load_metadata(metadata_path):
# # #     """
# # #     Load metadata from a YAML file.

# # #     Parameters:
# # #     metadata_path (str): Path to the metadata file.

# # #     Returns:
# # #     dict: Loaded metadata.
# # #     """
# # #     try:
# # #         return load_yaml(metadata_path)
# # #     except Exception as e:
# # #         print(f"Error loading metadata: {e}")
# # #         raise


# # # def apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update=None):
# # #     """
# # #     Update metadata with analysis results.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     schema (dict): Schema to validate against.
# # #     results (dict): Analysis results.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.
# # #     fields_to_update (list): List of fields to update in the metadata.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     try:
# # #         update_functions = {
# # #             'data_overview': update_data_overview,
# # #             'data_types': update_data_types_initial,
# # #             'stat_summary': update_stat_summary,
# # #             'missing_values': update_missing_values,
# # #             'feature_balance': update_feature_balance,
# # #             'target_correlations': update_target_correlations,
# # #             'target_analysis': update_target_analysis,
# # #             'descriptions': update_metadata_with_descriptions,
# # #             'classifications_initial': update_feature_classifications_initial,
# # #             'classifications_manual': update_feature_classifications_manual,
# # #             'example_values': update_example_values,
# # #             'high_correlation_pairs': update_high_correlation_pairs,
# # #             'missing_values_target_correlations': update_missing_values_target_correlations,
# # #             'missing_values_pair_correlations': update_missing_values_pair_correlations,
# # #             'imputation_strategy': update_imputation_strategy,
# # #             'data_types_after_initial_cleaning': update_data_types_after_initial_cleaning 
# # #         }

# # #         fields_to_update = fields_to_update or update_functions.keys()

# # #         for field in fields_to_update:
# # #             if field in update_functions:
# # #                 update_functions[field](metadata, results, paths, train_sample)

# # #         return metadata 
# # #     except Exception as e:
# # #         logging.error(f"Error updating metadata with results: {e}", exc_info=True)
# # #         raise


# # # def update_feature_metadata(metadata, feature, updates):
# # #     """
# # #     Update the feature metadata with the provided updates.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     feature (str): Feature to update.
# # #     updates (dict): Dictionary of updates to apply.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     for key, value in updates.items():
# # #         if isinstance(value, dict):
# # #             if metadata['features'][feature].get(key) is None:
# # #                 metadata['features'][feature][key] = {}
# # #             for sub_key, sub_value in value.items():
# # #                 metadata['features'][feature][key][sub_key] = sub_value
# # #         else:
# # #             metadata['features'][feature][key] = value
# # #     return metadata


# # # def update_missing_values(metadata, results, paths, train_sample):
# # #     """
# # #     Update metadata with missing values analysis results.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.
# # #     """
# # #     missing_values = results.get('missing_values', {})
# # #     for feature, data in missing_values.items():
# # #         if feature in metadata['features']:
# # #             metadata['features'][feature]['missing_values'] = {
# # #                 'count': data['count'],
# # #                 'percentage': data['percentage'],
# # #                 'correlation_with_target': data.get('correlation_with_target', metadata['features'][feature]['missing_values'].get('correlation_with_target'))
# # #             }
# # #             logging.debug(f"Updated missing values for feature: {feature}")
# # #         else:
# # #             logging.warning(f"Feature {feature} not found in metadata.")


# # # def update_missing_values_target_correlations(metadata, results, paths, train_sample):
# # #     """
# # #     Update metadata with missing values' correlation with the target variable.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results containing missing values correlations.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     if 'missing_values_correlations_with_target_variable' in results:
# # #         correlations = results['missing_values_correlations_with_target_variable']
# # #         logging.debug(f"Correlations found: {correlations}")
# # #         for item in correlations:
# # #             feature = item['Feature']
# # #             correlation = item['Correlation']
# # #             if feature in metadata['features']:
# # #                 if 'missing_values' not in metadata['features'][feature]:
# # #                     metadata['features'][feature]['missing_values'] = {}
# # #                 metadata['features'][feature]['missing_values']['missing_values_correlation_with_target'] = correlation
# # #                 logging.debug(f"Updated missing_values.missing_values_correlation_with_target for feature: {feature} to {correlation}")
# # #             else:
# # #                 logging.warning(f"Feature {feature} not found in metadata.")
# # #     else:
# # #         logging.warning("No missing values correlations with target found in the results.")
# # #     return metadata


# # # def update_missing_values_pair_correlations(metadata, results, paths, train_sample):
# # #     """
# # #     Update metadata with missing values' pair correlations.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results containing pair correlations.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     if 'missing_value_pair_correlations' in results:
# # #         pair_correlations = results['missing_value_pair_correlations']
# # #         logging.debug(f"Pair correlations found: {pair_correlations}")
# # #         for item in pair_correlations:
# # #             feature1 = item['Feature1']
# # #             feature2 = item['Feature2']
# # #             correlation = item['Correlation']
# # #             if feature1 in metadata['features']:
# # #                 if 'missing_values' not in metadata['features'][feature1]:
# # #                     metadata['features'][feature1]['missing_values'] = {}
# # #                 if 'missing_values_correlation_with_other_features' not in metadata['features'][feature1]['missing_values']:
# # #                     metadata['features'][feature1]['missing_values']['missing_values_correlation_with_other_features'] = {}
# # #                 metadata['features'][feature1]['missing_values']['missing_values_correlation_with_other_features'][feature2] = correlation
# # #                 logging.debug(f"Updated missing_values.missing_values_correlation_with_other_features for feature: {feature1} to {correlation} with {feature2}")
# # #             else:
# # #                 logging.warning(f"Feature {feature1} not found in metadata.")
# # #     else:
# # #         logging.warning("No missing values pair correlations found in the results.")
# # #     return metadata


# # # def update_data_overview(metadata, results, paths, train_sample):
# # #     if 'data_overview' in results:
# # #         metadata['data_overview'] = results['data_overview']
# # #         logging.debug("Updated data_overview.")


# # # def update_data_types_initial(metadata, results, paths, train_sample):
# # #     if 'data_types' in results:
# # #         for feature, dtype in results['data_types'].items():
# # #             metadata = update_feature_metadata(metadata, feature, {'technical_data_type': dtype})
# # #             logging.debug(f"Updated technical_data_type for feature: {feature} to {dtype}")


# # # def update_stat_summary(metadata, results, paths, train_sample):
# # #     if 'stat_summary' in results:
# # #         for feature, stats in results['stat_summary'].items():
# # #             metadata = update_feature_metadata(metadata, feature, {'summary_statistics': stats})
# # #             logging.debug(f"Updated summary_statistics for feature: {feature} to {stats}")


# # # def update_feature_balance(metadata, results, paths, train_sample):
# # #     if 'feature_balance' in results:
# # #         for feature, balance_data in results['feature_balance'].items():
# # #             metadata = update_feature_metadata(metadata, feature, {'balance': {'most_common_value_weight': balance_data['most_common_value_weight']}})
# # #             logging.debug(f"Updated balance for feature: {feature} to {balance_data['most_common_value_weight']}")


# # # def update_correlations(metadata, results, paths, train_sample):
# # #     if 'correlations' in results:
# # #         for feature, correlation_data in results['correlations'].items():
# # #             metadata = update_feature_metadata(metadata, feature, {'correlation_with_other_features': correlation_data})
# # #             logging.debug(f"Updated correlation_with_other_features for feature: {feature} to {correlation_data}")


# # # def update_target_correlations(metadata, results, paths, train_sample):
# # #     if 'target_correlations' in results:
# # #         for feature, correlation in results['target_correlations'].items():
# # #             metadata = update_feature_metadata(metadata, feature, {'correlation_with_target': correlation})
# # #             logging.debug(f"Updated correlation_with_target for feature: {feature} to {correlation}")


# # # def update_target_analysis(metadata, results, paths, train_sample):
# # #     if 'target_analysis' in results:
# # #         metadata['data_overview']['target_variable_analysis'] = results['target_analysis']
# # #         logging.debug("Updated target_variable_analysis.")


# # # def update_metadata_with_descriptions(metadata, results, paths, train_sample):
# # #     """
# # #     Update metadata with feature descriptions.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results (not used in this function).
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     try:
# # #         descriptions_data_path = paths['config']['feature_descriptions']
# # #         descriptions_data = load_yaml(descriptions_data_path)

# # #         for feature, details in descriptions_data['features'].items():
# # #             metadata = update_feature_metadata(metadata, feature, {
# # #                 'description': details['description'],
# # #                 'security_context': details['security_context']
# # #             })
# # #         return metadata
# # #     except Exception as e:
# # #         logging.error(f"Error updating metadata with descriptions: {e}", exc_info=True)
# # #         raise


# # # def update_feature_classifications_initial(metadata, results, paths, train_sample):
# # #     """
# # #     Classify and update feature types in metadata.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results (not used in this function).
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     try:
# # #         from src.analysis.data_understanding import feature_classification

# # #         binary_features_auto, categorical_features_auto, numerical_features_auto = feature_classification(train_sample)

# # #         for feature in binary_features_auto:
# # #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'binary'})

# # #         for feature in categorical_features_auto:
# # #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'categorical'})

# # #         for feature in numerical_features_auto:
# # #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'numerical'})

# # #         return metadata
# # #     except Exception as e:
# # #         logging.error(f"Error classifying and updating features: {e}")
# # #         raise


# # # def update_feature_classifications_manual(metadata, results, paths, train_sample):
# # #     """
# # #     Update metadata with manual feature classifications.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results (not used in this function).
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     try:
# # #         manual_classifications_path = paths['config']['manual_feature_classification_update']
# # #         manual_classifications = load_yaml(manual_classifications_path)

# # #         for feature, details in manual_classifications['manual_feature_classification_updates'].items():
# # #             classified_data_type = details['Manual Review and Update']
# # #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': classified_data_type})
# # #         return metadata
# # #     except Exception as e:
# # #         logging.error(f"Error updating manual feature classifications: {e}", exc_info=True)
# # #         raise


# # # def update_example_values(metadata, results, paths, train_sample):
# # #     """
# # #     Update metadata with example values for features.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     def select_example_values(feature, dtype, dataframe):
# # #         if dtype in ['int64', 'float64']:
# # #             return [
# # #                 dataframe[feature].min(),
# # #                 dataframe[feature].max(),
# # #                 dataframe[feature].mean(),
# # #                 dataframe[feature].median(),
# # #                 dataframe[feature].std()
# # #             ]
# # #         elif dtype == 'object':
# # #             return dataframe[feature].value_counts().index.tolist()[:5]
# # #         else:
# # #             return dataframe[feature].unique().tolist()

# # #     try:
# # #         if 'data_types' in results:
# # #             for feature, dtype in results['data_types'].items():
# # #                 example_values = select_example_values(feature, dtype, train_sample)
# # #                 metadata = update_feature_metadata(metadata, feature, {'example_values': example_values})
# # #         return metadata
# # #     except Exception as e:
# # #         logging.error(f"Error updating example values: {e}", exc_info=True)
# # #         raise


# # # def update_high_correlation_pairs(metadata, results, paths, train_sample):
# # #     """
# # #     Update metadata with high correlation pairs.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     try:
# # #         if 'high_correlation_pairs' in results:
# # #             high_corr_pairs = results['high_correlation_pairs']
# # #             for pair_str, corr_data in high_corr_pairs.items():
# # #                 feature1, feature2 = eval(pair_str)
# # #                 metadata = update_feature_metadata(metadata, feature1, {
# # #                     'correlation_with_other_features': {feature2: corr_data['correlation']}
# # #                 })
# # #                 metadata = update_feature_metadata(metadata, feature2, {
# # #                     'correlation_with_other_features': {feature1: corr_data['correlation']}
# # #                 })
# # #         return metadata
# # #     except Exception as e:
# # #         logging.error(f"Error updating high correlation pairs: {e}", exc_info=True)
# # #         raise



# # # def update_feature_metadata(feature_metadata, feature, updates):
# # #     """
# # #     Update feature metadata with given updates.

# # #     Parameters:
# # #     feature_metadata (dict): Existing feature metadata.
# # #     feature (str): Feature name to update.
# # #     updates (dict): Dictionary containing updates.

# # #     Returns:
# # #     dict: Updated feature metadata.
# # #     """
# # #     logging.debug(f"Updating feature: {feature} with updates: {updates}")
# # #     if feature not in feature_metadata["features"]:
# # #         feature_metadata["features"][feature] = {}
# # #     for key, value in updates.items():
# # #         if isinstance(value, dict):
# # #             feature_metadata["features"][feature].setdefault(key, {})
# # #             for sub_key, sub_value in value.items():
# # #                 feature_metadata["features"][feature][key][sub_key] = sub_value
# # #                 logging.debug(f"Updated {key}.{sub_key} for {feature} to {sub_value}")
# # #         else:
# # #             feature_metadata["features"][feature][key] = value
# # #             logging.debug(f"Updated {key} for {feature} to {value}")
# # #     return feature_metadata


# # # def update_imputation_strategy(metadata, results, paths, train_sample):
# # #     """
# # #     Update metadata with imputation strategies.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     if 'imputation_strategy' in results:
# # #         imputation_data = results['imputation_strategy']
# # #         logging.debug(f"Imputation strategies found: {imputation_data}")
# # #         for item in imputation_data:
# # #             feature = item['Feature']
# # #             strategy = item['Imputation Strategy']
# # #             if feature in metadata['features']:
# # #                 if 'imputation' not in metadata['features'][feature]:
# # #                     metadata['features'][feature]['imputation'] = {}
# # #                 metadata['features'][feature]['imputation']['imputation_strategy'] = strategy
# # #                 logging.debug(f"Updated imputation.imputation_strategy for feature: {feature} to {strategy}")
# # #             else:
# # #                 logging.warning(f"Feature {feature} not found in metadata.")
# # #     else:
# # #         logging.warning("No imputation strategies found in the results.")
# # #     return metadata


# # # def update_data_types_after_initial_cleaning(metadata, results, paths, train_sample):
# # #     """
# # #     Update the technical_data_type field in the metadata based on the analysis results after initial cleaning.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results containing feature data types.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     data_types_path = os.path.join(paths['reports']['analysis_results'], 'feature_data_types.json')
# # #     if os.path.isfile(data_types_path):
# # #         with open(data_types_path, 'r') as f:
# # #             data_types = json.load(f)
# # #         for item in data_types:
# # #             feature = item['Feature']
# # #             technical_data_type = item['Technical Data Type']
# # #             if feature in metadata['features']:
# # #                 previous_data_type = metadata['features'][feature].get('technical_data_type', 'Not Set')
# # #                 metadata['features'][feature]['technical_data_type'] = technical_data_type
# # #                 logging.info(f"Updated technical_data_type for feature: {feature} from {previous_data_type} to {technical_data_type}")
# # #             else:
# # #                 logging.warning(f"Feature {feature} not found in metadata.")
# # #     else:
# # #         logging.warning(f"File {data_types_path} not found.")
# # #     return metadata

# # # def update_data_types_after_initial_cleaning(metadata, results, paths, train_sample):
# # #     """
# # #     Update the technical_data_type field in the metadata based on the analysis results after initial cleaning.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results containing feature data types.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     data_types_path = os.path.join(paths['reports']['analysis_results'], 'feature_data_types.json')
# # #     if os.path.isfile(data_types_path):
# # #         with open(data_types_path, 'r') as f:
# # #             data_types = json.load(f)
# # #         for item in data_types:
# # #             feature = item['Feature']
# # #             technical_data_type = item['Technical Data Type']
# # #             if feature in metadata['features']:
# # #                 metadata['features'][feature]['technical_data_type'] = technical_data_type
# # #                 logging.debug(f"Updated technical_data_type for feature: {feature} to {technical_data_type}")
# # #             else:
# # #                 logging.warning(f"Feature {feature} not found in metadata.")
# # #     else:
# # #         logging.warning(f"File {data_types_path} not found.")
# # #     return metadata


# # def save_updated_metadata(metadata, paths, schema):
# #     """
# #     Save updated metadata after ensuring completeness and converting to native types.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     paths (dict): Dictionary containing file paths.
# #     schema (dict): Schema to validate against.
# #     """
# #     try:
# #         metadata = ensure_metadata_completeness(metadata, schema)
# #         metadata = convert_to_native_types(metadata)
# #         save_metadata(metadata, paths)
# #     except Exception as e:
# #         logging.error(f"Error saving updated metadata: {e}", exc_info=True)
# #         raise


# # import logging

# # def ensure_metadata_completeness(metadata, schema):
# #     default_attributes = schema['metadata_schema']['default_attributes']
# #     for feature, attributes in metadata['features'].items():
# #         if not isinstance(attributes, dict):
# #             logging.error(f"Expected dictionary for feature '{feature}', but got {type(attributes)}: {attributes}")
# #             continue  # Skip this feature as it does not have the expected structure

# #         for attribute, default_value in default_attributes.items():
# #             try:
# #                 if attribute not in attributes:
# #                     if isinstance(default_value, dict):
# #                         attributes[attribute] = default_value.copy()
# #                     elif isinstance(default_value, list):
# #                         attributes[attribute] = default_value[:]
# #                     else:
# #                         attributes[attribute] = default_value
# #                 else:
# #                     # Ensure existing attribute matches the type of the default value
# #                     existing_value = attributes[attribute]
# #                     if isinstance(default_value, dict) and not isinstance(existing_value, dict):
# #                         logging.error(f"Type mismatch for feature '{feature}' attribute '{attribute}': expected dict, got {type(existing_value)}")
# #                         attributes[attribute] = default_value.copy()
# #                     elif isinstance(default_value, list) and not isinstance(existing_value, list):
# #                         logging.error(f"Type mismatch for feature '{feature}' attribute '{attribute}': expected list, got {type(existing_value)}")
# #                         attributes[attribute] = default_value[:]
# #                     elif not isinstance(default_value, (dict, list)) and type(existing_value) != type(default_value):
# #                         logging.error(f"Type mismatch for feature '{feature}' attribute '{attribute}': expected {type(default_value)}, got {type(existing_value)}")
# #                         attributes[attribute] = default_value
# #             except TypeError as e:
# #                 logging.error(f"Error for feature '{feature}' attribute '{attribute}' with value '{attributes[attribute]}': {e}")
# #                 raise
# #     return metadata




# # def save_metadata(metadata, paths):
# #     metadata_path = paths['config']['feature_metadata']
# #     save_yaml(metadata, metadata_path)
# #     logging.info(f"Metadata saved to {metadata_path}")




# # # Update Functions



# # # def convert_to_native_types(data):
# # #     """
# # #     Convert numpy data types to native Python types.

# # #     Parameters:
# # #     data: Data to convert.

# # #     Returns:
# # #     Data converted to native Python types.
# # #     """
# # #     if isinstance(data, np.integer):
# # #         return int(data)
# # #     elif isinstance(data, np.floating):
# # #         return float(data)
# # #     elif isinstance(data, np.ndarray):
# # #         return data.tolist()
# # #     elif isinstance(data, dict):
# # #         return {k: convert_to_native_types(v) for k, v in data.items()}
# # #     elif isinstance(data, list):
# # #         return [convert_to_native_types(v) for v in data]
# # #     else:
# # #         return data

# # def display_metadata(metadata):
# #     """
# #     Display metadata content.

# #     Parameters:
# #     metadata (dict): Metadata to display.
# #     """
# #     try:
# #         print("\n--- Metadata to be stored in config/feature_metadata.yaml ---\n")
# #         print(yaml.dump(metadata, default_flow_style=False))
# #     except Exception as e:
# #         logging.error(f"Error displaying metadata: {e}", exc_info=True)
# #         raise


# # def backup_metadata(paths):
# #     """
# #     Create a backup of the metadata file.

# #     Parameters:
# #     paths (dict): Dictionary containing file paths.
# #     """
# #     try:
# #         backup_dir = os.path.join('config', 'backup')
# #         os.makedirs(backup_dir, exist_ok=True)
# #         timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
# #         backup_path = os.path.join(backup_dir, f'feature_metadata_{timestamp}.yaml')
# #         shutil.copy(paths['config']['feature_metadata'], backup_path)
# #         print(f"Backup created at {backup_path}")
# #     except Exception as e:
# #         print(f"Error creating metadata backup: {e}")
# #         raise


# # def determine_feature_type(feature, metadata):
# #     """
# #     Determine the type of a feature.

# #     Parameters:
# #     feature (str): Feature name.
# #     metadata (dict): Metadata containing feature details.

# #     Returns:
# #     str: Determined feature type.
# #     """
# #     classified_type = metadata['features'][feature].get('classified_data_type', '')
# #     if classified_type in ['binary', 'categorical']:
# #         return classified_type
# #     example_values = metadata['features'][feature].get('example_values', [])
# #     if all(isinstance(val, (int, float)) for val in example_values):
# #         if example_values and isinstance(example_values[0], int):
# #             return 'categorical'
# #     return 'numerical'


# # def add_new_features_to_metadata(metadata_path, new_features, schema):
# #     """
# #     Add new features to the feature metadata file.

# #     Parameters:
# #     metadata_path (str): Path to the metadata YAML file.
# #     new_features (list): List of dictionaries, each containing details of a new feature.
# #     schema (dict): Schema to validate against.
# #     """
# #     metadata = load_yaml(metadata_path)

# #     for feature in new_features:
# #         feature_name = feature['Feature']
# #         if feature_name not in metadata['features']:
# #             metadata['features'][feature_name] = {}
# #             for key, value in schema['default_attributes'].items():
# #                 if isinstance(value, dict):
# #                     metadata['features'][feature_name][key] = feature.get(key, {})
# #                 elif isinstance(value, list):
# #                     metadata['features'][feature_name][key] = feature.get(key, [])
# #                 else:
# #                     metadata['features'][feature_name][key] = feature.get(key, value)
# #         else:
# #             print(f"Feature {feature_name} already exists in metadata.")

# #     save_yaml(metadata, metadata_path)









































# # # Commented out duplicate or less relevant functions
# # # def ensure_data_overview(paths):
# # #     """
# # #     Ensure the data_overview section is present in the feature metadata.

# # #     Parameters:
# # #     paths (dict): Dictionary containing file paths from paths.yaml.

# # #     Returns:
# # #     dict: The updated feature metadata.
# # #     dict: The metadata schema.
# # #     """
# # #     feature_metadata_path = paths['config']['feature_metadata']
# # #     metadata_schema = load_yaml(paths['config']['feature_metadata_schema'])

# # #     if not os.path.exists(feature_metadata_path):
# # #         feature_metadata = {
# # #             "data_overview": metadata_schema["data_overview"],
# # #             "features": {}
# # #         }
# # #     else:
# # #         feature_metadata = load_yaml(feature_metadata_path)
# # #         if "data_overview" not in feature_metadata:
# # #             feature_metadata["data_overview"] = metadata_schema["data_overview"]
# # #         if "features" not in feature_metadata:
# # #             feature_metadata["features"] = {}

# # #     save_yaml(feature_metadata, feature_metadata_path)
# # #     return feature_metadata, metadata_schema

# # # def populate_feature_metadata(feature_metadata, feature_descriptions, metadata_schema):
# # #     """
# # #     Populate feature metadata with descriptions.

# # #     Parameters:
# # #     feature_metadata (dict): The feature metadata to update.
# # #     feature_descriptions (dict): The feature descriptions to add.
# # #     metadata_schema (dict): The metadata schema to use for default attributes.
    
# # #     Returns:
# # #     dict: The updated feature metadata.
# # #     """
# # #     for feature, description in feature_descriptions['features'].items():
# # #         if feature not in feature_metadata["features"]:
# # #             feature_metadata["features"][feature] = metadata_schema['default_attributes'].copy()
# # #         feature_metadata["features"][feature]['description'] = description.get('description', '')
# # #         feature_metadata["features"][feature]['security_context'] = description.get('security_context', '')

# # #     return feature_metadata

# # # def load_manual_review(manual_review_path):
# # #     """
# # #     Load the manually updated review file.

# # #     Parameters:
# # #     manual_review_path (str): Path to the manual review file.

# # #     Returns:
# # #     DataFrame: Loaded manual review DataFrame.
# # #     """
# # #     try:
# # #         return pd.read_csv(manual_review_path)
# # #     except Exception as e:
# # #         print(f"Error loading manual review file: {e}")
# # #         raise

# # # def update_metadata_from_review(manual_review, metadata, schema):
# # #     """
# # #     Update the metadata based on manual review.

# # #     Parameters:
# # #     manual_review (DataFrame): DataFrame containing the manual review.
# # #     metadata (dict): Current feature metadata.
# # #     schema (dict): Schema for feature metadata.

# # #     Returns:
# # #     dict: Updated feature metadata.
# # #     """
# # #     try:
# # #         metadata = ensure_metadata_completeness(metadata, schema)
# # #         for index, row in manual_review.iterrows():
# # #             feature = row['Feature']
# # #             new_type = row['Manual Review and Update']
# # #             updates = {'classified_data_type': new_type}
# # #             metadata = update_feature_metadata(metadata, feature, updates)
# # #         return metadata
# # #     except Exception as e:
# # #         print(f"Error updating metadata from review: {e}")
# # #         raise

# # # def initialize_feature_metadata(schema_path):
# # #     """
# # #     Initialize feature metadata based on a schema.

# # #     Parameters:
# # #     schema_path (str): Path to the schema YAML file.

# # #     Returns:
# # #     dict: Initialized feature metadata.
# # #     """
# # #     try:
# # #         schema = load_yaml(schema_path)
# # #         feature_metadata = {"features": {}}
# # #         for attribute, default_value in schema["default_attributes"].items():
# # #             feature_metadata[attribute] = default_value
# # #         return feature_metadata
# # #     except Exception as e:
# # #         print(f"Error initializing feature metadata: {e}")
# # #         raise

# # # def update_metadata_with_data_types(metadata, feature_data_types, attribute_name):
# # #     """
# # #     Update metadata with feature data types.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     feature_data_types (list): List of feature data types.
# # #     attribute_name (str): Attribute name to update in the metadata.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     for item in feature_data_types:
# # #         feature = item['Feature']
# # #         data_type = item[attribute_name]
# # #         if feature in metadata['features']:
# # #             metadata['features'][feature][attribute_name] = data_type
# # #     return metadata



# # # def print_update_message(paths, config_key):
# # #     """
# # #     Print an update message for classified data types.

# # #     Parameters:
# # #     paths (dict): Dictionary containing file paths.
# # #     config_key (str): Configuration key indicating the path to update.
# # #     """
# # #     try:
# # #         config_path = paths['config'][config_key]
# # #         print(f"Updated classified_data_type attribute in {config_path}")
# # #     except Exception as e:
# # #         print(f"Error printing update message: {e}")
# # #         raise

# # # def update_classified_data_type(manual_updates, metadata, schema):
# # #     """
# # #     Update the classified_data_type attribute in the metadata based on manual updates.

# # #     Parameters:
# # #     manual_updates (dict): Dictionary containing manual feature classification updates.
# # #     metadata (dict): Existing feature metadata.
# # #     schema (dict): Schema for feature metadata.

# # #     Returns:
# # #     dict: Updated feature metadata.
# # #     """
# # #     try:
# # #         metadata = ensure_metadata_completeness(metadata, schema)
# # #         for feature, update in manual_updates.items():
# # #             new_type = update['Manual Review and Update']
# # #             updates = {'classified_data_type': new_type}
# # #             metadata = update_feature_metadata(metadata, feature, updates)
# # #         return metadata
# # #     except Exception as e:
# # #         print(f"Error updating metadata from review: {e}")
# # #         raise








# # # def execute_metadata_update_pipeline(train_sample, paths, analysis_results_dir, json_files, fields_to_update=None):
# # #     """
# # #     Execute the metadata update pipeline with various steps.

# # #     Parameters:
# # #     train_sample (pd.DataFrame): The training sample DataFrame.
# # #     paths (dict): Dictionary containing file paths.
# # #     analysis_results_dir (str): Directory for saving analysis results.
# # #     json_files (list): List of JSON files to initialize.
# # #     fields_to_update (list): List of fields to update in the metadata.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     try:
# # #         results = load_analysis_results(analysis_results_dir, json_files)
# # #         logging.debug(f"Loaded analysis results: {results.keys()}")
# # #         metadata_path = paths['config']['feature_metadata']
# # #         metadata = load_metadata(metadata_path)
# # #         schema_path = paths['config']['feature_metadata_schema']
# # #         schema = load_yaml(schema_path)
        
# # #         backup_metadata(paths)
        
# # #         metadata = apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update)

# # #         save_updated_metadata(metadata, paths, schema)
        
# # #         logging.debug("Metadata before displaying:")
# # #         display_metadata(metadata)
        
# # #         return metadata
# # #     except Exception as e:
# # #         logging.error(f"Error in metadata update pipeline: {e}", exc_info=True)
# # #         raise

# # # def load_analysis_results(analysis_results_dir, json_files):
# # #     """
# # #     Load analysis results from JSON files.

# # #     Parameters:
# # #     analysis_results_dir (str): Directory containing analysis result files.
# # #     json_files (list): List of JSON files to load.

# # #     Returns:
# # #     dict: Loaded analysis results.
# # #     """
# # #     results = {}
# # #     for file in json_files:
# # #         file_path = os.path.join(analysis_results_dir, file)
# # #         if os.path.isfile(file_path):
# # #             with open(file_path, 'r') as f:
# # #                 results[file.split('.')[0]] = json.load(f)
# # #             logging.debug(f"Loaded {file} from {file_path}")
# # #     return results

# # # def load_metadata(metadata_path):
# # #     """
# # #     Load metadata from a YAML file.

# # #     Parameters:
# # #     metadata_path (str): Path to the metadata file.

# # #     Returns:
# # #     dict: Loaded metadata.
# # #     """
# # #     try:
# # #         return load_yaml(metadata_path)
# # #     except Exception as e:
# # #         print(f"Error loading metadata: {e}")
# # #         raise

# # # def apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update=None):
# # #     """
# # #     Update metadata with analysis results.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     schema (dict): Schema to validate against.
# # #     results (dict): Analysis results.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.
# # #     fields_to_update (list): List of fields to update in the metadata.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     try:
# # #         update_functions = {
# # #             'data_overview': update_data_overview,
# # #             'data_types': update_data_types,
# # #             'stat_summary': update_stat_summary,
# # #             'missing_values': update_missing_values,
# # #             'feature_balance': update_feature_balance,
# # #             'target_correlations': update_target_correlations,
# # #             'target_analysis': update_target_analysis,
# # #             'descriptions': update_metadata_with_descriptions,
# # #             'classifications_initial': update_feature_classifications_initial,
# # #             'classifications_manual': update_feature_classifications_manual,
# # #             'example_values': update_example_values,
# # #             'high_correlation_pairs': update_high_correlation_pairs,
# # #             'missing_values_target_correlations': update_missing_values_target_correlations,
# # #             'missing_values_pair_correlations': update_missing_values_pair_correlations
# # #         }

# # #         fields_to_update = fields_to_update or update_functions.keys()

# # #         for field in fields_to_update:
# # #             if field in update_functions:
# # #                 update_functions[field](metadata, results, paths, train_sample)

# # #         return metadata
# # #     except Exception as e:
# # #         logging.error(f"Error updating metadata with results: {e}", exc_info=True)
# # #         raise

# # # def update_feature_metadata(metadata, feature, updates):
# # #     """
# # #     Update the feature metadata with the provided updates.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     feature (str): Feature to update.
# # #     updates (dict): Dictionary of updates to apply.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     for key, value in updates.items():
# # #         if isinstance(value, dict):
# # #             if metadata['features'][feature].get(key) is None:
# # #                 metadata['features'][feature][key] = {}
# # #             for sub_key, sub_value in value.items():
# # #                 metadata['features'][feature][key][sub_key] = sub_value
# # #         else:
# # #             metadata['features'][feature][key] = value
# # #     return metadata


# # # def update_missing_values(metadata, results, paths, train_sample):
# # #     """
# # #     Update metadata with missing values analysis results.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.
# # #     """
# # #     missing_values = results.get('missing_values', {})
# # #     for feature, data in missing_values.items():
# # #         if feature in metadata['features']:
# # #             metadata['features'][feature]['missing_values'] = {
# # #                 'count': data['count'],
# # #                 'percentage': data['percentage'],
# # #                 'correlation_with_target': data.get('correlation_with_target', metadata['features'][feature]['missing_values'].get('correlation_with_target'))
# # #             }
# # #             logging.debug(f"Updated missing values for feature: {feature}")
# # #         else:
# # #             logging.warning(f"Feature {feature} not found in metadata.")


# # # def load_analysis_results(analysis_results_dir, json_files):
# # #     """
# # #     Load analysis results from JSON files.

# # #     Parameters:
# # #     analysis_results_dir (str): Directory containing analysis result files.
# # #     json_files (list): List of JSON files to load.

# # #     Returns:
# # #     dict: Loaded analysis results.
# # #     """
# # #     results = {}
# # #     for file in json_files:
# # #         file_path = os.path.join(analysis_results_dir, file)
# # #         if os.path.isfile(file_path):
# # #             with open(file_path, 'r') as f:
# # #                 results[file.split('.')[0]] = json.load(f)
# # #             logging.debug(f"Loaded {file} from {file_path}")
# # #         else:
# # #             logging.warning(f"File {file} not found in {analysis_results_dir}")
# # #     return results

# # # def load_metadata(metadata_path):
# # #     """
# # #     Load metadata from a YAML file.

# # #     Parameters:
# # #     metadata_path (str): Path to the metadata file.

# # #     Returns:
# # #     dict: Loaded metadata.
# # #     """
# # #     try:
# # #         return load_yaml(metadata_path)
# # #     except Exception as e:
# # #         print(f"Error loading metadata: {e}")
# # #         raise

# # # def apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update=None):
# # #     """
# # #     Update metadata with analysis results.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     schema (dict): Schema to validate against.
# # #     results (dict): Analysis results.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.
# # #     fields_to_update (list): List of fields to update in the metadata.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     try:
# # #         update_functions = {
# # #             'data_overview': update_data_overview,
# # #             'data_types': update_data_types,
# # #             'stat_summary': update_stat_summary,
# # #             'missing_values': update_missing_values,
# # #             'feature_balance': update_feature_balance,
# # #             'target_correlations': update_target_correlations,
# # #             'target_analysis': update_target_analysis,
# # #             'descriptions': update_metadata_with_descriptions,
# # #             'classifications_initial': update_feature_classifications_initial,
# # #             'classifications_manual': update_feature_classifications_manual,
# # #             'example_values': update_example_values,
# # #             'high_correlation_pairs': update_high_correlation_pairs,
# # #             'missing_values_target_correlations': update_missing_values_target_correlations,
# # #             'missing_values_pair_correlations': update_missing_values_pair_correlations,
# # #         }

# # #         fields_to_update = fields_to_update or update_functions.keys()

# # #         for field in fields_to_update:
# # #             if field in update_functions:
# # #                 logging.debug(f"Updating field: {field}")
# # #                 update_functions[field](metadata, results, paths, train_sample)

# # #         return metadata
# # #     except Exception as e:
# # #         logging.error(f"Error updating metadata with results: {e}", exc_info=True)
# # #         raise


# # # def update_missing_values_target_correlations(metadata, results, paths, train_sample):
# # #     """
# # #     Update metadata with missing values' correlation with the target variable.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results containing missing values correlations.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     if 'missing_values_correlations_with_target_variable' in results:
# # #         correlations = results['missing_values_correlations_with_target_variable']
# # #         logging.debug(f"Correlations found: {correlations}")
# # #         for item in correlations:
# # #             feature = item['Feature']
# # #             correlation = item['Correlation']
# # #             if feature in metadata['features']:
# # #                 if 'missing_values' not in metadata['features'][feature]:
# # #                     metadata['features'][feature]['missing_values'] = {}
# # #                 metadata['features'][feature]['missing_values']['missing_values_correlation_with_target'] = correlation
# # #                 logging.debug(f"Updated missing_values.missing_values_correlation_with_target for feature: {feature} to {correlation}")
# # #             else:
# # #                 logging.warning(f"Feature {feature} not found in metadata.")
# # #     else:
# # #         logging.warning("No missing values correlations with target found in the results.")
# # #     return metadata

# # # def update_missing_values_pair_correlations(metadata, results, paths, train_sample):
# # #     """
# # #     Update metadata with missing values' pair correlations.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results containing pair correlations.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     if 'missing_value_pair_correlations' in results:
# # #         pair_correlations = results['missing_value_pair_correlations']
# # #         logging.debug(f"Pair correlations found: {pair_correlations}")
# # #         for item in pair_correlations:
# # #             feature1 = item['Feature1']
# # #             feature2 = item['Feature2']
# # #             correlation = item['Correlation']
# # #             if feature1 in metadata['features']:
# # #                 if 'missing_values' not in metadata['features'][feature1]:
# # #                     metadata['features'][feature1]['missing_values'] = {}
# # #                 if 'missing_values_correlation_with_other_features' not in metadata['features'][feature1]['missing_values']:
# # #                     metadata['features'][feature1]['missing_values']['missing_values_correlation_with_other_features'] = {}
# # #                 metadata['features'][feature1]['missing_values']['missing_values_correlation_with_other_features'][feature2] = correlation
# # #                 logging.debug(f"Updated missing_values.missing_values_correlation_with_other_features for feature: {feature1} to {correlation} with {feature2}")
# # #             else:
# # #                 logging.warning(f"Feature {feature1} not found in metadata.")
# # #     else:
# # #         logging.warning("No missing values pair correlations found in the results.")
# # #     return metadata

# # # def update_data_overview(metadata, results, paths, train_sample):
# # #     if 'data_overview' in results:
# # #         metadata['data_overview'] = results['data_overview']
# # #         logging.debug("Updated data_overview.")

# # # def update_data_types(metadata, results, paths, train_sample):
# # #     if 'data_types' in results:
# # #         for feature, dtype in results['data_types'].items():
# # #             metadata = update_feature_metadata(metadata, feature, {'technical_data_type': dtype})
# # #             logging.debug(f"Updated technical_data_type for feature: {feature} to {dtype}")

# # # def update_stat_summary(metadata, results, paths, train_sample):
# # #     if 'stat_summary' in results:
# # #         for feature, stats in results['stat_summary'].items():
# # #             metadata = update_feature_metadata(metadata, feature, {'summary_statistics': stats})
# # #             logging.debug(f"Updated summary_statistics for feature: {feature} to {stats}")

# # # def update_missing_values(metadata, results, paths, train_sample):
# # #     if 'missing_values' in results:
# # #         for feature, missing_data in results['missing_values'].items():
# # #             metadata = update_feature_metadata(metadata, feature, {'missing_values': missing_data})
# # #             logging.debug(f"Updated missing_values for feature: {feature} to {missing_data}")

# # # def update_feature_balance(metadata, results, paths, train_sample):
# # #     if 'feature_balance' in results:
# # #         for feature, balance_data in results['feature_balance'].items():
# # #             metadata = update_feature_metadata(metadata, feature, {'balance': {'most_common_value_weight': balance_data['most_common_value_weight']}})
# # #             logging.debug(f"Updated balance for feature: {feature} to {balance_data['most_common_value_weight']}")

# # # def update_correlations(metadata, results, paths, train_sample):
# # #     if 'correlations' in results:
# # #         for feature, correlation_data in results['correlations'].items():
# # #             metadata = update_feature_metadata(metadata, feature, {'correlation_with_other_features': correlation_data})
# # #             logging.debug(f"Updated correlation_with_other_features for feature: {feature} to {correlation_data}")

# # # def update_target_correlations(metadata, results, paths, train_sample):
# # #     if 'target_correlations' in results:
# # #         for feature, correlation in results['target_correlations'].items():
# # #             metadata = update_feature_metadata(metadata, feature, {'correlation_with_target': correlation})
# # #             logging.debug(f"Updated correlation_with_target for feature: {feature} to {correlation}")

# # # def update_target_analysis(metadata, results, paths, train_sample):
# # #     if 'target_analysis' in results:
# # #         metadata['data_overview']['target_variable_analysis'] = results['target_analysis']
# # #         logging.debug("Updated target_variable_analysis.")


    
# # # def update_metadata_with_descriptions(metadata, results, paths, train_sample):
# # #     """
# # #     Update metadata with feature descriptions.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results (not used in this function).
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     try:
# # #         descriptions_data_path = paths['config']['feature_descriptions']
# # #         descriptions_data = load_yaml(descriptions_data_path)

# # #         for feature, details in descriptions_data['features'].items():
# # #             metadata = update_feature_metadata(metadata, feature, {
# # #                 'description': details['description'],
# # #                 'security_context': details['security_context']
# # #             })
# # #         return metadata
# # #     except Exception as e:
# # #         logging.error(f"Error updating metadata with descriptions: {e}", exc_info=True)
# # #         raise

# # # def update_feature_classifications_initial(metadata, results, paths, train_sample):
# # #     """
# # #     Classify and update feature types in metadata.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results (not used in this function).
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     try:
# # #         from src.analysis.data_understanding import feature_classification

# # #         binary_features_auto, categorical_features_auto, numerical_features_auto = feature_classification(train_sample)

# # #         for feature in binary_features_auto:
# # #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'binary'})

# # #         for feature in categorical_features_auto:
# # #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'categorical'})

# # #         for feature in numerical_features_auto:
# # #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'numerical'})

# # #         return metadata
# # #     except Exception as e:
# # #         logging.error(f"Error classifying and updating features: {e}")
# # #         raise

# # # def update_feature_classifications_manual(metadata, results, paths, train_sample):
# # #     """
# # #     Update metadata with manual feature classifications.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results (not used in this function).
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     try:
# # #         manual_classifications_path = paths['config']['manual_feature_classification_update']
# # #         manual_classifications = load_yaml(manual_classifications_path)

# # #         for feature, details in manual_classifications['manual_feature_classification_updates'].items():
# # #             classified_data_type = details['Manual Review and Update']
# # #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': classified_data_type})
# # #         return metadata
# # #     except Exception as e:
# # #         logging.error(f"Error updating manual feature classifications: {e}", exc_info=True)
# # #         raise

# # # def update_example_values(metadata, results, paths, train_sample):
# # #     """
# # #     Update metadata with example values for features.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     def select_example_values(feature, dtype, dataframe):
# # #         if dtype in ['int64', 'float64']:
# # #             return [
# # #                 dataframe[feature].min(),
# # #                 dataframe[feature].max(),
# # #                 dataframe[feature].mean(),
# # #                 dataframe[feature].median(),
# # #                 dataframe[feature].std()
# # #             ]
# # #         elif dtype == 'object':
# # #             return dataframe[feature].value_counts().index.tolist()[:5]
# # #         else:
# # #             return dataframe[feature].unique().tolist()

# # #     try:
# # #         if 'data_types' in results:
# # #             for feature, dtype in results['data_types'].items():
# # #                 example_values = select_example_values(feature, dtype, train_sample)
# # #                 metadata = update_feature_metadata(metadata, feature, {'example_values': example_values})
# # #         return metadata
# # #     except Exception as e:
# # #         logging.error(f"Error updating example values: {e}", exc_info=True)
# # #         raise

# # # def update_high_correlation_pairs(metadata, results, paths, train_sample):
# # #     """
# # #     Update metadata with high correlation pairs.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     results (dict): Analysis results.
# # #     paths (dict): Dictionary containing file paths.
# # #     train_sample (pd.DataFrame): The training sample data.

# # #     Returns:
# # #     dict: Updated metadata.
# # #     """
# # #     try:
# # #         if 'high_correlation_pairs' in results:
# # #             high_corr_pairs = results['high_correlation_pairs']
# # #             for pair_str, corr_data in high_corr_pairs.items():
# # #                 feature1, feature2 = eval(pair_str)
# # #                 metadata = update_feature_metadata(metadata, feature1, {
# # #                     'correlation_with_other_features': {feature2: corr_data['correlation']}
# # #                 })
# # #                 metadata = update_feature_metadata(metadata, feature2, {
# # #                     'correlation_with_other_features': {feature1: corr_data['correlation']}
# # #                 })
# # #         return metadata
# # #     except Exception as e:
# # #         logging.error(f"Error updating high correlation pairs: {e}", exc_info=True)
# # #         raise

# # # def update_feature_metadata(feature_metadata, feature, updates):
# # #     """
# # #     Update feature metadata with given updates.

# # #     Parameters:
# # #     feature_metadata (dict): Existing feature metadata.
# # #     feature (str): Feature name to update.
# # #     updates (dict): Dictionary containing updates.

# # #     Returns:
# # #     dict: Updated feature metadata.
# # #     """
# # #     logging.debug(f"Updating feature: {feature} with updates: {updates}")
# # #     if feature not in feature_metadata["features"]:
# # #         feature_metadata["features"][feature] = {}
# # #     for key, value in updates.items():
# # #         if isinstance(value, dict):
# # #             feature_metadata["features"][feature].setdefault(key, {})
# # #             for sub_key, sub_value in value.items():
# # #                 feature_metadata["features"][feature][key][sub_key] = sub_value
# # #                 logging.debug(f"Updated {key}.{sub_key} for {feature} to {sub_value}")
# # #         else:
# # #             feature_metadata["features"][feature][key] = value
# # #             logging.debug(f"Updated {key} for {feature} to {value}")
# # #     return feature_metadata

# # # def save_updated_metadata(metadata, paths, schema):
# # #     """
# # #     Save updated metadata after ensuring completeness and converting to native types.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     paths (dict): Dictionary containing file paths.
# # #     schema (dict): Schema to validate against.
# # #     """
# # #     try:
# # #         metadata = ensure_metadata_completeness(metadata, schema)
# # #         metadata = convert_to_native_types(metadata)
# # #         save_metadata(metadata, paths)
# # #     except Exception as e:
# # #         print(f"Error saving updated metadata: {e}")
# # #         raise

# # # def ensure_metadata_completeness(metadata, schema):
# # #     """
# # #     Ensure metadata completeness based on a schema.

# # #     Parameters:
# # #     metadata (dict): Existing metadata.
# # #     schema (dict): Schema to validate against.

# # #     Returns:
# # #     dict: Complete metadata.
# # #     """
# # #     for feature in metadata['features']:
# # #         for attribute, default_value in schema['default_attributes'].items():
# # #             metadata['features'][feature].setdefault(attribute, default_value)
# # #     return metadata

# # # def save_metadata(metadata, paths):
# # #     """
# # #     Save metadata to a YAML file.

# # #     Parameters:
# # #     metadata (dict): Metadata to save.
# # #     paths (dict): Dictionary containing file paths.
# # #     """
# # #     try:
# # #         save_yaml(metadata, paths['config']['feature_metadata'])
# # #     except Exception as e:
# # #         print(f"Error saving metadata: {e}")
# # #         raise

# # # def display_metadata(metadata):
# # #     """
# # #     Display metadata content.

# # #     Parameters:
# # #     metadata (dict): Metadata to display.
# # #     """
# # #     try:
# # #         print("\n--- Metadata to be stored in config/feature_metadata.yaml ---\n")
# # #         print(yaml.dump(metadata, default_flow_style=False))
# # #     except Exception as e:
# # #         logging.error(f"Error displaying metadata: {e}", exc_info=True)
# # #         raise