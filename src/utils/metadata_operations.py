# src/utils/metadata_operations.py

import os
import json
import logging
import pandas as pd
from typing import Dict, Any
from src.utils.file_operations import load_json_file, save_json_file
from src.utils.json_pipeline import save_json_with_pipeline

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

logger = logging.getLogger(__name__)


def save_updated_metadata(metadata: dict, metadata_path: str) -> None:
    """
    Save the updated metadata to a JSON file.

    Parameters:
    metadata (dict): The metadata dictionary to save.
    metadata_path (str): The path to the metadata JSON file.

    Raises:
    Exception: If there is an error saving the metadata.
    """
    try:
        save_json_file(metadata, metadata_path)
        logging.info(f"Metadata saved to {metadata_path}")
    except Exception as e:
        logging.error(f"Error saving updated metadata: {e}")
        raise


def update_metadata_from_json(metadata: dict, json_files: list) -> dict:
    """
    Update the metadata based on a list of JSON files.

    Parameters:
    metadata (dict): The metadata dictionary to update.
    json_files (list): List of file paths to JSON files to use for updates.

    Returns:
    dict: The updated metadata.
    """
    try:
        for json_file in json_files:
            update_data = load_json_file(json_file)
            logging.info(f"Processing update data from: {json_file}")
            if "balance" in update_data:
                _merge_balance_data(metadata, update_data)
            else:
                _merge_dicts(metadata, update_data)
            logging.info(f"Updated metadata with data from: {json_file}")
        return metadata
    except Exception as e:
        logging.error(f"Error updating metadata: {e}")
        raise

def _merge_balance_data(metadata: dict, balance_data: dict) -> None:
    """
    Merge balance data into the metadata structure.

    Parameters:
    metadata (dict): The original metadata dictionary.
    balance_data (dict): The balance data to merge into the metadata.
    """
    try:
        for feature, balance_info in balance_data.items():
            if feature in metadata["features"]:
                if "balance" not in metadata["features"][feature]:
                    metadata["features"][feature]["balance"] = {}
                metadata["features"][feature]["balance"].update(balance_info["balance"])
                logging.info(f"Balance data updated for feature: {feature}")
            else:
                logging.warning(f"Feature {feature} not found in metadata, skipping balance update.")
    except Exception as e:
        logging.error(f"Error merging balance data: {e}")
        raise

def _merge_dicts(original: dict, updates: dict) -> None:
    """
    Recursively merge two dictionaries.

    Parameters:
    original (dict): The original dictionary to update.
    updates (dict): The updates to apply to the original dictionary.
    """
    for key, value in updates.items():
        if isinstance(value, dict) and key in original:
            _merge_dicts(original[key], value)
        else:
            original[key] = value


def dynamic_metadata_pipeline(metadata_path: str, json_files: list, schema_path: str) -> None:
    """
    The dynamic pipeline for loading, updating, validating, and saving metadata.

    Parameters:
    metadata_path (str): Path to the metadata JSON file.
    json_files (list): List of JSON files to update the metadata.
    schema_path (str): Path to the schema JSON file for validation.
    """
    try:
        # Load existing metadata
        metadata = load_json_file(metadata_path)
        
        # Load the schema
        schema = load_json_file(schema_path)
        
        # Insert logging here to validate schema type and content
        logging.info(f"Loaded schema type: {type(schema)}")
        logging.info(f"Loaded schema content: {schema}")
        
        # Update metadata dynamically based on JSON files
        updated_metadata = update_metadata_from_json(metadata, json_files)
        
        # Validate the updated metadata
        validate_metadata(updated_metadata, schema)  # Ensure schema is a dictionary
        
        # Save the updated metadata back to the file
        save_updated_metadata(updated_metadata, metadata_path)
        
        logging.info("Dynamic metadata pipeline completed successfully.")
    except Exception as e:
        logging.error(f"Error in dynamic metadata pipeline: {e}")
        raise


def validate_metadata(metadata: dict, schema: dict) -> bool:
    """
    Validate the metadata against the provided schema.

    Parameters:
    metadata (dict): The metadata dictionary to validate.
    schema (dict): The schema dictionary to validate against.

    Returns:
    bool: True if validation is successful, otherwise raises ValidationError.
    """
    try:
        from jsonschema import validate, ValidationError  # Importing here to ensure module availability
        validate(instance=metadata, schema=schema)
        logging.info("Metadata validation successful.")
        return True
    except ValidationError as e:
        logging.error(f"Metadata validation error: {e}")
        raise

import os
import pandas as pd
import logging
from typing import Dict, Any
from src.utils.json_pipeline import save_json_with_pipeline

import os
import pandas as pd
import logging
from typing import Dict, Any
from src.utils.json_pipeline import save_json_with_pipeline

def generate_example_values(dataframe: pd.DataFrame, save_dir: str, schema_path: str) -> Dict[str, Any]:
    """
    Generate example values for each feature in the dataframe and save them to a JSON file.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    save_dir (str): Directory to save the generated example values JSON file.
    schema_path (str): Path to the schema file to ensure the JSON structure.

    Returns:
    Dict[str, Any]: A dictionary structured according to the feature_metadata_complete_schema.json for example values.
    """
    try:
        example_values = {"features": {}}

        for column in dataframe.columns:
            dtype = dataframe[column].dtype.name
            if dtype in ['int64', 'float64']:
                values = [
                    dataframe[column].min(),
                    dataframe[column].max(),
                    dataframe[column].mean(),
                    dataframe[column].median(),
                    dataframe[column].std()
                ]
            elif dtype == 'object':
                values = dataframe[column].value_counts().index.tolist()[:5]
            else:
                values = dataframe[column].unique().tolist()

            example_values["features"][column] = {
                "additional_attributes": {
                    "example_values": values
                }
            }

        # Ensure the directory exists
        os.makedirs(save_dir, exist_ok=True)

        # Define the file path for the JSON file
        file_path = os.path.join(save_dir, 'example_values.json')

        # Save the example values to the JSON file, ensuring it conforms to the schema
        save_json_with_pipeline(example_values, file_path)

        logging.info(f"Example values saved to {file_path}")

        return example_values

    except Exception as e:
        logging.error(f"Error generating example values: {e}", exc_info=True)
        raise


def clear_example_values_from_feature_metadata(metadata_path: str) -> None:
    """
    Clear the example_values field from all features in the feature_metadata.json file.

    Parameters:
    metadata_path (str): Path to the feature_metadata.json file.
    """
    try:
        # Load the existing metadata
        metadata = load_json_file(metadata_path)

        # Iterate over each feature and clear the example_values field
        for feature, attributes in metadata.get("features", {}).items():
            if "additional_attributes" in attributes:
                if "example_values" in attributes["additional_attributes"]:
                    del attributes["additional_attributes"]["example_values"]
                    logger.info(f"Cleared example_values for feature: {feature}")

        # Save the updated metadata back to the file
        save_json_file(metadata, metadata_path)
        logger.info(f"Updated metadata saved to {metadata_path}")

    except Exception as e:
        logger.error(f"Error clearing example values from metadata: {e}", exc_info=True)
        raise


def extract_feature_metadata_for_classification(metadata: dict, feature: str) -> pd.DataFrame:
    """
    Extract relevant metadata for feature classification using the scalable pipeline.

    Parameters:
    metadata (dict): The metadata dictionary.
    feature (str): The feature name to extract metadata for.

    Returns:
    pd.DataFrame: A DataFrame containing the extracted metadata for the feature.
    """
    try:
        # Extract the necessary fields using the scalable approach
        extracted_data = {}
        extracted_data.update(extract_general_attributes(metadata, feature))
        extracted_data.update(extract_missing_values(metadata, feature))
        extracted_data.update(extract_correlations(metadata, feature))
        
        # Convert the extracted data into a DataFrame
        extracted_df = pd.DataFrame([extracted_data], index=[feature])
        
        logging.info(f"Extracted metadata for classification for feature: {feature}")
        return extracted_df
    except Exception as e:
        logging.error(f"Error extracting metadata for classification for feature: {feature}: {e}")
        raise


def extract_general_attributes(metadata: dict, feature: str) -> dict:
    general_attributes = metadata.get('features', {}).get(feature, {}).get('general_attributes', {})
    return {
        'Classified Data Type': general_attributes.get('classified_data_type', 'Unknown'),
        'Technical Data Type': general_attributes.get('technical_data_type', 'Unknown'),
        # Add more fields as needed
    }

def extract_missing_values(metadata: dict, feature: str) -> dict:
    missing_values = metadata.get('features', {}).get(feature, {}).get('missing_values', {})
    return {
        'Missing Percentage (%)': missing_values.get('percentage', 0.0),
        'Missing Count': missing_values.get('count', 0),
        # Add more fields as needed
    }

def extract_correlations(metadata: dict, feature: str) -> dict:
    correlations = metadata.get('features', {}).get(feature, {}).get('correlations', {})
    return {
        'Correlation with Target': correlations.get('feature_correlation_with_target', None),
        'Missing Values Correlation with Target': correlations.get('missing_value_correlation_with_target', None),
        # Add more fields as needed
    }

def extract_feature_metadata(metadata: dict, feature: str) -> pd.DataFrame:
    try:
        extracted_data = {}
        extracted_data.update(extract_general_attributes(metadata, feature))
        extracted_data.update(extract_missing_values(metadata, feature))
        extracted_data.update(extract_correlations(metadata, feature))
        # Add more extraction functions as needed

        return pd.DataFrame([extracted_data], index=[feature])
    except Exception as e:
        logging.error(f"Error extracting metadata for {feature}: {e}")
        raise


def custom_extraction_pipeline(metadata: dict, feature: str, fields_to_extract: list) -> pd.DataFrame:
    extracted_data = {}

    if 'general_attributes' in fields_to_extract:
        extracted_data.update(extract_general_attributes(metadata, feature))
    if 'missing_values' in fields_to_extract:
        extracted_data.update(extract_missing_values(metadata, feature))
    if 'correlations' in fields_to_extract:
        extracted_data.update(extract_correlations(metadata, feature))
    # Add conditions for other modules

    return pd.DataFrame([extracted_data], index=[feature])

        
def save_extracted_metadata(metadata_df: pd.DataFrame, output_path: str, format: str = 'csv') -> None:
    """
    Save the extracted metadata to a file in the specified format.

    Parameters:
    metadata_df (pd.DataFrame): The DataFrame containing the extracted metadata.
    output_path (str): The path to save the file.
    format (str): The format to save the file in ('csv', 'json', 'dataframe').
    """
    try:
        if format == 'csv':
            metadata_df.to_csv(output_path, index=False)
            logging.info(f"Extracted metadata saved as CSV to {output_path}")
        elif format == 'json':
            metadata_df.to_json(output_path, orient='records', lines=True)
            logging.info(f"Extracted metadata saved as JSON to {output_path}")
        elif format == 'dataframe':
            logging.info("Extracted metadata returned as DataFrame")
            return metadata_df
        else:
            raise ValueError("Unsupported format. Use 'csv', 'json', or 'dataframe'.")
    except Exception as e:
        logging.error(f"Error saving extracted metadata: {e}")
        raise


def extract_metadata_pipeline(metadata_path: str, features: list, output_dir: str, format: str = 'csv') -> Dict[str, pd.DataFrame]:
    """
    A pipeline to extract metadata for specific features and save it in the desired format.

    Parameters:
    metadata_path (str): Path to the metadata JSON file.
    features (list): List of feature names to extract metadata for.
    output_dir (str): Directory to save the extracted metadata files if not returning DataFrames.
    format (str): The format to save or return the extracted data ('csv', 'json', 'dataframe').

    Returns:
    Dict[str, pd.DataFrame]: A dictionary of DataFrames if format is 'dataframe', otherwise None.
    """
    try:
        # Load the metadata
        metadata = load_metadata(metadata_path)

        # Dictionary to store DataFrames if returning as such
        dataframes = {}

        # Extract and save metadata for each feature
        for feature in features:
            extracted_df = extract_feature_metadata(metadata, feature)
            if format == 'dataframe':
                dataframes[feature] = extracted_df
            else:
                output_path = os.path.join(output_dir, f"{feature}_metadata.{format}")
                save_extracted_metadata(extracted_df, output_path, format)

        logging.info("Metadata extraction pipeline completed successfully.")
        
        if format == 'dataframe':
            return dataframes
    except Exception as e:
        logging.error(f"Error in metadata extraction pipeline: {e}")
        raise


def load_metadata(metadata_path: str) -> dict:
    """
    Load metadata from a JSON file.

    Parameters:
    metadata_path (str): The path to the metadata JSON file.

    Returns:
    dict: The metadata dictionary.
    """
    try:
        with open(metadata_path, 'r') as file:
            metadata = json.load(file)
        logging.info(f"Metadata loaded from {metadata_path}")
        return metadata
    except Exception as e:
        logging.error(f"Error loading metadata: {e}")
        raise


def extract_classified_data_type(metadata: dict, feature: str) -> str:
    """
    Extract the classified data type from the metadata for a given feature.
    
    Parameters:
    metadata (dict): The metadata dictionary.
    feature (str): The feature name to extract metadata for.

    Returns:
    str: The classified data type of the feature.
    """
    return metadata.get('features', {}).get(feature, {}).get('general_attributes', {}).get('classified_data_type', 'Unknown')


def extract_missing_count(metadata: dict, feature: str) -> int:
    """
    Extract the missing count from the metadata for a given feature.
    
    Parameters:
    metadata (dict): The metadata dictionary.
    feature (str): The feature name to extract the missing count for.

    Returns:
    int: The missing count for the feature.
    """
    return metadata.get('features', {}).get(feature, {}).get('missing_values', {}).get('count', 0)


def extract_missing_percentage(metadata: dict, feature: str) -> float:
    """
    Extract the missing percentage from the metadata for a given feature.
    
    Parameters:
    metadata (dict): The metadata dictionary.
    feature (str): The feature name to extract the missing percentage for.

    Returns:
    float: The missing percentage for the feature.
    """
    return metadata.get('features', {}).get(feature, {}).get('missing_values', {}).get('percentage', None)


def extract_imputation_strategy(metadata: dict, feature: str) -> str:
    """
    Extract the imputation strategy from the metadata for a given feature.
    
    Parameters:
    metadata (dict): The metadata dictionary.
    feature (str): The feature name to extract metadata for.

    Returns:
    str: The imputation strategy for the feature.
    """
    return metadata.get('features', {}).get(feature, {}).get('feature_engineering', {}).get('imputation', {}).get('type', None)


def extract_technical_data_type(metadata: dict, feature: str) -> str:
    """
    Extract the technical data type from the metadata for a given feature.
    
    Parameters:
    metadata (dict): The metadata dictionary.
    feature (str): The feature name to extract metadata for.

    Returns:
    str: The technical data type of the feature.
    """
    return metadata.get('features', {}).get(feature, {}).get('general_attributes', {}).get('technical_data_type', 'Unknown')










# def generate_example_values(dataframe: pd.DataFrame, save_dir: str, schema_path: str) -> Dict[str, Any]:
#     """
#     Generate example values for each feature in the dataframe and save them to a JSON file.

#     Parameters:
#     dataframe (pd.DataFrame): The input DataFrame.
#     save_dir (str): Directory to save the generated example values JSON file.
#     schema_path (str): Path to the schema file to ensure the JSON structure.

#     Returns:
#     Dict[str, Any]: A dictionary structured according to the feature_metadata_complete_schema.json for example values.
#     """
#     try:
#         example_values = {}

#         for column in dataframe.columns:
#             dtype = dataframe[column].dtype.name
#             if dtype in ['int64', 'float64']:
#                 values = [
#                     dataframe[column].min(),
#                     dataframe[column].max(),
#                     dataframe[column].mean(),
#                     dataframe[column].median(),
#                     dataframe[column].std()
#                 ]
#             elif dtype == 'object':
#                 values = dataframe[column].value_counts().index.tolist()[:5]
#             else:
#                 values = dataframe[column].unique().tolist()

#             example_values[column] = {
#                 "additional_attributes": {
#                     "example_values": values
#                 }
#             }

#         os.makedirs(save_dir, exist_ok=True)
#         file_path = os.path.join(save_dir, 'example_values.json')
#         save_json_with_pipeline(example_values, file_path)

#         logging.info(f"Example values saved to {file_path}")

#         return example_values

#     except Exception as e:
#         logger.error(f"Error generating example values: {e}", exc_info=True)
#         raise



# def extract_feature_metadata(metadata: dict, feature: str) -> pd.DataFrame:
#     """
#     Extract specific metadata for a given feature.

#     Parameters:
#     metadata (dict): The metadata dictionary.
#     feature (str): The feature name to extract metadata for.

#     Returns:
#     pd.DataFrame: A DataFrame containing the extracted metadata for the feature.
#     """
#     try:
#         feature_data = metadata.get('features', {}).get(feature, {})
#         general_attributes = feature_data.get('general_attributes', {})
#         missing_values = feature_data.get('missing_values', {})

#         # Log the entire feature data to validate extraction
#         logging.info(f"Extracting metadata for feature: {feature}")
#         logging.info(f"Feature data: {feature_data}")

#         # Validate that missing percentage exists
#         missing_percentage = missing_values.get('percentage', None)
#         if missing_percentage is None:
#             logging.warning(f"Missing percentage not found for feature {feature}, defaulting to 0.")
#             missing_percentage = 0.0

#         extracted_data = {
#             'Classified Data Type': general_attributes.get('classified_data_type', 'Unknown'),
#             'Missing Percentage (%)': missing_percentage,
#             'Correlation with Target': feature_data.get('correlations', {}).get('feature_correlation_with_target', None),
#             'Missing Values Correlation with Target': feature_data.get('correlations', {}).get('missing_value_correlation_with_target', None)
#         }

#         extracted_df = pd.DataFrame([extracted_data], index=[feature])
#         logging.info(f"Successfully extracted metadata for feature: {feature}")
#         return extracted_df
#     except Exception as e:
#         logging.error(f"Error extracting metadata for {feature}: {e}")
#         raise





# def extract_feature_metadata(metadata: dict, feature: str) -> pd.DataFrame:
#     """
#     Extract specific metadata for a given feature.

#     Parameters:
#     metadata (dict): The metadata dictionary.
#     feature (str): The feature name to extract metadata for.

#     Returns:
#     pd.DataFrame: A DataFrame containing the extracted metadata for the feature.
#     """
#     try:
#         feature_data = metadata.get('features', {}).get(feature, {})
#         general_attributes = feature_data.get('general_attributes', {})
#         missing_values = feature_data.get('missing_values', {})

#         extracted_data = {
#             'Technical Data Type': general_attributes.get('technical_data_type', 'N/A'),
#             'Classified Data Type': general_attributes.get('classified_data_type', 'N/A'),
#             'Missing Values Count': missing_values.get('count', 0),
#             'Total Count': missing_values.get('total_count', 0),  # Ensure this field is present in the metadata
#             'Correlation with Target': feature_data.get('correlations', {}).get('feature_correlation_with_target', None),
#             'Missing Values Correlation with Target': feature_data.get('correlations', {}).get('missing_value_correlation_with_target', None)
#         }
#         extracted_df = pd.DataFrame([extracted_data], index=[feature])
#         logging.info(f"Extracted metadata for feature: {feature}")
#         return extracted_df
#     except Exception as e:
#         logging.error(f"Error extracting metadata for {feature}: {e}")
#         raise





# def extract_feature_metadata(metadata: dict, feature: str) -> pd.DataFrame:
#     """
#     Extract specific metadata for a given feature.

#     Parameters:
#     metadata (dict): The metadata dictionary.
#     feature (str): The feature name to extract metadata for.

#     Returns:
#     pd.DataFrame: A DataFrame containing the extracted metadata for the feature.
#     """
#     try:
#         feature_data = metadata.get('features', {}).get(feature, {})
#         general_attributes = feature_data.get('general_attributes', {})
#         extracted_data = {
#             'Technical Data Type': general_attributes.get('technical_data_type', 'N/A'),
#             'Classified Data Type': general_attributes.get('classified_data_type', 'N/A'),
#         }
#         extracted_df = pd.DataFrame([extracted_data], index=[feature])
#         logging.info(f"Extracted metadata for feature: {feature}")
#         return extracted_df
#     except Exception as e:
#         logging.error(f"Error extracting metadata for {feature}: {e}")
#         raise




# # Example usage
# if __name__ == "__main__":
#     metadata_path = 'config/feature_metadata.json'
#     features_to_extract = ['feature1', 'feature2']  # Replace with your actual features
#     output_directory = 'output/extracted_metadata'
#     os.makedirs(output_directory, exist_ok=True)
    
#     # Extract as CSV
#     extract_metadata_pipeline(metadata_path, features_to_extract, output_directory, format='csv')
    
#     # Extract as JSON
#     extract_metadata_pipeline(metadata_path, features_to_extract, output_directory, format='json')
    
#     # Extract as DataFrames
#     dfs = extract_metadata_pipeline(metadata_path, features_to_extract, output_directory, format='dataframe')
#     print(dfs)







# Old function adapted for JSON
# def determine_feature_type(metadata: Dict[str, Any], feature_name: str) -> str:
#     """
#     Determine the type of a feature.

#     Parameters:
#     metadata (Dict[str, Any]): The metadata dictionary.
#     feature_name (str): The name of the feature to determine the type.

#     Returns:
#     str: Determined feature type.
#     """
#     try:
#         # Check for the classified data type first
#         classified_type = metadata['features'][feature_name]['general_attributes'].get('classified_data_type', 'unknown')
#         if classified_type in ['binary', 'categorical']:
#             return classified_type
        
#         # Analyze example values to determine the type
#         example_values = metadata['features'][feature_name]['additional_attributes'].get('example_values', [])
#         if all(isinstance(val, (int, float)) for val in example_values):
#             if example_values and isinstance(example_values[0], int):
#                 return 'categorical'
#         return 'numerical'
    
#     except (KeyError, TypeError) as e:
#         logging.error(f"Error determining feature type for {feature_name}: {e}")
#         return 'unknown'

# def determine_feature_type(metadata: Dict[str, Any], feature_name: str) -> str:
#     """
#     Determine the feature type based on the `technical_data_type` in the metadata.

#     Parameters:
#     metadata (Dict[str, Any]): The metadata dictionary loaded from `config/feature_metadata.json`.
#     feature_name (str): The name of the feature to determine the type.

#     Returns:
#     str: The `technical_data_type` of the feature if found, otherwise 'unknown'.
#     """
#     try:
#         print(f"metadata type: {type(metadata)}")  # Debugging
#         print(f"metadata content: {metadata}")  # Debugging
#         print(f"feature_name: {feature_name}")  # Debugging

#         # Access the feature's general attributes in the metadata
#         feature_info = metadata.get("features", {}).get(feature_name, {})
#         print(f"feature_info: {feature_info}")  # Debugging

#         general_attributes = feature_info.get("general_attributes", {})
#         print(f"general_attributes: {general_attributes}")  # Debugging
        
#         # Retrieve the technical data type
#         feature_type = general_attributes.get("technical_data_type", "unknown")
        
#         if feature_type == "unknown":
#             logger.warning(f"Feature type for {feature_name} is unknown.")
        
#         return feature_type
#     except KeyError as e:
#         logger.error(f"KeyError: {e} while determining the feature type for {feature_name}.")
#         return "unknown"
#     except Exception as e:
#         logger.error(f"Error in determine_feature_type: {e}")
#         return "unknown"


        
# def dynamic_metadata_pipeline(metadata_path: str, json_files: list, schema_path: str) -> None:
#     """
#     The dynamic pipeline for loading, updating, validating, and saving metadata.

#     Parameters:
#     metadata_path (str): Path to the metadata JSON file.
#     json_files (list): List of JSON files to update the metadata.
#     schema_path (str): Path to the schema JSON file for validation.
#     """
#     try:
#         logging.info("Starting dynamic metadata pipeline.")
        
#         # Load existing metadata
#         metadata = load_json_file(metadata_path)
#         logging.info("Loaded existing metadata.")
        
#         # Load the schema
#         schema = load_json_file(schema_path)
#         logging.info("Loaded schema.")
#         logging.info(f"Loaded schema type: {type(schema)}")
#         logging.info(f"Loaded schema content: {schema}")
        
#         # Update metadata dynamically based on JSON files
#         updated_metadata = update_metadata_from_json(metadata, json_files)
#         logging.info("Metadata updated.")
        
#         # Validate the updated metadata
#         if validate_metadata(updated_metadata, schema):
#             logging.info("Updated metadata validated successfully.")
        
#         # Save the updated metadata back to the file
#         save_updated_metadata(updated_metadata, metadata_path)
#         logging.info("Dynamic metadata pipeline completed successfully.")
        
#     except Exception as e:
#         logging.error(f"Error in dynamic metadata pipeline: {e}")
#         raise








# """
# Module for handling dynamic metadata operations, including loading, updating, 
# and saving metadata based on various JSON files.
# """

# import os
# import json
# import logging
# from src.utils.file_operations import load_yaml, save_yaml, load_json_file, save_json_file
# from src.data.data_preparation import convert_to_native_types


# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# def save_updated_metadata(metadata: dict, metadata_path: str) -> None:
#     """
#     Save the updated metadata to a JSON file.

#     Parameters:
#     metadata (dict): The metadata dictionary to save.
#     metadata_path (str): The path to the metadata JSON file.

#     Raises:
#     Exception: If there is an error saving the metadata.
#     """
#     try:
#         save_json_file(metadata, metadata_path)
#         logging.info(f"Metadata saved to {metadata_path}")
#     except Exception as e:
#         logging.error(f"Error saving updated metadata: {e}")
#         raise

# def update_metadata_from_json(metadata: dict, json_files: list) -> dict:
#     """
#     Update the metadata based on a list of JSON files.

#     Parameters:
#     metadata (dict): The metadata dictionary to update.
#     json_files (list): List of file paths to JSON files to use for updates.

#     Returns:
#     dict: The updated metadata.
#     """
#     try:
#         for json_file in json_files:
#             update_data = load_json_file(json_file)
#             if "balance" in update_data:
#                 _merge_balance_data(metadata, update_data)
#             else:
#                 _merge_dicts(metadata, update_data)
#             logging.info(f"Updated metadata with data from: {json_file}")
#         return metadata
#     except Exception as e:
#         logging.error(f"Error updating metadata: {e}")
#         raise

# def _merge_balance_data(metadata: dict, balance_data: dict) -> None:
#     """
#     Merge balance data into the metadata structure.

#     Parameters:
#     metadata (dict): The original metadata dictionary.
#     balance_data (dict): The balance data to merge into the metadata.
#     """
#     for feature, balance_info in balance_data.items():
#         if feature in metadata["features"]:
#             if "balance" not in metadata["features"][feature]:
#                 metadata["features"][feature]["balance"] = {}
#             metadata["features"][feature]["balance"].update(balance_info["balance"])
#         else:
#             logging.warning(f"Feature {feature} not found in metadata, skipping balance update.")

# def _merge_dicts(original: dict, updates: dict) -> None:
#     """
#     Recursively merge two dictionaries.

#     Parameters:
#     original (dict): The original dictionary to update.
#     updates (dict): The updates to apply to the original dictionary.
#     """
#     for key, value in updates.items():
#         if isinstance(value, dict) and key in original:
#             _merge_dicts(original[key], value)
#         else:
#             original[key] = value

# def validate_metadata(metadata: dict, schema: dict) -> bool:
#     """
#     Validate the metadata against the provided schema.

#     Parameters:
#     metadata (dict): The metadata dictionary to validate.
#     schema (dict): The schema dictionary to validate against.

#     Returns:
#     bool: True if validation is successful, otherwise raises ValidationError.
#     """
#     try:
#         from jsonschema import validate, ValidationError  # Importing here to ensure module availability
#         validate(instance=metadata, schema=schema)
#         logging.info("Metadata validation successful.")
#         return True
#     except ValidationError as e:
#         logging.error(f"Metadata validation error: {e}")
#         raise

# def dynamic_metadata_pipeline(metadata_path: str, json_files: list, schema_path: str) -> None:
#     """
#     The dynamic pipeline for loading, updating, validating, and saving metadata.

#     Parameters:
#     metadata_path (str): Path to the metadata JSON file.
#     json_files (list): List of JSON files to update the metadata.
#     schema_path (str): Path to the schema JSON file for validation.
#     """
#     try:
#         # Load existing metadata
#         metadata = load_json_file(metadata_path)
        
#         # Load the schema
#         schema = load_json_file(schema_path)
        
#         # Update metadata dynamically based on JSON files
#         updated_metadata = update_metadata_from_json(metadata, json_files)
        
#         # Validate the updated metadata
#         validate_metadata(updated_metadata, schema)
        
#         # Save the updated metadata back to the file
#         save_updated_metadata(updated_metadata, metadata_path)
        
#         logging.info("Dynamic metadata pipeline completed successfully.")
#     except Exception as e:
#         logging.error(f"Error in dynamic metadata pipeline: {e}")
#         raise




# # src/utils/metadata_operations.py

# """
# Module for handling dynamic metadata operations, including loading, updating, 
# and saving metadata based on various JSON files.
# """

# import os
# import json
# import logging
# from src.utils.file_operations import load_yaml, save_yaml, load_json_file, save_json_file
# from src.data.data_preparation import convert_to_native_types


# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# def save_updated_metadata(metadata, metadata_path):
#     """
#     Save the updated metadata to a JSON file.

#     Parameters:
#     metadata (dict): The metadata dictionary to save.
#     metadata_path (str): The path to the metadata JSON file.

#     Raises:
#     Exception: If there is an error saving the metadata.
#     """
#     try:
#         save_json_file(metadata, metadata_path)
#         logging.info(f"Metadata saved to {metadata_path}")
#     except Exception as e:
#         logging.error(f"Error saving updated metadata: {e}")
#         raise




# def update_metadata_from_json(metadata, json_files):
#     """
#     Update the metadata based on a list of JSON files.

#     Parameters:
#     metadata (dict): The metadata dictionary to update.
#     json_files (list): List of file paths to JSON files to use for updates.

#     Returns:
#     dict: The updated metadata.
#     """
#     try:
#         for json_file in json_files:
#             update_data = load_json_file(json_file)
#             _merge_dicts(metadata, update_data)
#             logging.info(f"Updated metadata with data from: {json_file}")
#         return metadata
#     except Exception as e:
#         logging.error(f"Error updating metadata: {e}")
#         raise

# def _merge_dicts(original, updates):
#     """
#     Recursively merge two dictionaries.

#     Parameters:
#     original (dict): The original dictionary to update.
#     updates (dict): The updates to apply to the original dictionary.
#     """
#     for key, value in updates.items():
#         if isinstance(value, dict) and key in original:
#             _merge_dicts(original[key], value)
#         else:
#             original[key] = value

# def validate_metadata(metadata, schema):
#     """
#     Validate the metadata against the provided schema.

#     Parameters:
#     metadata (dict): The metadata dictionary to validate.
#     schema (dict): The schema dictionary to validate against.

#     Returns:
#     bool: True if validation is successful, otherwise raises ValidationError.
#     """
#     try:
#         from jsonschema import validate, ValidationError  # Importing here to ensure module availability
#         validate(instance=metadata, schema=schema)
#         logging.info("Metadata validation successful.")
#         return True
#     except ValidationError as e:
#         logging.error(f"Metadata validation error: {e}")
#         raise

# def dynamic_metadata_pipeline(metadata_path, json_files, schema_path):
#     """
#     The dynamic pipeline for loading, updating, validating, and saving metadata.

#     Parameters:
#     metadata_path (str): Path to the metadata JSON file.
#     json_files (list): List of JSON files to update the metadata.
#     schema_path (str): Path to the schema JSON file for validation.
#     """
#     try:
#         # Load existing metadata
#         metadata = load_json_file(metadata_path)
        
#         # Load the schema
#         schema = load_json_file(schema_path)
        
#         # Update metadata dynamically based on JSON files
#         updated_metadata = update_metadata_from_json(metadata, json_files)
        
#         # Validate the updated metadata
#         validate_metadata(updated_metadata, schema)
        
#         # Save the updated metadata back to the file
#         save_json_file(updated_metadata, metadata_path)
        
#         logging.info("Dynamic metadata pipeline completed successfully.")
#     except Exception as e:
#         logging.error(f"Error in dynamic metadata pipeline: {e}")
#         raise



        


        

# def execute_metadata_update_pipeline(train_sample, paths, analysis_results_dir, json_files, fields_to_update=None):
#     """
#     Execute the metadata update pipeline with various steps.

#     Parameters:
#     train_sample (pd.DataFrame): The training sample DataFrame.
#     paths (dict): Dictionary containing file paths.
#     analysis_results_dir (str): Directory for saving analysis results.
#     json_files (list): List of JSON files to initialize.
#     fields_to_update (list): List of fields to update in the metadata.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         results = load_analysis_results(analysis_results_dir, json_files)
#         logging.debug(f"Loaded analysis results: {results.keys()}")
#         metadata_path = paths['config']['feature_metadata']
#         metadata = load_metadata(metadata_path)
#         logging.debug(f"Loaded metadata: {metadata}")
#         schema_path = paths['config']['feature_metadata_schema']
#         schema = load_yaml(schema_path)
        
#         backup_metadata(paths)
        
#         metadata = apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update)

#         save_updated_metadata(metadata, paths, schema)
        
#         logging.debug("Metadata before displaying:")
#         display_metadata(metadata)
        
#         return metadata
#     except Exception as e:
#         logging.error(f"Error in metadata update pipeline: {e}", exc_info=True)
#         raise

# # def load_analysis_results(analysis_results_dir, json_files):
# #     """
# #     Load analysis results from JSON files.

# #     Parameters:
# #     analysis_results_dir (str): Directory containing analysis result files.
# #     json_files (list): List of JSON files to load.

#     Returns:
#     dict: Loaded analysis results.
#     """
#     results = {}
#     for file in json_files:
#         file_path = os.path.join(analysis_results_dir, file)
#         if os.path.isfile(file_path):
#             with open(file_path, 'r') as f:
#                 results[file.split('.')[0]] = json.load(f)
#             logging.debug(f"Loaded {file} from {file_path}")
#         else:
#             logging.warning(f"File {file} not found in {analysis_results_dir}")
#     return results

# def load_metadata(metadata_path):



# def apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update=None):
#     """
#     Update metadata with analysis results.

#     Parameters:
#     metadata (dict): Existing metadata.
#     schema (dict): Schema to validate against.
#     results (dict): Analysis results.
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.
#     fields_to_update (list): List of fields to update in the metadata.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         update_functions = {
#             'data_overview': update_data_overview,
#             'data_types': update_data_types_initial,
#             'stat_summary': update_stat_summary,
#             'missing_values': update_missing_values,
#             'feature_balance': update_feature_balance,
#             'target_correlations': update_target_correlations,
#             'target_analysis': update_target_analysis,
#             'descriptions': update_metadata_with_descriptions,
#             'classifications_initial': update_feature_classifications_initial,
#             'classifications_manual': update_feature_classifications_manual,
#             'example_values': update_example_values,
#             'high_correlation_pairs': update_high_correlation_pairs,
#             'missing_values_target_correlations': update_missing_values_target_correlations,
#             'missing_values_pair_correlations': update_missing_values_pair_correlations,
#             'imputation_strategy': update_imputation_strategy,
#             'data_types_after_initial_cleaning': update_data_types_after_initial_cleaning 
#         }

#         fields_to_update = fields_to_update or update_functions.keys()

#         for field in fields_to_update:
#             if field in update_functions:
#                 update_functions[field](metadata, results, paths, train_sample)

#         return metadata 
#     except Exception as e:
#         logging.error(f"Error updating metadata with results: {e}", exc_info=True)
#         raise



# def update_feature_metadata(metadata, feature, updates):



# def update_data_overview(metadata, results, paths, train_sample):







# def update_missing_values(metadata, results, paths, train_sample):
#     """
#     Update metadata with missing values analysis results.

#     Parameters:
#     metadata (dict): Existing metadata.
#     results (dict): Analysis results.
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.
#     """
#     missing_values = results.get('missing_values', {})
#     for feature, data in missing_values.items():
#         if feature in metadata['features']:
#             metadata['features'][feature]['missing_values'] = {
#                 'count': data['count'],
#                 'percentage': data['percentage']
#             }
#             logging.debug(f"Updated missing values for feature: {feature}")
#         else:
#             logging.warning(f"Feature {feature} not found in metadata.")


# # def update_missing_values_target_correlations(metadata, results, paths, train_sample):
# #     """
# #     Update metadata with missing values' correlation with the target variable.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results containing missing values correlations.
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     if 'missing_values_correlations_with_target_variable' in results:
# #         correlations = results['missing_values_correlations_with_target_variable']
# #         logging.debug(f"Correlations found: {correlations}")
# #         for item in correlations:
# #             feature = item['Feature']
# #             correlation = item['Correlation']
# #             if feature in metadata['features']:
# #                 if 'missing_values' not in metadata['features'][feature]:
# #                     metadata['features'][feature]['missing_values'] = {}
# #                 metadata['features'][feature]['missing_values']['missing_value_correlation_with_target'] = correlation
# #                 logging.debug(f"Updated missing_values.missing_value_correlation_with_target for feature: {feature} to {correlation}")
# #             else:
# #                 logging.warning(f"Feature {feature} not found in metadata.")
# #     else:
# #         logging.warning("No missing values correlations with target found in the results.")
# #     return metadata


# # def update_missing_values_pair_correlations(metadata, results, paths, train_sample):
# #     """
# #     Update metadata with missing values' pair correlations.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results containing pair correlations.
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     if 'missing_value_pair_correlations' in results:
# #         pair_correlations = results['missing_value_pair_correlations']
# #         logging.debug(f"Pair correlations found: {pair_correlations}")
# #         for item in pair_correlations:
# #             feature1 = item['Feature1']
# #             feature2 = item['Feature2']
# #             correlation = item['Correlation']
# #             if feature1 in metadata['features']:
# #                 if 'missing_values' not in metadata['features'][feature1]:
# #                     metadata['features'][feature1]['missing_values'] = {}
# #                 if 'missing_value_correlation_with_other_features' not in metadata['features'][feature1]['missing_values']:
# #                     metadata['features'][feature1]['missing_values']['missing_value_correlation_with_other_features'] = {}
# #                 metadata['features'][feature1]['missing_values']['missing_value_correlation_with_other_features'][feature2] = correlation
# #                 logging.debug(f"Updated missing_values.missing_value_correlation_with_other_features for feature: {feature1} to {correlation} with {feature2}")
# #             else:
# #                 logging.warning(f"Feature {feature1} not found in metadata.")
# #     else:
# #         logging.warning("No missing values pair correlations found in the results.")
# #     return metadata


# # def update_data_overview(metadata, results, paths, train_sample):
# #     """
# #     Update metadata with data overview analysis results.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results.
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.
# #     """
# #     if 'data_overview' in results:
# #         if 'data_overview' not in metadata:
# #             metadata['data_overview'] = {}
# #         if isinstance(metadata['data_overview'], dict) and isinstance(results['data_overview'], dict):
# #             metadata['data_overview'].update(results['data_overview'])
# #         else:
# #             logging.error("Type mismatch in data_overview update.")
# #         logging.debug("Updated data_overview.")





        

# # def update_data_types_initial(metadata, results, paths, train_sample):
# #     if 'data_types' in results:
# #         for feature, dtype in results['data_types'].items():
# #             metadata = update_feature_metadata(metadata, feature, {'technical_data_type': dtype})
# #             logging.debug(f"Updated technical_data_type for feature: {feature} to {dtype}")


# # def update_stat_summary(metadata, results, paths, train_sample):
# #     if 'stat_summary' in results:
# #         for feature, stats in results['stat_summary'].items():
# #             metadata = update_feature_metadata(metadata, feature, {'summary_statistics': stats})
# #             logging.debug(f"Updated summary_statistics for feature: {feature} to {stats}")


# # def update_feature_balance(metadata, results, paths, train_sample):
# #     if 'feature_balance' in results:
# #         for feature, balance_data in results['feature_balance'].items():
# #             metadata = update_feature_metadata(metadata, feature, {'balance': {'most_common_value_weight': balance_data['most_common_value_weight']}})
# #             logging.debug(f"Updated balance for feature: {feature} to {balance_data['most_common_value_weight']}")


# # def update_correlations(metadata, results, paths, train_sample):
# #     if 'correlations' in results:
# #         for feature, correlation_data in results['correlations'].items():
# #             metadata = update_feature_metadata(metadata, feature, {'correlations': {'feature_correlation_with_other_features': correlation_data}})
# #             logging.debug(f"Updated correlation_with_other_features for feature: {feature} to {correlation_data}")


# # def update_target_correlations(metadata, results, paths, train_sample):
# #     if 'target_correlations' in results:
# #         for feature, correlation in results['target_correlations'].items():
# #             metadata = update_feature_metadata(metadata, feature, {'correlations': {'feature_correlation_with_target': correlation}})
# #             logging.debug(f"Updated correlation_with_target for feature: {feature} to {correlation}")


# # def update_target_analysis(metadata, results, paths, train_sample):
# #     if 'target_analysis' in results:
# #         metadata['data_overview']['target_variable_analysis'] = results['target_analysis']
# #         logging.debug("Updated target_variable_analysis.")


# # def update_metadata_with_descriptions(metadata, results, paths, train_sample):
# #     """
# #     Update metadata with feature descriptions.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results (not used in this function).
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     try:
# #         descriptions_data_path = paths['config']['feature_descriptions']
# #         descriptions_data = load_yaml(descriptions_data_path)

# #         for feature, details in descriptions_data['features'].items():
# #             metadata = update_feature_metadata(metadata, feature, {
# #                 'description': details['description'],
# #                 'security_context': details['security_context']
# #             })
# #         return metadata
# #     except Exception as e:
# #         logging.error(f"Error updating metadata with descriptions: {e}", exc_info=True)
# #         raise


# # def update_feature_classifications_initial(metadata, results, paths, train_sample):
# #     """
# #     Classify and update feature types in metadata.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results (not used in this function).
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     try:
# #         from src.analysis.data_understanding import feature_classification

# #         binary_features_auto, categorical_features_auto, numerical_features_auto = feature_classification(train_sample)

# #         for feature in binary_features_auto:
# #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'binary'})

# #         for feature in categorical_features_auto:
# #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'categorical'})

# #         for feature in numerical_features_auto:
# #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'numerical'})

# #         return metadata
# #     except Exception as e:
# #         logging.error(f"Error classifying and updating features: {e}")
# #         raise


# # def update_feature_classifications_manual(metadata, results, paths, train_sample):
# #     """
# #     Update metadata with manual feature classifications.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results (not used in this function).
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     try:
# #         manual_classifications_path = paths['config']['manual_feature_classification_update']
# #         manual_classifications = load_yaml(manual_classifications_path)

# #         for feature, details in manual_classifications['manual_feature_classification_updates'].items():
# #             classified_data_type = details['Manual Review and Update']
# #             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': classified_data_type})
# #         return metadata
# #     except Exception as e:
# #         logging.error(f"Error updating manual feature classifications: {e}", exc_info=True)
# #         raise





# # def update_high_correlation_pairs(metadata, results, paths, train_sample):
# #     """
# #     Update metadata with high correlation pairs.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results.
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     try:
# #         if 'high_correlation_pairs' in results:
# #             high_corr_pairs = results['high_correlation_pairs']
# #             for pair_str, corr_data in high_corr_pairs.items():
# #                 feature1, feature2 = eval(pair_str)
# #                 metadata = update_feature_metadata(metadata, feature1, {
# #                     'correlations': {'feature_correlation_with_other_features': {feature2: corr_data['correlation']}}
# #                 })
# #                 metadata = update_feature_metadata(metadata, feature2, {
# #                     'correlations': {'feature_correlation_with_other_features': {feature1: corr_data['correlation']}}
# #                 })
# #         return metadata
# #     except Exception as e:
# #         logging.error(f"Error updating high correlation pairs: {e}", exc_info=True)
# #         raise


# # def update_imputation_strategy(metadata, results, paths, train_sample):
# #     """
# #     Update metadata with imputation strategies.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results.
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     if 'imputation_strategy' in results:
# #         imputation_data = results['imputation_strategy']
# #         logging.debug(f"Imputation strategies found: {imputation_data}")
# #         for item in imputation_data:
# #             feature = item['Feature']
# #             strategy = item['Imputation Strategy']
# #             if feature in metadata['features']:
# #                 if 'feature_engineering' not in metadata['features'][feature]:
# #                     metadata['features'][feature]['feature_engineering'] = {}
# #                 if 'imputation' not in metadata['features'][feature]['feature_engineering']:
# #                     metadata['features'][feature]['feature_engineering']['imputation'] = {}
# #                 metadata['features'][feature]['feature_engineering']['imputation']['type'] = strategy
# #                 logging.debug(f"Updated feature_engineering.imputation.type for feature: {feature} to {strategy}")
# #             else:
# #                 logging.warning(f"Feature {feature} not found in metadata.")
# #     else:
# #         logging.warning("No imputation strategies found in the results.")
# #     return metadata


# # def update_data_types_after_initial_cleaning(metadata, results, paths, train_sample):
# #     """
# #     Update the technical_data_type field in the metadata based on the analysis results after initial cleaning.

# #     Parameters:
# #     metadata (dict): Existing metadata.
# #     results (dict): Analysis results containing feature data types.
# #     paths (dict): Dictionary containing file paths.
# #     train_sample (pd.DataFrame): The training sample data.

# #     Returns:
# #     dict: Updated metadata.
# #     """
# #     data_types_path = os.path.join(paths['reports']['analysis_results'], 'feature_data_types.json')
# #     if os.path.isfile(data_types_path):
# #         with open(data_types_path, 'r') as f:
# #             data_types = json.load(f)
# #         for item in data_types:
# #             feature = item['Feature']
# #             technical_data_type = item['Technical Data Type']
# #             if feature in metadata['features']:
# #                 previous_data_type = metadata['features'][feature].get('technical_data_type', 'Not Set')
# #                 metadata['features'][feature]['technical_data_type'] = technical_data_type
# #                 logging.info(f"Updated technical_data_type for feature: {feature} from {previous_data_type} to {technical_data_type}")
# #             else:
# #                 logging.warning(f"Feature {feature} not found in metadata.")
# #     else:
# #         logging.warning(f"File {data_types_path} not found.")
# #     return metadata




