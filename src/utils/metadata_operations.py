# src/utils/metadata_operations.py

import os
import json
import logging
import pandas as pd
from typing import Dict, Any
from src.utils.file_operations import load_json_file, save_json_file
from src.utils.json_pipeline import save_json_with_pipeline
from collections import OrderedDict

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

logger = logging.getLogger(__name__)


def initialize_metadata_structure(schema: dict) -> dict:
    """
    Initialize a metadata structure based on the provided schema.

    Parameters:
    schema (dict): The schema that defines the structure of the metadata.

    Returns:
    dict: A dictionary initialized according to the schema structure.
    """
    initialized_metadata = {}
    for key, value in schema.get('properties', {}).items():
        if isinstance(value, dict) and value.get('type') == 'object':
            initialized_metadata[key] = initialize_metadata_structure(value)
        elif isinstance(value, dict) and value.get('type') == 'array':
            initialized_metadata[key] = []
        else:
            initialized_metadata[key] = None
    return initialized_metadata


def update_metadata_with_essential_features(metadata: dict, essential_features: dict, schema: dict) -> dict:
    """
    Update the metadata with essential missing value features, ensuring all keys are present and in order.

    Parameters:
    metadata (dict): The original metadata.
    essential_features (dict): The essential missing value features to add.
    schema (dict): The schema to ensure key consistency.

    Returns:
    dict: The updated metadata.
    """
    for feature_name, feature_data in essential_features.get("features", {}).items():
        if feature_name not in metadata["features"]:
            # Initialize the feature structure based on the schema
            metadata["features"][feature_name] = initialize_metadata_structure(schema["metadata_schema"]["properties"]["default_attributes"])
        
        # Update the metadata with the provided feature data
        metadata["features"][feature_name].update(feature_data)
    
    # Reorder the keys to match the schema order
    ordered_metadata = OrderedDict()
    for feature_name, feature_data in metadata["features"].items():
        ordered_metadata[feature_name] = OrderedDict()
        for key in schema["metadata_schema"]["properties"]["default_attributes"]["properties"]:
            ordered_metadata[feature_name][key] = feature_data.get(key)
    
    metadata["features"] = ordered_metadata
    return metadata


def create_data_overview_report(train_data: pd.DataFrame, test_data: pd.DataFrame, save_dir: str, step_name: str, report_file_name: str) -> None:
    """
    Create a JSON report for the data overview after each transformation step.

    Parameters:
    train_data (pd.DataFrame): The processed training data.
    test_data (pd.DataFrame): The processed testing data.
    save_dir (str): The directory where the JSON report should be saved.
    step_name (str): The name of the transformation step (e.g., 'missing_values_indicators_added').
    report_file_name (str): The name of the report file to save (e.g., 'data_overview.json').
    """
    try:
        data_overview = {
            "data_overview": {
                f"train_{step_name}_shape": list(train_data.shape),
                f"test_{step_name}_shape": list(test_data.shape),
                f"train_columns_{step_name}": train_data.columns.tolist(),
                f"test_columns_{step_name}": test_data.columns.tolist()
            }
        }

        save_path = os.path.join(save_dir, report_file_name)
        save_json_with_pipeline(data_overview, save_path)
        logging.info(f"Data overview report saved to {save_path}")
    except Exception as e:
        logging.error(f"Error creating data overview report: {e}")
        raise


def save_updated_metadata(metadata: dict, metadata_path: str) -> None:
    """
    Save the updated metadata to a JSON file.

    Parameters:
    metadata (dict): The metadata dictionary to save.
    metadata_path (str): The path to the metadata JSON file.

    Raises:
    Exception: If there is an error saving the metadata.
    """
    try:
        save_json_file(metadata, metadata_path)
        logging.info(f"Metadata saved to {metadata_path}")
    except Exception as e:
        logging.error(f"Error saving updated metadata: {e}")
        raise


def update_metadata_from_json(metadata: dict, json_files: list) -> dict:
    """
    Update the metadata based on a list of JSON files.

    Parameters:
    metadata (dict): The metadata dictionary to update.
    json_files (list): List of file paths to JSON files to use for updates.

    Returns:
    dict: The updated metadata.
    """
    try:
        for json_file in json_files:
            update_data = load_json_file(json_file)
            logging.info(f"Processing update data from: {json_file}")
            if "balance" in update_data:
                _merge_balance_data(metadata, update_data)
            else:
                _merge_dicts(metadata, update_data)
            logging.info(f"Updated metadata with data from: {json_file}")
        return metadata
    except Exception as e:
        logging.error(f"Error updating metadata: {e}")
        raise

def _merge_balance_data(metadata: dict, balance_data: dict) -> None:
    """
    Merge balance data into the metadata structure.

    Parameters:
    metadata (dict): The original metadata dictionary.
    balance_data (dict): The balance data to merge into the metadata.
    """
    try:
        for feature, balance_info in balance_data.items():
            if feature in metadata["features"]:
                if "balance" not in metadata["features"][feature]:
                    metadata["features"][feature]["balance"] = {}
                metadata["features"][feature]["balance"].update(balance_info["balance"])
                logging.info(f"Balance data updated for feature: {feature}")
            else:
                logging.warning(f"Feature {feature} not found in metadata, skipping balance update.")
    except Exception as e:
        logging.error(f"Error merging balance data: {e}")
        raise

def _merge_dicts(original: dict, updates: dict) -> None:
    """
    Recursively merge two dictionaries.

    Parameters:
    original (dict): The original dictionary to update.
    updates (dict): The updates to apply to the original dictionary.
    """
    for key, value in updates.items():
        if isinstance(value, dict) and key in original:
            _merge_dicts(original[key], value)
        else:
            original[key] = value


def dynamic_metadata_pipeline(metadata_path: str, json_files: list, schema_path: str) -> None:
    """
    The dynamic pipeline for loading, updating, validating, and saving metadata.

    Parameters:
    metadata_path (str): Path to the metadata JSON file.
    json_files (list): List of JSON files to update the metadata.
    schema_path (str): Path to the schema JSON file for validation.
    """
    try:
        # Load existing metadata
        metadata = load_json_file(metadata_path)
        
        # Load the schema
        schema = load_json_file(schema_path)
        
        # Insert logging here to validate schema type and content
        logging.info(f"Loaded schema type: {type(schema)}")
        logging.info(f"Loaded schema content: {schema}")
        
        # Update metadata dynamically based on JSON files
        updated_metadata = update_metadata_from_json(metadata, json_files)
        
        # Validate the updated metadata
        validate_metadata(updated_metadata, schema)  # Ensure schema is a dictionary
        
        # Save the updated metadata back to the file
        save_updated_metadata(updated_metadata, metadata_path)
        
        logging.info("Dynamic metadata pipeline completed successfully.")
    except Exception as e:
        logging.error(f"Error in dynamic metadata pipeline: {e}")
        raise


def validate_metadata(metadata: dict, schema: dict) -> bool:
    """
    Validate the metadata against the provided schema.

    Parameters:
    metadata (dict): The metadata dictionary to validate.
    schema (dict): The schema dictionary to validate against.

    Returns:
    bool: True if validation is successful, otherwise raises ValidationError.
    """
    try:
        from jsonschema import validate, ValidationError  # Importing here to ensure module availability
        validate(instance=metadata, schema=schema)
        logging.info("Metadata validation successful.")
        return True
    except ValidationError as e:
        logging.error(f"Metadata validation error: {e}")
        raise

import os
import pandas as pd
import logging
from typing import Dict, Any
from src.utils.json_pipeline import save_json_with_pipeline

import os
import pandas as pd
import logging
from typing import Dict, Any
from src.utils.json_pipeline import save_json_with_pipeline

def generate_example_values(dataframe: pd.DataFrame, save_dir: str, schema_path: str) -> Dict[str, Any]:
    """
    Generate example values for each feature in the dataframe and save them to a JSON file.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    save_dir (str): Directory to save the generated example values JSON file.
    schema_path (str): Path to the schema file to ensure the JSON structure.

    Returns:
    Dict[str, Any]: A dictionary structured according to the feature_metadata_complete_schema.json for example values.
    """
    try:
        example_values = {"features": {}}

        for column in dataframe.columns:
            dtype = dataframe[column].dtype.name
            if dtype in ['int64', 'float64']:
                values = [
                    dataframe[column].min(),
                    dataframe[column].max(),
                    dataframe[column].mean(),
                    dataframe[column].median(),
                    dataframe[column].std()
                ]
            elif dtype == 'object':
                values = dataframe[column].value_counts().index.tolist()[:5]
            else:
                values = dataframe[column].unique().tolist()

            example_values["features"][column] = {
                "additional_attributes": {
                    "example_values": values
                }
            }

        # Ensure the directory exists
        os.makedirs(save_dir, exist_ok=True)

        # Define the file path for the JSON file
        file_path = os.path.join(save_dir, 'example_values.json')

        # Save the example values to the JSON file, ensuring it conforms to the schema
        save_json_with_pipeline(example_values, file_path)

        logging.info(f"Example values saved to {file_path}")

        return example_values

    except Exception as e:
        logging.error(f"Error generating example values: {e}", exc_info=True)
        raise


def clear_example_values_from_feature_metadata(metadata_path: str) -> None:
    """
    Clear the example_values field from all features in the feature_metadata.json file.

    Parameters:
    metadata_path (str): Path to the feature_metadata.json file.
    """
    try:
        # Load the existing metadata
        metadata = load_json_file(metadata_path)

        # Iterate over each feature and clear the example_values field
        for feature, attributes in metadata.get("features", {}).items():
            if "additional_attributes" in attributes:
                if "example_values" in attributes["additional_attributes"]:
                    del attributes["additional_attributes"]["example_values"]
                    logger.info(f"Cleared example_values for feature: {feature}")

        # Save the updated metadata back to the file
        save_json_file(metadata, metadata_path)
        logger.info(f"Updated metadata saved to {metadata_path}")

    except Exception as e:
        logger.error(f"Error clearing example values from metadata: {e}", exc_info=True)
        raise


def extract_feature_metadata_for_classification(metadata: dict, feature: str) -> pd.DataFrame:
    """
    Extract relevant metadata for feature classification using the scalable pipeline.

    Parameters:
    metadata (dict): The metadata dictionary.
    feature (str): The feature name to extract metadata for.

    Returns:
    pd.DataFrame: A DataFrame containing the extracted metadata for the feature.
    """
    try:
        # Extract the necessary fields using the scalable approach
        extracted_data = {}
        extracted_data.update(extract_general_attributes(metadata, feature))
        extracted_data.update(extract_missing_values(metadata, feature))
        extracted_data.update(extract_correlations(metadata, feature))
        
        # Convert the extracted data into a DataFrame
        extracted_df = pd.DataFrame([extracted_data], index=[feature])
        
        logging.info(f"Extracted metadata for classification for feature: {feature}")
        return extracted_df
    except Exception as e:
        logging.error(f"Error extracting metadata for classification for feature: {feature}: {e}")
        raise


def extract_general_attributes(metadata: dict, feature: str) -> dict:
    general_attributes = metadata.get('features', {}).get(feature, {}).get('general_attributes', {})
    return {
        'Classified Data Type': general_attributes.get('classified_data_type', 'Unknown'),
        'Technical Data Type': general_attributes.get('technical_data_type', 'Unknown'),
        # Add more fields as needed
    }

def extract_missing_values(metadata: dict, feature: str) -> dict:
    missing_values = metadata.get('features', {}).get(feature, {}).get('missing_values', {})
    return {
        'Missing Percentage (%)': missing_values.get('percentage', 0.0),
        'Missing Count': missing_values.get('count', 0),
        # Add more fields as needed
    }

def extract_correlations(metadata: dict, feature: str) -> dict:
    correlations = metadata.get('features', {}).get(feature, {}).get('correlations', {})
    return {
        'Correlation with Target': correlations.get('feature_correlation_with_target', None),
        'Missing Values Correlation with Target': correlations.get('missing_value_correlation_with_target', None),
        # Add more fields as needed
    }

def extract_feature_metadata(metadata: dict, feature: str) -> pd.DataFrame:
    try:
        extracted_data = {}
        extracted_data.update(extract_general_attributes(metadata, feature))
        extracted_data.update(extract_missing_values(metadata, feature))
        extracted_data.update(extract_correlations(metadata, feature))
        # Add more extraction functions as needed

        return pd.DataFrame([extracted_data], index=[feature])
    except Exception as e:
        logging.error(f"Error extracting metadata for {feature}: {e}")
        raise


def custom_extraction_pipeline(metadata: dict, feature: str, fields_to_extract: list) -> pd.DataFrame:
    extracted_data = {}

    if 'general_attributes' in fields_to_extract:
        extracted_data.update(extract_general_attributes(metadata, feature))
    if 'missing_values' in fields_to_extract:
        extracted_data.update(extract_missing_values(metadata, feature))
    if 'correlations' in fields_to_extract:
        extracted_data.update(extract_correlations(metadata, feature))
    # Add conditions for other modules

    return pd.DataFrame([extracted_data], index=[feature])

        
def save_extracted_metadata(metadata_df: pd.DataFrame, output_path: str, format: str = 'csv') -> None:
    """
    Save the extracted metadata to a file in the specified format.

    Parameters:
    metadata_df (pd.DataFrame): The DataFrame containing the extracted metadata.
    output_path (str): The path to save the file.
    format (str): The format to save the file in ('csv', 'json', 'dataframe').
    """
    try:
        if format == 'csv':
            metadata_df.to_csv(output_path, index=False)
            logging.info(f"Extracted metadata saved as CSV to {output_path}")
        elif format == 'json':
            metadata_df.to_json(output_path, orient='records', lines=True)
            logging.info(f"Extracted metadata saved as JSON to {output_path}")
        elif format == 'dataframe':
            logging.info("Extracted metadata returned as DataFrame")
            return metadata_df
        else:
            raise ValueError("Unsupported format. Use 'csv', 'json', or 'dataframe'.")
    except Exception as e:
        logging.error(f"Error saving extracted metadata: {e}")
        raise


def extract_metadata_pipeline(metadata_path: str, features: list, output_dir: str, format: str = 'csv') -> Dict[str, pd.DataFrame]:
    """
    A pipeline to extract metadata for specific features and save it in the desired format.

    Parameters:
    metadata_path (str): Path to the metadata JSON file.
    features (list): List of feature names to extract metadata for.
    output_dir (str): Directory to save the extracted metadata files if not returning DataFrames.
    format (str): The format to save or return the extracted data ('csv', 'json', 'dataframe').

    Returns:
    Dict[str, pd.DataFrame]: A dictionary of DataFrames if format is 'dataframe', otherwise None.
    """
    try:
        # Load the metadata
        metadata = load_metadata(metadata_path)

        # Dictionary to store DataFrames if returning as such
        dataframes = {}

        # Extract and save metadata for each feature
        for feature in features:
            extracted_df = extract_feature_metadata(metadata, feature)
            if format == 'dataframe':
                dataframes[feature] = extracted_df
            else:
                output_path = os.path.join(output_dir, f"{feature}_metadata.{format}")
                save_extracted_metadata(extracted_df, output_path, format)

        logging.info("Metadata extraction pipeline completed successfully.")
        
        if format == 'dataframe':
            return dataframes
    except Exception as e:
        logging.error(f"Error in metadata extraction pipeline: {e}")
        raise


def load_metadata(metadata_path: str) -> dict:
    """
    Load metadata from a JSON file.

    Parameters:
    metadata_path (str): The path to the metadata JSON file.

    Returns:
    dict: The metadata dictionary.
    """
    try:
        with open(metadata_path, 'r') as file:
            metadata = json.load(file)
        logging.info(f"Metadata loaded from {metadata_path}")
        return metadata
    except Exception as e:
        logging.error(f"Error loading metadata: {e}")
        raise


def extract_classified_data_type(metadata: dict, feature: str) -> str:
    """
    Extract the classified data type from the metadata for a given feature.
    
    Parameters:
    metadata (dict): The metadata dictionary.
    feature (str): The feature name to extract metadata for.

    Returns:
    str: The classified data type of the feature.
    """
    return metadata.get('features', {}).get(feature, {}).get('general_attributes', {}).get('classified_data_type', 'Unknown')


def extract_missing_count(metadata: dict, feature: str) -> int:
    """
    Extract the missing count from the metadata for a given feature.
    
    Parameters:
    metadata (dict): The metadata dictionary.
    feature (str): The feature name to extract the missing count for.

    Returns:
    int: The missing count for the feature.
    """
    return metadata.get('features', {}).get(feature, {}).get('missing_values', {}).get('count', 0)


def extract_missing_percentage(metadata: dict, feature: str) -> float:
    """
    Extract the missing percentage from the metadata for a given feature.
    
    Parameters:
    metadata (dict): The metadata dictionary.
    feature (str): The feature name to extract the missing percentage for.

    Returns:
    float: The missing percentage for the feature.
    """
    return metadata.get('features', {}).get(feature, {}).get('missing_values', {}).get('percentage', None)


def extract_imputation_strategy(metadata: dict, feature: str) -> str:
    """
    Extract the imputation strategy from the metadata for a given feature.
    
    Parameters:
    metadata (dict): The metadata dictionary.
    feature (str): The feature name to extract metadata for.

    Returns:
    str: The imputation strategy for the feature.
    """
    return metadata.get('features', {}).get(feature, {}).get('feature_engineering', {}).get('imputation', {}).get('type', None)


def extract_technical_data_type(metadata: dict, feature: str) -> str:
    """
    Extract the technical data type from the metadata for a given feature.
    
    Parameters:
    metadata (dict): The metadata dictionary.
    feature (str): The feature name to extract metadata for.

    Returns:
    str: The technical data type of the feature.
    """
    return metadata.get('features', {}).get(feature, {}).get('general_attributes', {}).get('technical_data_type', 'Unknown')


def should_drop_feature(metadata: dict, feature: str) -> bool:
    """
    Determine whether a feature should be dropped based on various criteria in the metadata.
    
    This function checks the imputation strategy, classified data type, and other relevant
    metadata attributes to decide if a feature should be excluded from analysis.
    
    Parameters:
    metadata (dict): The metadata dictionary.
    feature (str): The feature name to evaluate.

    Returns:
    bool: True if the feature should be dropped, False otherwise.
    """
    # Check if the feature should be dropped based on imputation strategy
    imputation_strategy = extract_imputation_strategy(metadata, feature)
    if imputation_strategy == "Drop":
        logging.info(f"Feature '{feature}' marked for dropping due to imputation strategy.")
        return True

    # Add additional conditions for dropping features based on other metadata attributes


    # Future conditions can be added here

    return False


def extract_outlier_metrics(metadata_path: str) -> Dict[str, Dict[str, Any]]:
    """
    Extract outlier metrics from the metadata.

    Parameters:
    metadata_path (str): Path to the metadata JSON file.

    Returns:
    Dict[str, Dict[str, Any]]: Dictionary of outlier metrics for each feature.
    """
    try:
        # Ensure that metadata is loaded correctly from the file
        metadata = load_metadata(metadata_path)
        
        outlier_metrics = {
            feature: data['outliers']['metrics']
            for feature, data in metadata.get('features', {}).items()
            if 'outliers' in data and 'metrics' in data['outliers']
        }
        logging.info("Outlier metrics extracted successfully.")
        return outlier_metrics
    except Exception as e:
        logging.error(f"Error extracting outlier metrics: {e}")
        raise









# def generate_example_values(dataframe: pd.DataFrame, save_dir: str, schema_path: str) -> Dict[str, Any]:
#     """
#     Generate example values for each feature in the dataframe and save them to a JSON file.

#     Parameters:
#     dataframe (pd.DataFrame): The input DataFrame.
#     save_dir (str): Directory to save the generated example values JSON file.
#     schema_path (str): Path to the schema file to ensure the JSON structure.

#     Returns:
#     Dict[str, Any]: A dictionary structured according to the feature_metadata_complete_schema.json for example values.
#     """
#     try:
#         example_values = {}

#         for column in dataframe.columns:
#             dtype = dataframe[column].dtype.name
#             if dtype in ['int64', 'float64']:
#                 values = [
#                     dataframe[column].min(),
#                     dataframe[column].max(),
#                     dataframe[column].mean(),
#                     dataframe[column].median(),
#                     dataframe[column].std()
#                 ]
#             elif dtype == 'object':
#                 values = dataframe[column].value_counts().index.tolist()[:5]
#             else:
#                 values = dataframe[column].unique().tolist()

#             example_values[column] = {
#                 "additional_attributes": {
#                     "example_values": values
#                 }
#             }

#         os.makedirs(save_dir, exist_ok=True)
#         file_path = os.path.join(save_dir, 'example_values.json')
#         save_json_with_pipeline(example_values, file_path)

#         logging.info(f"Example values saved to {file_path}")

#         return example_values

#     except Exception as e:
#         logger.error(f"Error generating example values: {e}", exc_info=True)
#         raise



# def extract_feature_metadata(metadata: dict, feature: str) -> pd.DataFrame:
#     """
#     Extract specific metadata for a given feature.

#     Parameters:
#     metadata (dict): The metadata dictionary.
#     feature (str): The feature name to extract metadata for.

#     Returns:
#     pd.DataFrame: A DataFrame containing the extracted metadata for the feature.
#     """
#     try:
#         feature_data = metadata.get('features', {}).get(feature, {})
#         general_attributes = feature_data.get('general_attributes', {})
#         missing_values = feature_data.get('missing_values', {})

#         # Log the entire feature data to validate extraction
#         logging.info(f"Extracting metadata for feature: {feature}")
#         logging.info(f"Feature data: {feature_data}")

#         # Validate that missing percentage exists
#         missing_percentage = missing_values.get('percentage', None)
#         if missing_percentage is None:
#             logging.warning(f"Missing percentage not found for feature {feature}, defaulting to 0.")
#             missing_percentage = 0.0

#         extracted_data = {
#             'Classified Data Type': general_attributes.get('classified_data_type', 'Unknown'),
#             'Missing Percentage (%)': missing_percentage,
#             'Correlation with Target': feature_data.get('correlations', {}).get('feature_correlation_with_target', None),
#             'Missing Values Correlation with Target': feature_data.get('correlations', {}).get('missing_value_correlation_with_target', None)
#         }

#         extracted_df = pd.DataFrame([extracted_data], index=[feature])
#         logging.info(f"Successfully extracted metadata for feature: {feature}")
#         return extracted_df
#     except Exception as e:
#         logging.error(f"Error extracting metadata for {feature}: {e}")
#         raise





# def extract_feature_metadata(metadata: dict, feature: str) -> pd.DataFrame:
#     """
#     Extract specific metadata for a given feature.

#     Parameters:
#     metadata (dict): The metadata dictionary.
#     feature (str): The feature name to extract metadata for.

#     Returns:
#     pd.DataFrame: A DataFrame containing the extracted metadata for the feature.
#     """
#     try:
#         feature_data = metadata.get('features', {}).get(feature, {})
#         general_attributes = feature_data.get('general_attributes', {})
#         missing_values = feature_data.get('missing_values', {})

#         extracted_data = {
#             'Technical Data Type': general_attributes.get('technical_data_type', 'N/A'),
#             'Classified Data Type': general_attributes.get('classified_data_type', 'N/A'),
#             'Missing Values Count': missing_values.get('count', 0),
#             'Total Count': missing_values.get('total_count', 0),  # Ensure this field is present in the metadata
#             'Correlation with Target': feature_data.get('correlations', {}).get('feature_correlation_with_target', None),
#             'Missing Values Correlation with Target': feature_data.get('correlations', {}).get('missing_value_correlation_with_target', None)
#         }
#         extracted_df = pd.DataFrame([extracted_data], index=[feature])
#         logging.info(f"Extracted metadata for feature: {feature}")
#         return extracted_df
#     except Exception as e:
#         logging.error(f"Error extracting metadata for {feature}: {e}")
#         raise





# def extract_feature_metadata(metadata: dict, feature: str) -> pd.DataFrame:
#     """
#     Extract specific metadata for a given feature.

#     Parameters:
#     metadata (dict): The metadata dictionary.
#     feature (str): The feature name to extract metadata for.

#     Returns:
#     pd.DataFrame: A DataFrame containing the extracted metadata for the feature.
#     """
#     try:
#         feature_data = metadata.get('features', {}).get(feature, {})
#         general_attributes = feature_data.get('general_attributes', {})
#         extracted_data = {
#             'Technical Data Type': general_attributes.get('technical_data_type', 'N/A'),
#             'Classified Data Type': general_attributes.get('classified_data_type', 'N/A'),
#         }
#         extracted_df = pd.DataFrame([extracted_data], index=[feature])
#         logging.info(f"Extracted metadata for feature: {feature}")
#         return extracted_df
#     except Exception as e:
#         logging.error(f"Error extracting metadata for {feature}: {e}")
#         raise




# # Example usage
# if __name__ == "__main__":
#     metadata_path = 'config/feature_metadata.json'
#     features_to_extract = ['feature1', 'feature2']  # Replace with your actual features
#     output_directory = 'output/extracted_metadata'
#     os.makedirs(output_directory, exist_ok=True)
    
#     # Extract as CSV
#     extract_metadata_pipeline(metadata_path, features_to_extract, output_directory, format='csv')
    
#     # Extract as JSON
#     extract_metadata_pipeline(metadata_path, features_to_extract, output_directory, format='json')
    
#     # Extract as DataFrames
#     dfs = extract_metadata_pipeline(metadata_path, features_to_extract, output_directory, format='dataframe')
#     print(dfs)







# Old function adapted for JSON
# def determine_feature_type(metadata: Dict[str, Any], feature_name: str) -> str:
#     """
#     Determine the type of a feature.

#     Parameters:
#     metadata (Dict[str, Any]): The metadata dictionary.
#     feature_name (str): The name of the feature to determine the type.

#     Returns:
#     str: Determined feature type.
#     """
#     try:
#         # Check for the classified data type first
#         classified_type = metadata['features'][feature_name]['general_attributes'].get('classified_data_type', 'unknown')
#         if classified_type in ['binary', 'categorical']:
#             return classified_type
        
#         # Analyze example values to determine the type
#         example_values = metadata['features'][feature_name]['additional_attributes'].get('example_values', [])
#         if all(isinstance(val, (int, float)) for val in example_values):
#             if example_values and isinstance(example_values[0], int):
#                 return 'categorical'
#         return 'numerical'
    
#     except (KeyError, TypeError) as e:
#         logging.error(f"Error determining feature type for {feature_name}: {e}")
#         return 'unknown'

# def determine_feature_type(metadata: Dict[str, Any], feature_name: str) -> str:
#     """
#     Determine the feature type based on the `technical_data_type` in the metadata.

#     Parameters:
#     metadata (Dict[str, Any]): The metadata dictionary loaded from `config/feature_metadata.json`.
#     feature_name (str): The name of the feature to determine the type.

#     Returns:
#     str: The `technical_data_type` of the feature if found, otherwise 'unknown'.
#     """
#     try:
#         print(f"metadata type: {type(metadata)}")  # Debugging
#         print(f"metadata content: {metadata}")  # Debugging
#         print(f"feature_name: {feature_name}")  # Debugging

#         # Access the feature's general attributes in the metadata
#         feature_info = metadata.get("features", {}).get(feature_name, {})
#         print(f"feature_info: {feature_info}")  # Debugging

#         general_attributes = feature_info.get("general_attributes", {})
#         print(f"general_attributes: {general_attributes}")  # Debugging
        
#         # Retrieve the technical data type
#         feature_type = general_attributes.get("technical_data_type", "unknown")
        
#         if feature_type == "unknown":
#             logger.warning(f"Feature type for {feature_name} is unknown.")
        
#         return feature_type
#     except KeyError as e:
#         logger.error(f"KeyError: {e} while determining the feature type for {feature_name}.")
#         return "unknown"
#     except Exception as e:
#         logger.error(f"Error in determine_feature_type: {e}")
#         return "unknown"


        
# def dynamic_metadata_pipeline(metadata_path: str, json_files: list, schema_path: str) -> None:
#     """
#     The dynamic pipeline for loading, updating, validating, and saving metadata.

#     Parameters:
#     metadata_path (str): Path to the metadata JSON file.
#     json_files (list): List of JSON files to update the metadata.
#     schema_path (str): Path to the schema JSON file for validation.
#     """
#     try:
#         logging.info("Starting dynamic metadata pipeline.")
        
#         # Load existing metadata
#         metadata = load_json_file(metadata_path)
#         logging.info("Loaded existing metadata.")
        
#         # Load the schema
#         schema = load_json_file(schema_path)
#         logging.info("Loaded schema.")
#         logging.info(f"Loaded schema type: {type(schema)}")
#         logging.info(f"Loaded schema content: {schema}")
        
#         # Update metadata dynamically based on JSON files
#         updated_metadata = update_metadata_from_json(metadata, json_files)
#         logging.info("Metadata updated.")
        
#         # Validate the updated metadata
#         if validate_metadata(updated_metadata, schema):
#             logging.info("Updated metadata validated successfully.")
        
#         # Save the updated metadata back to the file
#         save_updated_metadata(updated_metadata, metadata_path)
#         logging.info("Dynamic metadata pipeline completed successfully.")
        
#     except Exception as e:
#         logging.error(f"Error in dynamic metadata pipeline: {e}")
#         raise








# """
# Module for handling dynamic metadata operations, including loading, updating, 
# and saving metadata based on various JSON files.
# """

# import os
# import json
# import logging
# from src.utils.file_operations import load_yaml, save_yaml, load_json_file, save_json_file
# from src.data.data_preparation import convert_to_native_types


# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# def save_updated_metadata(metadata: dict, metadata_path: str) -> None:
#     """
#     Save the updated metadata to a JSON file.

#     Parameters:
#     metadata (dict): The metadata dictionary to save.
#     metadata_path (str): The path to the metadata JSON file.

#     Raises:
#     Exception: If there is an error saving the metadata.
#     """
#     try:
#         save_json_file(metadata, metadata_path)
#         logging.info(f"Metadata saved to {metadata_path}")
#     except Exception as e:
#         logging.error(f"Error saving updated metadata: {e}")
#         raise

# def update_metadata_from_json(metadata: dict, json_files: list) -> dict:
#     """
#     Update the metadata based on a list of JSON files.

#     Parameters:
#     metadata (dict): The metadata dictionary to update.
#     json_files (list): List of file paths to JSON files to use for updates.

#     Returns:
#     dict: The updated metadata.
#     """
#     try:
#         for json_file in json_files:
#             update_data = load_json_file(json_file)
#             if "balance" in update_data:
#                 _merge_balance_data(metadata, update_data)
#             else:
#                 _merge_dicts(metadata, update_data)
#             logging.info(f"Updated metadata with data from: {json_file}")
#         return metadata
#     except Exception as e:
#         logging.error(f"Error updating metadata: {e}")
#         raise

# def _merge_balance_data(metadata: dict, balance_data: dict) -> None:
#     """
#     Merge balance data into the metadata structure.

#     Parameters:
#     metadata (dict): The original metadata dictionary.
#     balance_data (dict): The balance data to merge into the metadata.
#     """
#     for feature, balance_info in balance_data.items():
#         if feature in metadata["features"]:
#             if "balance" not in metadata["features"][feature]:
#                 metadata["features"][feature]["balance"] = {}
#             metadata["features"][feature]["balance"].update(balance_info["balance"])
#         else:
#             logging.warning(f"Feature {feature} not found in metadata, skipping balance update.")

# def _merge_dicts(original: dict, updates: dict) -> None:
#     """
#     Recursively merge two dictionaries.

#     Parameters:
#     original (dict): The original dictionary to update.
#     updates (dict): The updates to apply to the original dictionary.
#     """
#     for key, value in updates.items():
#         if isinstance(value, dict) and key in original:
#             _merge_dicts(original[key], value)
#         else:
#             original[key] = value

# def validate_metadata(metadata: dict, schema: dict) -> bool:
#     """
#     Validate the metadata against the provided schema.

#     Parameters:
#     metadata (dict): The metadata dictionary to validate.
#     schema (dict): The schema dictionary to validate against.

#     Returns:
#     bool: True if validation is successful, otherwise raises ValidationError.
#     """
#     try:
#         from jsonschema import validate, ValidationError  # Importing here to ensure module availability
#         validate(instance=metadata, schema=schema)
#         logging.info("Metadata validation successful.")
#         return True
#     except ValidationError as e:
#         logging.error(f"Metadata validation error: {e}")
#         raise

# def dynamic_metadata_pipeline(metadata_path: str, json_files: list, schema_path: str) -> None:
#     """
#     The dynamic pipeline for loading, updating, validating, and saving metadata.

#     Parameters:
#     metadata_path (str): Path to the metadata JSON file.
#     json_files (list): List of JSON files to update the metadata.
#     schema_path (str): Path to the schema JSON file for validation.
#     """
#     try:
#         # Load existing metadata
#         metadata = load_json_file(metadata_path)
        
#         # Load the schema
#         schema = load_json_file(schema_path)
        
#         # Update metadata dynamically based on JSON files
#         updated_metadata = update_metadata_from_json(metadata, json_files)
        
#         # Validate the updated metadata
#         validate_metadata(updated_metadata, schema)
        
#         # Save the updated metadata back to the file
#         save_updated_metadata(updated_metadata, metadata_path)
        
#         logging.info("Dynamic metadata pipeline completed successfully.")
#     except Exception as e:
#         logging.error(f"Error in dynamic metadata pipeline: {e}")
#         raise




# # src/utils/metadata_operations.py

# """
# Module for handling dynamic metadata operations, including loading, updating, 
# and saving metadata based on various JSON files.
# """

# import os
# import json
# import logging
# from src.utils.file_operations import load_yaml, save_yaml, load_json_file, save_json_file
# from src.data.data_preparation import convert_to_native_types


# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# def save_updated_metadata(metadata, metadata_path):
#     """
#     Save the updated metadata to a JSON file.

#     Parameters:
#     metadata (dict): The metadata dictionary to save.
#     metadata_path (str): The path to the metadata JSON file.

#     Raises:
#     Exception: If there is an error saving the metadata.
#     """
#     try:
#         save_json_file(metadata, metadata_path)
#         logging.info(f"Metadata saved to {metadata_path}")
#     except Exception as e:
#         logging.error(f"Error saving updated metadata: {e}")
#         raise




# def update_metadata_from_json(metadata, json_files):
#     """
#     Update the metadata based on a list of JSON files.

#     Parameters:
#     metadata (dict): The metadata dictionary to update.
#     json_files (list): List of file paths to JSON files to use for updates.

#     Returns:
#     dict: The updated metadata.
#     """
#     try:
#         for json_file in json_files:
#             update_data = load_json_file(json_file)
#             _merge_dicts(metadata, update_data)
#             logging.info(f"Updated metadata with data from: {json_file}")
#         return metadata
#     except Exception as e:
#         logging.error(f"Error updating metadata: {e}")
#         raise

# def _merge_dicts(original, updates):
#     """
#     Recursively merge two dictionaries.

#     Parameters:
#     original (dict): The original dictionary to update.
#     updates (dict): The updates to apply to the original dictionary.
#     """
#     for key, value in updates.items():
#         if isinstance(value, dict) and key in original:
#             _merge_dicts(original[key], value)
#         else:
#             original[key] = value

# def validate_metadata(metadata, schema):
#     """
#     Validate the metadata against the provided schema.

#     Parameters:
#     metadata (dict): The metadata dictionary to validate.
#     schema (dict): The schema dictionary to validate against.

#     Returns:
#     bool: True if validation is successful, otherwise raises ValidationError.
#     """
#     try:
#         from jsonschema import validate, ValidationError  # Importing here to ensure module availability
#         validate(instance=metadata, schema=schema)
#         logging.info("Metadata validation successful.")
#         return True
#     except ValidationError as e:
#         logging.error(f"Metadata validation error: {e}")
#         raise

# def dynamic_metadata_pipeline(metadata_path, json_files, schema_path):
#     """
#     The dynamic pipeline for loading, updating, validating, and saving metadata.

#     Parameters:
#     metadata_path (str): Path to the metadata JSON file.
#     json_files (list): List of JSON files to update the metadata.
#     schema_path (str): Path to the schema JSON file for validation.
#     """
#     try:
#         # Load existing metadata
#         metadata = load_json_file(metadata_path)
        
#         # Load the schema
#         schema = load_json_file(schema_path)
        
#         # Update metadata dynamically based on JSON files
#         updated_metadata = update_metadata_from_json(metadata, json_files)
        
#         # Validate the updated metadata
#         validate_metadata(updated_metadata, schema)
        
#         # Save the updated metadata back to the file
#         save_json_file(updated_metadata, metadata_path)
        
#         logging.info("Dynamic metadata pipeline completed successfully.")
#     except Exception as e:
#         logging.error(f"Error in dynamic metadata pipeline: {e}")
#         raise



        


        

# def execute_metadata_update_pipeline(train_sample, paths, analysis_results_dir, json_files, fields_to_update=None):
#     """
#     Execute the metadata update pipeline with various steps.

#     Parameters:
#     train_sample (pd.DataFrame): The training sample DataFrame.
#     paths (dict): Dictionary containing file paths.
#     analysis_results_dir (str): Directory for saving analysis results.
#     json_files (list): List of JSON files to initialize.
#     fields_to_update (list): List of fields to update in the metadata.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         results = load_analysis_results(analysis_results_dir, json_files)
#         logging.debug(f"Loaded analysis results: {results.keys()}")
#         metadata_path = paths['config']['feature_metadata']
#         metadata = load_metadata(metadata_path)
#         logging.debug(f"Loaded metadata: {metadata}")
#         schema_path = paths['config']['feature_metadata_schema']
#         schema = load_yaml(schema_path)
        
#         backup_metadata(paths)
        
#         metadata = apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update)

#         save_updated_metadata(metadata, paths, schema)
        
#         logging.debug("Metadata before displaying:")
#         display_metadata(metadata)
        
#         return metadata
#     except Exception as e:
#         logging.error(f"Error in metadata update pipeline: {e}", exc_info=True)
#         raise

# # def load_analysis_results(analysis_results_dir, json_files):
# #     """
# #     Load analysis results from JSON files.

# #     Parameters:
# #     analysis_results_dir (str): Directory containing analysis result files.
# #     json_files (list): List of JSON files to load.

#     Returns:
#     dict: Loaded analysis results.
#     """
#     results = {}
#     for file in json_files:
#         file_path = os.path.join(analysis_results_dir, file)
#         if os.path.isfile(file_path):
#             with open(file_path, 'r') as f:
#                 results[file.split('.')[0]] = json.load(f)
#             logging.debug(f"Loaded {file} from {file_path}")
#         else:
#             logging.warning(f"File {file} not found in {analysis_results_dir}")
#     return results

# def load_metadata(metadata_path):



# def apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update=None):
#     """
#     Update metadata with analysis results.

#     Parameters:
#     metadata (dict): Existing metadata.
#     schema (dict): Schema to validate against.
#     results (dict): Analysis results.
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.
#     fields_to_update (list): List of fields to update in the metadata.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         update_functions = {
#             'data_overview': update_data_overview,
#             'data_types': update_data_types_initial,
#             'stat_summary': update_stat_summary,
#             'missing_values': update_missing_values,
#             'feature_balance': update_feature_balance,
#             'target_correlations': update_target_correlations,
#             'target_analysis': update_target_analysis,
#             'descriptions': update_metadata_with_descriptions,
#             'classifications_initial': update_feature_classifications_initial,
#             'classifications_manual': update_feature_classifications_manual,
#             'example_values': update_example_values,
#             'high_correlation_pairs': update_high_correlation_pairs,
#             'missing_values_target_correlations': update_missing_values_target_correlations,
#             'missing_values_pair_correlations': update_missing_values_pair_correlations,
#             'imputation_strategy': update_imputation_strategy,
#             'data_types_after_initial_cleaning': update_data_types_after_initial_cleaning 
#         }

#         fields_to_update = fields_to_update or update_functions.keys()

#         for field in fields_to_update:
#             if field in update_functions:
#                 update_functions[field](metadata, results, paths, train_sample)

#         return metadata 
#     except Exception as e:
#         logging.error(f"Error updating metadata with results: {e}", exc_info=True)
#         raise



# def update_feature_metadata(metadata, feature, updates):



# def update_data_overview(metadata, results, paths, train_sample):







# def update_missing_values(metadata, results, paths, train_sample):
#     """
#     Update metadata with missing values analysis results.

#     Parameters:
#     metadata (dict): Existing metadata.
#     results (dict): Analysis results.
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.
#     """
#     missing_values = results.get('missing_values', {})
#     for feature, data in missing_values.items():
#         if feature in metadata['features']:
#             metadata['features'][feature]['missing_values'] = {
#                 'count': data['count'],
#                 'percentage': data['percentage']
#             }
#             logging.debug(f"Updated missing values for feature: {feature}")
#         else:
#             logging.warning(f"Feature {feature} not found in metadata.")

