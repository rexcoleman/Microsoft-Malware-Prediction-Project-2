# src/utils/metadata_operations.py

import logging
import yaml
import json
from src.utils.file_operations import load_yaml, save_yaml, convert_to_native_types, load_json_file
from datetime import datetime
import shutil
import os

def execute_metadata_update_pipeline(train_sample, paths, analysis_results_dir, json_files, fields_to_update=None):
    """
    Execute the metadata update pipeline with various steps.

    Parameters:
    train_sample (pd.DataFrame): The training sample DataFrame.
    paths (dict): Dictionary containing file paths.
    analysis_results_dir (str): Directory for saving analysis results.
    json_files (list): List of JSON files to initialize.
    fields_to_update (list): List of fields to update in the metadata.

    Returns:
    dict: Updated metadata.
    """
    try:
        results = load_analysis_results(analysis_results_dir, json_files)
        metadata_path = paths['config']['feature_metadata']
        metadata = load_metadata(metadata_path)
        schema_path = paths['config']['feature_metadata_schema']
        schema = load_yaml(schema_path)
        
        backup_metadata(paths)
        
        metadata = apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update)

        save_updated_metadata(metadata, paths, schema)
        
        logging.debug("Metadata before displaying:")
        display_metadata(metadata)
        
        return metadata
    except Exception as e:
        logging.error(f"Error in metadata update pipeline: {e}", exc_info=True)
        raise

def apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update=None):
    """
    Update metadata with analysis results.

    Parameters:
    metadata (dict): Existing metadata.
    schema (dict): Schema to validate against.
    results (dict): Analysis results.
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.
    fields_to_update (list): List of fields to update in the metadata.

    Returns:
    dict: Updated metadata.
    """
    try:
        update_functions = {
            'data_overview': update_data_overview,
            'data_types': update_data_types,
            'stat_summary': update_stat_summary,
            'missing_values': update_missing_values,
            'feature_balance': update_feature_balance,
            'target_correlations': update_target_correlations,
            'target_analysis': update_target_analysis,
            'descriptions': update_metadata_with_descriptions,
            'classifications_initial': update_feature_classifications_initial,
            'classifications_manual': update_feature_classifications_manual,
            'example_values': update_example_values,
            'high_correlation_pairs': update_high_correlation_pairs
        }

        fields_to_update = fields_to_update or update_functions.keys()

        for field in fields_to_update:
            if field in update_functions:
                update_functions[field](metadata, results, paths, train_sample)

        return metadata
    except Exception as e:
        logging.error(f"Error updating metadata with results: {e}", exc_info=True)
        raise

def update_data_overview(metadata, results, paths, train_sample):
    if 'data_overview' in results:
        metadata['data_overview'] = results['data_overview']
        logging.debug("Updated data_overview.")

def update_data_types(metadata, results, paths, train_sample):
    if 'data_types' in results:
        for feature, dtype in results['data_types'].items():
            metadata = update_feature_metadata(metadata, feature, {'technical_data_type': dtype})
            logging.debug(f"Updated technical_data_type for feature: {feature} to {dtype}")

def update_stat_summary(metadata, results, paths, train_sample):
    if 'stat_summary' in results:
        for feature, stats in results['stat_summary'].items():
            metadata = update_feature_metadata(metadata, feature, {'summary_statistics': stats})
            logging.debug(f"Updated summary_statistics for feature: {feature} to {stats}")

def update_missing_values(metadata, results, paths, train_sample):
    if 'missing_values' in results:
        for feature, missing_data in results['missing_values'].items():
            metadata = update_feature_metadata(metadata, feature, {'missing_values': missing_data})
            logging.debug(f"Updated missing_values for feature: {feature} to {missing_data}")

def update_feature_balance(metadata, results, paths, train_sample):
    if 'feature_balance' in results:
        for feature, balance_data in results['feature_balance'].items():
            metadata = update_feature_metadata(metadata, feature, {'balance': {'most_common_value_weight': balance_data['most_common_value_weight']}})
            logging.debug(f"Updated balance for feature: {feature} to {balance_data['most_common_value_weight']}")

def update_correlations(metadata, results, paths, train_sample):
    if 'correlations' in results:
        for feature, correlation_data in results['correlations'].items():
            metadata = update_feature_metadata(metadata, feature, {'correlation_with_other_features': correlation_data})
            logging.debug(f"Updated correlation_with_other_features for feature: {feature} to {correlation_data}")

def update_target_correlations(metadata, results, paths, train_sample):
    if 'target_correlations' in results:
        for feature, correlation in results['target_correlations'].items():
            metadata = update_feature_metadata(metadata, feature, {'correlation_with_target': correlation})
            logging.debug(f"Updated correlation_with_target for feature: {feature} to {correlation}")

def update_target_analysis(metadata, results, paths, train_sample):
    if 'target_analysis' in results:
        metadata['data_overview']['target_variable_analysis'] = results['target_analysis']
        logging.debug("Updated target_variable_analysis.")

def update_metadata_with_descriptions(metadata, results, paths, train_sample):
    """
    Update metadata with feature descriptions.

    Parameters:
    metadata (dict): Existing metadata.
    results (dict): Analysis results (not used in this function).
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.

    Returns:
    dict: Updated metadata.
    """
    try:
        descriptions_data_path = paths['config']['feature_descriptions']
        descriptions_data = load_yaml(descriptions_data_path)

        for feature, details in descriptions_data['features'].items():
            metadata = update_feature_metadata(metadata, feature, {
                'description': details['description'],
                'security_context': details['security_context']
            })
        return metadata
    except Exception as e:
        logging.error(f"Error updating metadata with descriptions: {e}", exc_info=True)
        raise

def update_feature_classifications_initial(metadata, results, paths, train_sample):
    """
    Classify and update feature types in metadata.

    Parameters:
    metadata (dict): Existing metadata.
    results (dict): Analysis results (not used in this function).
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.

    Returns:
    dict: Updated metadata.
    """
    try:
        from src.analysis.data_understanding import feature_classification

        binary_features_auto, categorical_features_auto, numerical_features_auto = feature_classification(train_sample)

        for feature in binary_features_auto:
            metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'binary'})

        for feature in categorical_features_auto:
            metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'categorical'})

        for feature in numerical_features_auto:
            metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'numerical'})

        return metadata
    except Exception as e:
        logging.error(f"Error classifying and updating features: {e}")
        raise

def update_feature_classifications_manual(metadata, results, paths, train_sample):
    """
    Update metadata with manual feature classifications.

    Parameters:
    metadata (dict): Existing metadata.
    results (dict): Analysis results (not used in this function).
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.

    Returns:
    dict: Updated metadata.
    """
    try:
        manual_classifications_path = paths['config']['manual_feature_classification_updates']
        manual_classifications = load_yaml(manual_classifications_path)

        for feature, details in manual_classifications['manual_feature_classification_updates'].items():
            classified_data_type = details['Manual Review and Update']
            metadata = update_feature_metadata(metadata, feature, {'classified_data_type': classified_data_type})
        return metadata
    except Exception as e:
        logging.error(f"Error updating manual feature classifications: {e}", exc_info=True)
        raise

def update_example_values(metadata, results, paths, train_sample):
    """
    Update metadata with example values for features.

    Parameters:
    metadata (dict): Existing metadata.
    results (dict): Analysis results.
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.

    Returns:
    dict: Updated metadata.
    """
    def select_example_values(feature, dtype, dataframe):
        if dtype in ['int64', 'float64']:
            return [
                dataframe[feature].min(),
                dataframe[feature].max(),
                dataframe[feature].mean(),
                dataframe[feature].median(),
                dataframe[feature].std()
            ]
        elif dtype == 'object':
            return dataframe[feature].value_counts().index.tolist()[:5]
        else:
            return dataframe[feature].unique().tolist()

    try:
        if 'data_types' in results:
            for feature, dtype in results['data_types'].items():
                example_values = select_example_values(feature, dtype, train_sample)
                metadata = update_feature_metadata(metadata, feature, {'example_values': example_values})
        return metadata
    except Exception as e:
        logging.error(f"Error updating example values: {e}", exc_info=True)
        raise

def update_high_correlation_pairs(metadata, results, paths, train_sample):
    """
    Update metadata with high correlation pairs.

    Parameters:
    metadata (dict): Existing metadata.
    results (dict): Analysis results.
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.

    Returns:
    dict: Updated metadata.
    """
    try:
        if 'high_correlation_pairs' in results:
            high_corr_pairs = results['high_correlation_pairs']
            for pair_str, corr_data in high_corr_pairs.items():
                feature1, feature2 = eval(pair_str)
                metadata = update_feature_metadata(metadata, feature1, {
                    'correlation_with_other_features': {feature2: corr_data['correlation']}
                })
                metadata = update_feature_metadata(metadata, feature2, {
                    'correlation_with_other_features': {feature1: corr_data['correlation']}
                })
        return metadata
    except Exception as e:
        logging.error(f"Error updating high correlation pairs: {e}", exc_info=True)
        raise

def save_updated_metadata(metadata, paths, schema):
    """
    Save updated metadata after ensuring completeness and converting to native types.

    Parameters:
    metadata (dict): Existing metadata.
    paths (dict): Dictionary containing file paths.
    schema (dict): Schema to validate against.
    """
    try:
        metadata = ensure_metadata_completeness(metadata, schema)
        metadata = convert_to_native_types(metadata)
        save_metadata(metadata, paths)
    except Exception as e:
        print(f"Error saving updated metadata: {e}")
        raise

def load_analysis_results(analysis_results_dir, json_files):
    """
    Load analysis results from JSON files.

    Parameters:
    analysis_results_dir (str): Directory containing analysis result files.
    json_files (list): List of JSON files to load.

    Returns:
    dict: Loaded analysis results.
    """
    results = {}
    for file in json_files:
        file_path = os.path.join(analysis_results_dir, file)
        if os.path.isfile(file_path):
            with open(file_path, 'r') as f:
                results[file.split('.')[0]] = json.load(f)
            logging.debug(f"Loaded {file} from {file_path}")
    return results

def display_metadata(metadata):
    """
    Display metadata content.

    Parameters:
    metadata (dict): Metadata to display.
    """
    try:
        print("\n--- Metadata to be stored in config/feature_metadata.yaml ---\n")
        print(yaml.dump(metadata, default_flow_style=False))
    except Exception as e:
        logging.error(f"Error displaying metadata: {e}", exc_info=True)
        raise



















# def update_metadata_with_results(metadata, schema, results):
#     """
#     Update metadata with analysis results.

#     Parameters:
#     metadata (dict): Existing metadata.
#     schema (dict): Schema to validate against.
#     results (dict): Analysis results.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         if 'data_overview' in results:
#             metadata['data_overview'] = results['data_overview']
#             logging.debug("Updated data_overview.")

#         if 'data_types' in results:
#             for feature, dtype in results['data_types'].items():
#                 metadata = update_feature_metadata(metadata, feature, {'technical_data_type': dtype})
#                 logging.debug(f"Updated technical_data_type for feature: {feature} to {dtype}")

#         if 'stat_summary' in results:
#             for feature, stats in results['stat_summary'].items():
#                 metadata = update_feature_metadata(metadata, feature, {'summary_statistics': stats})
#                 logging.debug(f"Updated summary_statistics for feature: {feature} to {stats}")

#         if 'missing_values' in results:
#             for feature, missing_data in results['missing_values'].items():
#                 metadata = update_feature_metadata(metadata, feature, {'missing_values': missing_data})
#                 logging.debug(f"Updated missing_values for feature: {feature} to {missing_data}")

#         if 'feature_balance' in results:
#             for feature, balance_data in results['feature_balance'].items():
#                 metadata = update_feature_metadata(metadata, feature, {'balance': {'most_common_value_weight': balance_data['most_common_value_weight']}})
#                 logging.debug(f"Updated balance for feature: {feature} to {balance_data['most_common_value_weight']}")

#         if 'correlations' in results:
#             for feature, correlation_data in results['correlations'].items():
#                 metadata = update_feature_metadata(metadata, feature, {'correlation_with_other_features': correlation_data})
#                 logging.debug(f"Updated correlation_with_other_features for feature: {feature} to {correlation_data}")

#         if 'target_correlations' in results:
#             for feature, correlation in results['target_correlations'].items():
#                 metadata = update_feature_metadata(metadata, feature, {'correlation_with_target': correlation})
#                 logging.debug(f"Updated correlation_with_target for feature: {feature} to {correlation}")

#         return metadata
#     except Exception as e:
#         logging.error(f"Error updating metadata with results: {e}", exc_info=True)
#         raise




# def update_metadata_pipeline(train_sample, paths, analysis_results_dir, target_variable_analysis, json_files):
#     """
#     Execute the metadata update pipeline with various steps.

#     Parameters:
#     train_sample (pd.DataFrame): The training sample DataFrame.
#     paths (dict): Dictionary containing file paths.
#     analysis_results_dir (str): Directory for saving analysis results.
#     target_variable_analysis (dict): Analysis results for the target variable.
#     json_files (list): List of JSON files to initialize.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         results = load_analysis_results(analysis_results_dir, json_files)
#         metadata_path = paths['config']['feature_metadata']  
#         metadata = load_metadata(metadata_path)
#         schema_path = paths['config']['feature_metadata_schema']
#         schema = load_yaml(schema_path)
#         backup_metadata(paths)
#         metadata = update_metadata_with_results(metadata, schema, results)
#         descriptions_data_path = paths['config']['feature_descriptions']
#         descriptions_data = load_yaml(descriptions_data_path)
#         metadata = update_metadata_with_descriptions(metadata, descriptions_data, schema)
#         metadata = classify_and_update_features(train_sample, metadata, schema)
        
#         if 'data_types' in results:
#             metadata = update_example_values(metadata, results['data_types'], train_sample)

#         save_updated_metadata(metadata, paths, schema, target_variable_analysis)
#         display_metadata(metadata)
        
#         return metadata
#     except Exception as e:
#         logging.error(f"Error in metadata update pipeline: {e}", exc_info=True)
#         raise




# def load_analysis_results(analysis_results_dir, json_files):
#     """
#     Load analysis results from JSON files.

#     Parameters:
#     analysis_results_dir (str): Directory containing analysis result files.
#     json_files (list): List of JSON files to load.

#     Returns:
#     dict: Loaded analysis results.
#     """
#     results = {}
#     try:
#         for file in json_files:
#             file_path = os.path.join(analysis_results_dir, file)
#             results[file.split('.')[0]] = load_json_file(file_path)
#             logging.debug(f"Content of {file}: {results[file.split('.')[0]]}")
#             logging.debug(f"Loaded {file} from {file_path}")
#         return results
#     except Exception as e:
#         logging.error(f"Error loading analysis results: {e}", exc_info=True)
#         raise

def load_metadata(metadata_path):
    """
    Load metadata from a YAML file.

    Parameters:
    metadata_path (str): Path to the metadata file.

    Returns:
    dict: Loaded metadata.
    """
    try:
        return load_yaml(metadata_path)
    except Exception as e:
        print(f"Error loading metadata: {e}")
        raise


def ensure_data_overview(paths):
    """
    Ensure the data_overview section is present in the feature metadata.

    Parameters:
    paths (dict): Dictionary containing file paths from paths.yaml.

    Returns:
    dict: The updated feature metadata.
    dict: The metadata schema.
    """
    feature_metadata_path = paths['config']['feature_metadata']
    metadata_schema = load_yaml(paths['config']['feature_metadata_schema'])

    if not os.path.exists(feature_metadata_path):
        feature_metadata = {
            "data_overview": metadata_schema["data_overview"],
            "features": {}
        }
    else:
        feature_metadata = load_yaml(feature_metadata_path)
        if "data_overview" not in feature_metadata:
            feature_metadata["data_overview"] = metadata_schema["data_overview"]
        if "features" not in feature_metadata:
            feature_metadata["features"] = {}

    save_yaml(feature_metadata, feature_metadata_path)
    return feature_metadata, metadata_schema

def populate_feature_metadata(feature_metadata, feature_descriptions, metadata_schema):
    """
    Populate feature metadata with descriptions.

    Parameters:
    feature_metadata (dict): The feature metadata to update.
    feature_descriptions (dict): The feature descriptions to add.
    metadata_schema (dict): The metadata schema to use for default attributes.
    
    Returns:
    dict: The updated feature metadata.
    """
    for feature, description in feature_descriptions['features'].items():
        if feature not in feature_metadata["features"]:
            feature_metadata["features"][feature] = metadata_schema['default_attributes'].copy()
        feature_metadata["features"][feature]['description'] = description.get('description', '')
        feature_metadata["features"][feature]['security_context'] = description.get('security_context', '')

    return feature_metadata

def load_manual_review(manual_review_path):
    """
    Load the manually updated review file.

    Parameters:
    manual_review_path (str): Path to the manual review file.

    Returns:
    DataFrame: Loaded manual review DataFrame.
    """
    try:
        return pd.read_csv(manual_review_path)
    except Exception as e:
        print(f"Error loading manual review file: {e}")
        raise

def update_metadata_from_review(manual_review, metadata, schema):
    """
    Update the metadata based on manual review.

    Parameters:
    manual_review (DataFrame): DataFrame containing the manual review.
    metadata (dict): Current feature metadata.
    schema (dict): Schema for feature metadata.

    Returns:
    dict: Updated feature metadata.
    """
    try:
        metadata = ensure_metadata_completeness(metadata, schema)
        for index, row in manual_review.iterrows():
            feature = row['Feature']
            new_type = row['Manual Review and Update']
            updates = {'classified_data_type': new_type}
            metadata = update_feature_metadata(metadata, feature, updates)
        return metadata
    except Exception as e:
        print(f"Error updating metadata from review: {e}")
        raise

def save_metadata(metadata, paths):
    """
    Save metadata to a YAML file.

    Parameters:
    metadata (dict): Metadata to save.
    paths (dict): Dictionary containing file paths.
    """
    try:
        save_yaml(metadata, paths['config']['feature_metadata'])
    except Exception as e:
        print(f"Error saving metadata: {e}")
        raise

def update_feature_metadata(feature_metadata, feature, updates):
    """
    Update feature metadata with given updates.

    Parameters:
    feature_metadata (dict): Existing feature metadata.
    feature (str): Feature name to update.
    updates (dict): Dictionary containing updates.

    Returns:
    dict: Updated feature metadata.
    """
    logging.debug(f"Updating feature: {feature} with updates: {updates}")
    if feature not in feature_metadata["features"]:
        feature_metadata["features"][feature] = {}
    for key, value in updates.items():
        if isinstance(value, dict):
            feature_metadata["features"][feature].setdefault(key, {})
            for sub_key, sub_value in value.items():
                feature_metadata["features"][feature][key][sub_key] = sub_value
                logging.debug(f"Updated {key}.{sub_key} for {feature} to {sub_value}")
        else:
            feature_metadata["features"][feature][key] = value
            logging.debug(f"Updated {key} for {feature} to {value}")
    return feature_metadata

def initialize_feature_metadata(schema_path):
    """
    Initialize feature metadata based on a schema.

    Parameters:
    schema_path (str): Path to the schema YAML file.

    Returns:
    dict: Initialized feature metadata.
    """
    try:
        schema = load_yaml(schema_path)
        feature_metadata = {"features": {}}
        for attribute, default_value in schema["default_attributes"].items():
            feature_metadata[attribute] = default_value
        return feature_metadata
    except Exception as e:
        print(f"Error initializing feature metadata: {e}")
        raise

def ensure_metadata_completeness(metadata, schema):
    """
    Ensure metadata completeness based on a schema.

    Parameters:
    metadata (dict): Existing metadata.
    schema (dict): Schema to validate against.

    Returns:
    dict: Complete metadata.
    """
    for feature in metadata['features']:
        for attribute, default_value in schema['default_attributes'].items():
            metadata['features'][feature].setdefault(attribute, default_value)
    return metadata

def update_metadata_with_data_types(metadata, feature_data_types, attribute_name):
    """
    Update metadata with feature data types.

    Parameters:
    metadata (dict): Existing metadata.
    feature_data_types (list): List of feature data types.
    attribute_name (str): Attribute name to update in the metadata.

    Returns:
    dict: Updated metadata.
    """
    for item in feature_data_types:
        feature = item['Feature']
        data_type = item[attribute_name]
        if feature in metadata['features']:
            metadata['features'][feature][attribute_name] = data_type
    return metadata

def determine_feature_type(feature, metadata):
    """
    Determine the type of a feature.

    Parameters:
    feature (str): Feature name.
    metadata (dict): Metadata containing feature details.

    Returns:
    str: Determined feature type.
    """
    classified_type = metadata['features'][feature].get('classified_data_type', '')
    if classified_type in ['binary', 'categorical']:
        return classified_type
    example_values = metadata['features'][feature].get('example_values', [])
    if all(isinstance(val, (int, float)) for val in example_values):
        if example_values and isinstance(example_values[0], int):
            return 'categorical'
    return 'numerical'


def backup_metadata(paths):
    """
    Create a backup of the metadata file.

    Parameters:
    paths (dict): Dictionary containing file paths.
    """
    try:
        backup_dir = os.path.join('config', 'backup')
        os.makedirs(backup_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
        backup_path = os.path.join(backup_dir, f'feature_metadata_{timestamp}.yaml')
        shutil.copy(paths['config']['feature_metadata'], backup_path)
        print(f"Backup created at {backup_path}")
    except Exception as e:
        print(f"Error creating metadata backup: {e}")
        raise



# def update_metadata_with_descriptions(metadata, descriptions_data, schema):
#     """
#     Update metadata with feature descriptions.

#     Parameters:
#     metadata (dict): Existing metadata.
#     descriptions_data (dict): Descriptions data.
#     schema (dict): Schema to validate against.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         for feature, details in descriptions_data['features'].items():
#             metadata = update_feature_metadata(metadata, feature, {
#                 'description': details['description'],
#                 'security_context': details['security_context']
#             })
#         return metadata
#     except Exception as e:
#         print(f"Error updating metadata with descriptions: {e}")
#         raise

# def classify_and_update_features(train_sample, metadata, schema):
#     """
#     Classify and update feature types in metadata.

#     Parameters:
#     train_sample (pd.DataFrame): Training sample data.
#     metadata (dict): Existing metadata.
#     schema (dict): Schema to validate against.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         from src.analysis.data_understanding import feature_classification

#         binary_features_auto, categorical_features_auto, numerical_features_auto = feature_classification(train_sample)

#         for feature in binary_features_auto:
#             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'binary'})

#         for feature in categorical_features_auto:
#             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'categorical'})

#         for feature in numerical_features_auto:
#             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'numerical'})

#         return metadata
#     except Exception as e:
#         print(f"Error classifying and updating features: {e}")
#         raise

# def update_example_values(metadata, data_types, train_sample):
#     """
#     Update metadata with example values for features.

#     Parameters:
#     metadata (dict): Existing metadata.
#     data_types (dict): Data types of features.
#     train_sample (pd.DataFrame): Training sample data.

#     Returns:
#     dict: Updated metadata.
#     """
#     def select_example_values(feature, dtype, dataframe):
#         if dtype in ['int64', 'float64']:
#             return [
#                 dataframe[feature].min(),
#                 dataframe[feature].max(),
#                 dataframe[feature].mean(),
#                 dataframe[feature].median(),
#                 dataframe[feature].std()
#             ]
#         elif dtype == 'object':
#             return dataframe[feature].value_counts().index.tolist()[:5]
#         else:
#             return dataframe[feature].unique().tolist()

#     try:
#         for feature, dtype in data_types.items():
#             example_values = select_example_values(feature, dtype, train_sample)
#             metadata = update_feature_metadata(metadata, feature, {'example_values': example_values})
#         return metadata
#     except Exception as e:
#         print(f"Error updating example values: {e}")
#         raise
        
# # def save_updated_metadata(metadata, paths, schema, target_variable_analysis):
# #     """
# #     Save updated metadata after ensuring completeness and converting to native types.

#     Parameters:
#     metadata (dict): Existing metadata.
#     paths (dict): Dictionary containing file paths.
#     schema (dict): Schema to validate against.
#     target_variable_analysis (dict): Analysis of the target variable.
#     """
#     try:
#         metadata = ensure_metadata_completeness(metadata, schema)
#         metadata = convert_to_native_types(metadata)
#         metadata = {'target_variable_analysis': target_variable_analysis, **metadata}
#         save_metadata(metadata, paths)
#     except Exception as e:
#         print(f"Error saving updated metadata: {e}")
#         raise



# def display_metadata(metadata):
#     """
#     Display metadata content.

#     Parameters:
#     metadata (dict): Metadata to display.
#     """
#     try:
#         print("\n--- Metadata to be stored in config/feature_metadata.yaml ---\n")
#         print(yaml.dump(metadata, default_flow_style=False))
#     except Exception as e:
#         print(f"Error displaying metadata: {e}")
#         raise

def print_update_message(paths, config_key):
    """
    Print an update message for classified data types.

    Parameters:
    paths (dict): Dictionary containing file paths.
    config_key (str): Configuration key indicating the path to update.
    """
    try:
        config_path = paths['config'][config_key]
        print(f"Updated classified_data_type attribute in {config_path}")
    except Exception as e:
        print(f"Error printing update message: {e}")
        raise



def update_classified_data_type(manual_updates, metadata, schema):
    """
    Update the classified_data_type attribute in the metadata based on manual updates.

    Parameters:
    manual_updates (dict): Dictionary containing manual feature classification updates.
    metadata (dict): Existing feature metadata.
    schema (dict): Schema for feature metadata.

    Returns:
    dict: Updated feature metadata.
    """
    try:
        metadata = ensure_metadata_completeness(metadata, schema)
        for feature, update in manual_updates.items():
            new_type = update['Manual Review and Update']
            updates = {'classified_data_type': new_type}
            metadata = update_feature_metadata(metadata, feature, updates)
        return metadata
    except Exception as e:
        print(f"Error updating metadata from review: {e}")
        raise








# # src/utils/metadata_operations.py

# import pandas as pd
# import yaml
# from src.utils.file_operations import load_yaml, save_yaml, convert_to_native_types, load_json_file
# from datetime import datetime
# import shutil
# import os

# def load_manual_review(manual_review_path):
#     """
#     Load the manually updated review file.

#     Parameters:
#     manual_review_path (str): Path to the manual review file.

#     Returns:
#     DataFrame: Loaded manual review DataFrame.
#     """
#     try:
#         return pd.read_csv(manual_review_path)
#     except Exception as e:
#         print(f"Error loading manual review file: {e}")
#         raise

# def update_metadata_from_review(manual_review, metadata, schema):
#     """
#     Update the metadata based on manual review.

#     Parameters:
#     manual_review (DataFrame): DataFrame containing the manual review.
#     metadata (dict): Current feature metadata.
#     schema (dict): Schema for feature metadata.

#     Returns:
#     dict: Updated feature metadata.
#     """
#     try:
#         metadata = ensure_metadata_completeness(metadata, schema)
#         for index, row in manual_review.iterrows():
#             feature = row['Feature']
#             new_type = row['Manual Review and Update']
#             updates = {'classified_data_type': new_type}
#             metadata = update_feature_metadata(metadata, feature, updates)
#         return metadata
#     except Exception as e:
#         print(f"Error updating metadata from review: {e}")
#         raise

# def load_metadata(metadata_path):
#     """
#     Load metadata from a YAML file.

#     Parameters:
#     metadata_path (str): Path to the metadata file.

#     Returns:
#     dict: Loaded metadata.
#     """
#     try:
#         return load_yaml(metadata_path)
#     except Exception as e:
#         print(f"Error loading metadata: {e}")
#         raise

# def save_metadata(metadata, paths):
#     """
#     Save metadata to a YAML file.

#     Parameters:
#     metadata (dict): Metadata to save.
#     paths (dict): Dictionary containing file paths.
#     """
#     try:
#         save_yaml(metadata, paths['config']['feature_metadata'])
#     except Exception as e:
#         print(f"Error saving metadata: {e}")
#         raise

# def update_feature_metadata(feature_metadata, feature, updates):
#     """
#     Update feature metadata with given updates.

#     Parameters:
#     feature_metadata (dict): Existing feature metadata.
#     feature (str): Feature name to update.
#     updates (dict): Dictionary containing updates.

#     Returns:
#     dict: Updated feature metadata.
#     """
#     if feature not in feature_metadata["features"]:
#         feature_metadata["features"][feature] = {}
#     for key, value in updates.items():
#         if isinstance(value, dict):
#             feature_metadata["features"][feature].setdefault(key, {})
#             for sub_key, sub_value in value.items():
#                 feature_metadata["features"][feature][key][sub_key] = sub_value
#         else:
#             feature_metadata["features"][feature][key] = value
#     return feature_metadata

# def initialize_feature_metadata(schema_path):
#     """
#     Initialize feature metadata based on a schema.

#     Parameters:
#     schema_path (str): Path to the schema YAML file.

#     Returns:
#     dict: Initialized feature metadata.
#     """
#     try:
#         schema = load_yaml(schema_path)
#         feature_metadata = {"features": {}}
#         for attribute, default_value in schema["default_attributes"].items():
#             feature_metadata[attribute] = default_value
#         return feature_metadata
#     except Exception as e:
#         print(f"Error initializing feature metadata: {e}")
#         raise

# def ensure_metadata_completeness(metadata, schema):
#     """
#     Ensure metadata completeness based on a schema.

#     Parameters:
#     metadata (dict): Existing metadata.
#     schema (dict): Schema to validate against.

#     Returns:
#     dict: Complete metadata.
#     """
#     for feature in metadata['features']:
#         for attribute, default_value in schema['default_attributes'].items():
#             metadata['features'][feature].setdefault(attribute, default_value)
#     return metadata

# def update_metadata_with_data_types(metadata, feature_data_types, attribute_name):
#     """
#     Update metadata with feature data types.

#     Parameters:
#     metadata (dict): Existing metadata.
#     feature_data_types (list): List of feature data types.
#     attribute_name (str): Attribute name to update in the metadata.

#     Returns:
#     dict: Updated metadata.
#     """
#     for item in feature_data_types:
#         feature = item['Feature']
#         data_type = item[attribute_name]
#         if feature in metadata['features']:
#             metadata['features'][feature][attribute_name] = data_type
#     return metadata

# def determine_feature_type(feature, metadata):
#     """
#     Determine the type of a feature.

#     Parameters:
#     feature (str): Feature name.
#     metadata (dict): Metadata containing feature details.

#     Returns:
#     str: Determined feature type.
#     """
#     classified_type = metadata['features'][feature].get('classified_data_type', '')
#     if classified_type in ['binary', 'categorical']:
#         return classified_type
#     example_values = metadata['features'][feature].get('example_values', [])
#     if all(isinstance(val, (int, float)) for val in example_values):
#         if example_values and isinstance(example_values[0], int):
#             return 'categorical'
#     return 'numerical'

# def backup_metadata(paths):
#     """
#     Create a backup of the metadata file.

#     Parameters:
#     paths (dict): Dictionary containing file paths.
#     """
#     try:
#         backup_dir = os.path.join('config', 'backup')
#         os.makedirs(backup_dir, exist_ok=True)
#         timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
#         backup_path = os.path.join(backup_dir, f'feature_metadata_{timestamp}.yaml')
#         shutil.copy(paths['config']['feature_metadata'], backup_path)
#         print(f"Backup created at {backup_path}")
#     except Exception as e:
#         print(f"Error creating metadata backup: {e}")
#         raise

# def update_metadata_with_results(metadata, schema, results):
#     """
#     Update metadata with analysis results.

#     Parameters:
#     metadata (dict): Existing metadata.
#     schema (dict): Schema to validate against.
#     results (dict): Analysis results.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         if 'data_overview' in results:
#             metadata['data_overview'] = results['data_overview']

#         if 'data_types' in results:
#             for feature, dtype in results['data_types'].items():
#                 metadata = update_feature_metadata(metadata, feature, {'technical_data_type': dtype})

#         if 'stat_summary' in results:
#             for feature, stats in results['stat_summary'].items():
#                 metadata = update_feature_metadata(metadata, feature, {'summary_statistics': stats})

#         if 'missing_values' in results:
#             for feature, missing_data in results['missing_values'].items():
#                 metadata = update_feature_metadata(metadata, feature, {'missing_values': missing_data})

#         if 'feature_balance' in results:
#             for feature, balance_data in results['feature_balance'].items():
#                 metadata = update_feature_metadata(metadata, feature, {'balance': {'most_common_value_weight': balance_data['most_common_value_weight']}})

#         if 'correlations' in results:
#             for feature, correlation_data in results['correlations'].items():
#                 metadata = update_feature_metadata(metadata, feature, {'correlation_with_other_features': correlation_data})

#         if 'target_correlations' in results:
#             for feature, correlation in results['target_correlations'].items():
#                 metadata = update_feature_metadata(metadata, feature, {'correlation_with_target': correlation})

#         return metadata
#     except Exception as e:
#         print(f"Error updating metadata with results: {e}")
#         raise

# def update_metadata_with_descriptions(metadata, descriptions_data, schema):
#     """
#     Update metadata with feature descriptions.

#     Parameters:
#     metadata (dict): Existing metadata.
#     descriptions_data (dict): Descriptions data.
#     schema (dict): Schema to validate against.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         for feature, details in descriptions_data['features'].items():
#             metadata = update_feature_metadata(metadata, feature, {
#                 'description': details['description'],
#                 'security_context': details['security_context']
#             })
#         return metadata
#     except Exception as e:
#         print(f"Error updating metadata with descriptions: {e}")
#         raise

# def classify_and_update_features(train_sample, metadata, schema):
#     """
#     Classify and update feature types in metadata.

#     Parameters:
#     train_sample (pd.DataFrame): Training sample data.
#     metadata (dict): Existing metadata.
#     schema (dict): Schema to validate against.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         from src.analysis.data_understanding import feature_classification

#         binary_features_auto, categorical_features_auto, numerical_features_auto = feature_classification(train_sample)

#         for feature in binary_features_auto:
#             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'binary'})

#         for feature in categorical_features_auto:
#             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'categorical'})

#         for feature in numerical_features_auto:
#             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'numerical'})

#         return metadata
#     except Exception as e:
#         print(f"Error classifying and updating features: {e}")
#         raise

# def update_example_values(metadata, data_types, train_sample):
#     """
#     Update metadata with example values for features.

#     Parameters:
#     metadata (dict): Existing metadata.
#     data_types (dict): Data types of features.
#     train_sample (pd.DataFrame): Training sample data.

#     Returns:
#     dict: Updated metadata.
#     """
#     def select_example_values(feature, dtype, dataframe):
#         if dtype in ['int64', 'float64']:
#             return [
#                 dataframe[feature].min(),
#                 dataframe[feature].max(),
#                 dataframe[feature].mean(),
#                 dataframe[feature].median(),
#                 dataframe[feature].std()
#             ]
#         elif dtype == 'object':
#             return dataframe[feature].value_counts().index.tolist()[:5]
#         else:
#             return dataframe[feature].unique().tolist()

#     try:
#         for feature, dtype in data_types.items():
#             example_values = select_example_values(feature, dtype, train_sample)
#             metadata = update_feature_metadata(metadata, feature, {'example_values': example_values})
#         return metadata
#     except Exception as e:
#         print(f"Error updating example values: {e}")
#         raise

# def save_updated_metadata(metadata, paths, schema, target_variable_analysis):
#     """
#     Save updated metadata after ensuring completeness and converting to native types.

#     Parameters:
#     metadata (dict): Existing metadata.
#     paths (dict): Dictionary containing file paths.
#     schema (dict): Schema to validate against.
#     target_variable_analysis (dict): Analysis of the target variable.
#     """
#     try:
#         metadata = ensure_metadata_completeness(metadata, schema)
#         metadata = convert_to_native_types(metadata)
#         metadata = {'target_variable_analysis': target_variable_analysis, **metadata}
#         save_metadata(metadata, paths)
#     except Exception as e:
#         print(f"Error saving updated metadata: {e}")
#         raise

# def load_analysis_results(analysis_results_dir, json_files):
#     """
#     Load analysis results from JSON files.

#     Parameters:
#     analysis_results_dir (str): Directory containing analysis result files.
#     json_files (list): List of JSON files to load.

#     Returns:
#     dict: Loaded analysis results.
#     """
#     try:
#         return {file: load_json_file(os.path.join(analysis_results_dir, file)) for file in json_files}
#     except Exception as e:
#         print(f"Error loading analysis results: {e}")
#         raise

# def display_metadata(metadata):
#     """
#     Display metadata content.

#     Parameters:
#     metadata (dict): Metadata to display.
#     """
#     try:
#         print("\n--- Metadata to be stored in config/feature_metadata.yaml ---\n")
#         print(yaml.dump(metadata, default_flow_style=False))
#     except Exception as e:
#         print(f"Error displaying metadata: {e}")
#         raise

# def print_update_message(paths, config_key):
#     """
#     Print an update message for classified data types.

#     Parameters:
#     paths (dict): Dictionary containing file paths.
#     config_key (str): Configuration key indicating the path to update.
#     """
#     try:
#         config_path = paths['config'][config_key]
#         print(f"Updated classified_data_type attribute in {config_path}")
#     except Exception as e:
#         print(f"Error printing update message: {e}")
#         raise

