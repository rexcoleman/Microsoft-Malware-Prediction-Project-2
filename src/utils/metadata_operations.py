# src/utils/metadata_operations.py

import logging
import yaml
import json
from src.utils.file_operations import load_yaml, save_yaml, convert_to_native_types, load_json_file
from datetime import datetime
import shutil
import os

def execute_metadata_update_pipeline(train_sample, paths, analysis_results_dir, json_files, fields_to_update=None):
    """
    Execute the metadata update pipeline with various steps.

    Parameters:
    train_sample (pd.DataFrame): The training sample DataFrame.
    paths (dict): Dictionary containing file paths.
    analysis_results_dir (str): Directory for saving analysis results.
    json_files (list): List of JSON files to initialize.
    fields_to_update (list): List of fields to update in the metadata.

    Returns:
    dict: Updated metadata.
    """
    try:
        results = load_analysis_results(analysis_results_dir, json_files)
        logging.debug(f"Loaded analysis results: {results.keys()}")
        metadata_path = paths['config']['feature_metadata']
        metadata = load_metadata(metadata_path)
        schema_path = paths['config']['feature_metadata_schema']
        schema = load_yaml(schema_path)
        
        backup_metadata(paths)
        
        metadata = apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update)

        save_updated_metadata(metadata, paths, schema)
        
        logging.debug("Metadata before displaying:")
        display_metadata(metadata)
        
        return metadata
    except Exception as e:
        logging.error(f"Error in metadata update pipeline: {e}", exc_info=True)
        raise

def load_analysis_results(analysis_results_dir, json_files):
    """
    Load analysis results from JSON files.

    Parameters:
    analysis_results_dir (str): Directory containing analysis result files.
    json_files (list): List of JSON files to load.

    Returns:
    dict: Loaded analysis results.
    """
    results = {}
    for file in json_files:
        file_path = os.path.join(analysis_results_dir, file)
        if os.path.isfile(file_path):
            with open(file_path, 'r') as f:
                results[file.split('.')[0]] = json.load(f)
            logging.debug(f"Loaded {file} from {file_path}")
        else:
            logging.warning(f"File {file} not found in {analysis_results_dir}")
    return results

def load_metadata(metadata_path):
    """
    Load metadata from a YAML file.

    Parameters:
    metadata_path (str): Path to the metadata file.

    Returns:
    dict: Loaded metadata.
    """
    try:
        return load_yaml(metadata_path)
    except Exception as e:
        print(f"Error loading metadata: {e}")
        raise

def apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update=None):
    """
    Update metadata with analysis results.

    Parameters:
    metadata (dict): Existing metadata.
    schema (dict): Schema to validate against.
    results (dict): Analysis results.
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.
    fields_to_update (list): List of fields to update in the metadata.

    Returns:
    dict: Updated metadata.
    """
    try:
        update_functions = {
            'data_overview': update_data_overview,
            'data_types': update_data_types,
            'stat_summary': update_stat_summary,
            'missing_values': update_missing_values,
            'feature_balance': update_feature_balance,
            'target_correlations': update_target_correlations,
            'target_analysis': update_target_analysis,
            'descriptions': update_metadata_with_descriptions,
            'classifications_initial': update_feature_classifications_initial,
            'classifications_manual': update_feature_classifications_manual,
            'example_values': update_example_values,
            'high_correlation_pairs': update_high_correlation_pairs,
            'missing_values_target_correlations': update_missing_values_target_correlations,
            'missing_values_pair_correlations': update_missing_values_pair_correlations,
            'imputation_strategy': update_imputation_strategy
        }

        fields_to_update = fields_to_update or update_functions.keys()

        for field in fields_to_update:
            if field in update_functions:
                update_functions[field](metadata, results, paths, train_sample)

        return metadata
    except Exception as e:
        logging.error(f"Error updating metadata with results: {e}", exc_info=True)
        raise

def update_feature_metadata(metadata, feature, updates):
    """
    Update the feature metadata with the provided updates.

    Parameters:
    metadata (dict): Existing metadata.
    feature (str): Feature to update.
    updates (dict): Dictionary of updates to apply.

    Returns:
    dict: Updated metadata.
    """
    for key, value in updates.items():
        if isinstance(value, dict):
            if metadata['features'][feature].get(key) is None:
                metadata['features'][feature][key] = {}
            for sub_key, sub_value in value.items():
                metadata['features'][feature][key][sub_key] = sub_value
        else:
            metadata['features'][feature][key] = value
    return metadata

def update_missing_values(metadata, results, paths, train_sample):
    """
    Update metadata with missing values analysis results.

    Parameters:
    metadata (dict): Existing metadata.
    results (dict): Analysis results.
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.
    """
    missing_values = results.get('missing_values', {})
    for feature, data in missing_values.items():
        if feature in metadata['features']:
            metadata['features'][feature]['missing_values'] = {
                'count': data['count'],
                'percentage': data['percentage'],
                'correlation_with_target': data.get('correlation_with_target', metadata['features'][feature]['missing_values'].get('correlation_with_target'))
            }
            logging.debug(f"Updated missing values for feature: {feature}")
        else:
            logging.warning(f"Feature {feature} not found in metadata.")

def update_missing_values_target_correlations(metadata, results, paths, train_sample):
    """
    Update metadata with missing values' correlation with the target variable.

    Parameters:
    metadata (dict): Existing metadata.
    results (dict): Analysis results containing missing values correlations.
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.

    Returns:
    dict: Updated metadata.
    """
    if 'missing_values_correlations_with_target_variable' in results:
        correlations = results['missing_values_correlations_with_target_variable']
        logging.debug(f"Correlations found: {correlations}")
        for item in correlations:
            feature = item['Feature']
            correlation = item['Correlation']
            if feature in metadata['features']:
                if 'missing_values' not in metadata['features'][feature]:
                    metadata['features'][feature]['missing_values'] = {}
                metadata['features'][feature]['missing_values']['missing_values_correlation_with_target'] = correlation
                logging.debug(f"Updated missing_values.missing_values_correlation_with_target for feature: {feature} to {correlation}")
            else:
                logging.warning(f"Feature {feature} not found in metadata.")
    else:
        logging.warning("No missing values correlations with target found in the results.")
    return metadata

def update_missing_values_pair_correlations(metadata, results, paths, train_sample):
    """
    Update metadata with missing values' pair correlations.

    Parameters:
    metadata (dict): Existing metadata.
    results (dict): Analysis results containing pair correlations.
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.

    Returns:
    dict: Updated metadata.
    """
    if 'missing_value_pair_correlations' in results:
        pair_correlations = results['missing_value_pair_correlations']
        logging.debug(f"Pair correlations found: {pair_correlations}")
        for item in pair_correlations:
            feature1 = item['Feature1']
            feature2 = item['Feature2']
            correlation = item['Correlation']
            if feature1 in metadata['features']:
                if 'missing_values' not in metadata['features'][feature1]:
                    metadata['features'][feature1]['missing_values'] = {}
                if 'missing_values_correlation_with_other_features' not in metadata['features'][feature1]['missing_values']:
                    metadata['features'][feature1]['missing_values']['missing_values_correlation_with_other_features'] = {}
                metadata['features'][feature1]['missing_values']['missing_values_correlation_with_other_features'][feature2] = correlation
                logging.debug(f"Updated missing_values.missing_values_correlation_with_other_features for feature: {feature1} to {correlation} with {feature2}")
            else:
                logging.warning(f"Feature {feature1} not found in metadata.")
    else:
        logging.warning("No missing values pair correlations found in the results.")
    return metadata

def update_data_overview(metadata, results, paths, train_sample):
    if 'data_overview' in results:
        metadata['data_overview'] = results['data_overview']
        logging.debug("Updated data_overview.")

def update_data_types(metadata, results, paths, train_sample):
    if 'data_types' in results:
        for feature, dtype in results['data_types'].items():
            metadata = update_feature_metadata(metadata, feature, {'technical_data_type': dtype})
            logging.debug(f"Updated technical_data_type for feature: {feature} to {dtype}")

def update_stat_summary(metadata, results, paths, train_sample):
    if 'stat_summary' in results:
        for feature, stats in results['stat_summary'].items():
            metadata = update_feature_metadata(metadata, feature, {'summary_statistics': stats})
            logging.debug(f"Updated summary_statistics for feature: {feature} to {stats}")

def update_feature_balance(metadata, results, paths, train_sample):
    if 'feature_balance' in results:
        for feature, balance_data in results['feature_balance'].items():
            metadata = update_feature_metadata(metadata, feature, {'balance': {'most_common_value_weight': balance_data['most_common_value_weight']}})
            logging.debug(f"Updated balance for feature: {feature} to {balance_data['most_common_value_weight']}")

def update_correlations(metadata, results, paths, train_sample):
    if 'correlations' in results:
        for feature, correlation_data in results['correlations'].items():
            metadata = update_feature_metadata(metadata, feature, {'correlation_with_other_features': correlation_data})
            logging.debug(f"Updated correlation_with_other_features for feature: {feature} to {correlation_data}")

def update_target_correlations(metadata, results, paths, train_sample):
    if 'target_correlations' in results:
        for feature, correlation in results['target_correlations'].items():
            metadata = update_feature_metadata(metadata, feature, {'correlation_with_target': correlation})
            logging.debug(f"Updated correlation_with_target for feature: {feature} to {correlation}")

def update_target_analysis(metadata, results, paths, train_sample):
    if 'target_analysis' in results:
        metadata['data_overview']['target_variable_analysis'] = results['target_analysis']
        logging.debug("Updated target_variable_analysis.")

def update_metadata_with_descriptions(metadata, results, paths, train_sample):
    """
    Update metadata with feature descriptions.

    Parameters:
    metadata (dict): Existing metadata.
    results (dict): Analysis results (not used in this function).
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.

    Returns:
    dict: Updated metadata.
    """
    try:
        descriptions_data_path = paths['config']['feature_descriptions']
        descriptions_data = load_yaml(descriptions_data_path)

        for feature, details in descriptions_data['features'].items():
            metadata = update_feature_metadata(metadata, feature, {
                'description': details['description'],
                'security_context': details['security_context']
            })
        return metadata
    except Exception as e:
        logging.error(f"Error updating metadata with descriptions: {e}", exc_info=True)
        raise

def update_feature_classifications_initial(metadata, results, paths, train_sample):
    """
    Classify and update feature types in metadata.

    Parameters:
    metadata (dict): Existing metadata.
    results (dict): Analysis results (not used in this function).
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.

    Returns:
    dict: Updated metadata.
    """
    try:
        from src.analysis.data_understanding import feature_classification

        binary_features_auto, categorical_features_auto, numerical_features_auto = feature_classification(train_sample)

        for feature in binary_features_auto:
            metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'binary'})

        for feature in categorical_features_auto:
            metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'categorical'})

        for feature in numerical_features_auto:
            metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'numerical'})

        return metadata
    except Exception as e:
        logging.error(f"Error classifying and updating features: {e}")
        raise

def update_feature_classifications_manual(metadata, results, paths, train_sample):
    """
    Update metadata with manual feature classifications.

    Parameters:
    metadata (dict): Existing metadata.
    results (dict): Analysis results (not used in this function).
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.

    Returns:
    dict: Updated metadata.
    """
    try:
        manual_classifications_path = paths['config']['manual_feature_classification_update']
        manual_classifications = load_yaml(manual_classifications_path)

        for feature, details in manual_classifications['manual_feature_classification_updates'].items():
            classified_data_type = details['Manual Review and Update']
            metadata = update_feature_metadata(metadata, feature, {'classified_data_type': classified_data_type})
        return metadata
    except Exception as e:
        logging.error(f"Error updating manual feature classifications: {e}", exc_info=True)
        raise

def update_example_values(metadata, results, paths, train_sample):
    """
    Update metadata with example values for features.

    Parameters:
    metadata (dict): Existing metadata.
    results (dict): Analysis results.
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.

    Returns:
    dict: Updated metadata.
    """
    def select_example_values(feature, dtype, dataframe):
        if dtype in ['int64', 'float64']:
            return [
                dataframe[feature].min(),
                dataframe[feature].max(),
                dataframe[feature].mean(),
                dataframe[feature].median(),
                dataframe[feature].std()
            ]
        elif dtype == 'object':
            return dataframe[feature].value_counts().index.tolist()[:5]
        else:
            return dataframe[feature].unique().tolist()

    try:
        if 'data_types' in results:
            for feature, dtype in results['data_types'].items():
                example_values = select_example_values(feature, dtype, train_sample)
                metadata = update_feature_metadata(metadata, feature, {'example_values': example_values})
        return metadata
    except Exception as e:
        logging.error(f"Error updating example values: {e}", exc_info=True)
        raise

def update_high_correlation_pairs(metadata, results, paths, train_sample):
    """
    Update metadata with high correlation pairs.

    Parameters:
    metadata (dict): Existing metadata.
    results (dict): Analysis results.
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.

    Returns:
    dict: Updated metadata.
    """
    try:
        if 'high_correlation_pairs' in results:
            high_corr_pairs = results['high_correlation_pairs']
            for pair_str, corr_data in high_corr_pairs.items():
                feature1, feature2 = eval(pair_str)
                metadata = update_feature_metadata(metadata, feature1, {
                    'correlation_with_other_features': {feature2: corr_data['correlation']}
                })
                metadata = update_feature_metadata(metadata, feature2, {
                    'correlation_with_other_features': {feature1: corr_data['correlation']}
                })
        return metadata
    except Exception as e:
        logging.error(f"Error updating high correlation pairs: {e}", exc_info=True)
        raise

def update_feature_metadata(feature_metadata, feature, updates):
    """
    Update feature metadata with given updates.

    Parameters:
    feature_metadata (dict): Existing feature metadata.
    feature (str): Feature name to update.
    updates (dict): Dictionary containing updates.

    Returns:
    dict: Updated feature metadata.
    """
    logging.debug(f"Updating feature: {feature} with updates: {updates}")
    if feature not in feature_metadata["features"]:
        feature_metadata["features"][feature] = {}
    for key, value in updates.items():
        if isinstance(value, dict):
            feature_metadata["features"][feature].setdefault(key, {})
            for sub_key, sub_value in value.items():
                feature_metadata["features"][feature][key][sub_key] = sub_value
                logging.debug(f"Updated {key}.{sub_key} for {feature} to {sub_value}")
        else:
            feature_metadata["features"][feature][key] = value
            logging.debug(f"Updated {key} for {feature} to {value}")
    return feature_metadata

def update_imputation_strategy(metadata, results, paths, train_sample):
    """
    Update metadata with imputation strategies.

    Parameters:
    metadata (dict): Existing metadata.
    results (dict): Analysis results.
    paths (dict): Dictionary containing file paths.
    train_sample (pd.DataFrame): The training sample data.

    Returns:
    dict: Updated metadata.
    """
    if 'imputation_strategy' in results:
        imputation_data = results['imputation_strategy']
        logging.debug(f"Imputation strategies found: {imputation_data}")
        for item in imputation_data:
            feature = item['Feature']
            strategy = item['Imputation Strategy']
            if feature in metadata['features']:
                if 'imputation' not in metadata['features'][feature]:
                    metadata['features'][feature]['imputation'] = {}
                metadata['features'][feature]['imputation']['imputation_strategy'] = strategy
                logging.debug(f"Updated imputation.imputation_strategy for feature: {feature} to {strategy}")
            else:
                logging.warning(f"Feature {feature} not found in metadata.")
    else:
        logging.warning("No imputation strategies found in the results.")
    return metadata

def save_updated_metadata(metadata, paths, schema):
    """
    Save updated metadata after ensuring completeness and converting to native types.

    Parameters:
    metadata (dict): Existing metadata.
    paths (dict): Dictionary containing file paths.
    schema (dict): Schema to validate against.
    """
    try:
        metadata = ensure_metadata_completeness(metadata, schema)
        metadata = convert_to_native_types(metadata)
        save_metadata(metadata, paths)
    except Exception as e:
        print(f"Error saving updated metadata: {e}")
        raise

def ensure_metadata_completeness(metadata, schema):
    """
    Ensure metadata completeness based on a schema.

    Parameters:
    metadata (dict): Existing metadata.
    schema (dict): Schema to validate against.

    Returns:
    dict: Complete metadata.
    """
    for feature in metadata['features']:
        for attribute, default_value in schema['default_attributes'].items():
            metadata['features'][feature].setdefault(attribute, default_value)
    return metadata

def save_metadata(metadata, paths):
    """
    Save metadata to a YAML file.

    Parameters:
    metadata (dict): Metadata to save.
    paths (dict): Dictionary containing file paths.
    """
    try:
        save_yaml(metadata, paths['config']['feature_metadata'])
    except Exception as e:
        print(f"Error saving metadata: {e}")
        raise

def display_metadata(metadata):
    """
    Display metadata content.

    Parameters:
    metadata (dict): Metadata to display.
    """
    try:
        print("\n--- Metadata to be stored in config/feature_metadata.yaml ---\n")
        print(yaml.dump(metadata, default_flow_style=False))
    except Exception as e:
        logging.error(f"Error displaying metadata: {e}", exc_info=True)
        raise

def backup_metadata(paths):
    """
    Create a backup of the metadata file.

    Parameters:
    paths (dict): Dictionary containing file paths.
    """
    try:
        backup_dir = os.path.join('config', 'backup')
        os.makedirs(backup_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
        backup_path = os.path.join(backup_dir, f'feature_metadata_{timestamp}.yaml')
        shutil.copy(paths['config']['feature_metadata'], backup_path)
        print(f"Backup created at {backup_path}")
    except Exception as e:
        print(f"Error creating metadata backup: {e}")
        raise

def determine_feature_type(feature, metadata):
    """
    Determine the type of a feature.

    Parameters:
    feature (str): Feature name.
    metadata (dict): Metadata containing feature details.

    Returns:
    str: Determined feature type.
    """
    classified_type = metadata['features'][feature].get('classified_data_type', '')
    if classified_type in ['binary', 'categorical']:
        return classified_type
    example_values = metadata['features'][feature].get('example_values', [])
    if all(isinstance(val, (int, float)) for val in example_values):
        if example_values and isinstance(example_values[0], int):
            return 'categorical'
    return 'numerical'











# Commented out duplicate or less relevant functions
# def ensure_data_overview(paths):
#     """
#     Ensure the data_overview section is present in the feature metadata.

#     Parameters:
#     paths (dict): Dictionary containing file paths from paths.yaml.

#     Returns:
#     dict: The updated feature metadata.
#     dict: The metadata schema.
#     """
#     feature_metadata_path = paths['config']['feature_metadata']
#     metadata_schema = load_yaml(paths['config']['feature_metadata_schema'])

#     if not os.path.exists(feature_metadata_path):
#         feature_metadata = {
#             "data_overview": metadata_schema["data_overview"],
#             "features": {}
#         }
#     else:
#         feature_metadata = load_yaml(feature_metadata_path)
#         if "data_overview" not in feature_metadata:
#             feature_metadata["data_overview"] = metadata_schema["data_overview"]
#         if "features" not in feature_metadata:
#             feature_metadata["features"] = {}

#     save_yaml(feature_metadata, feature_metadata_path)
#     return feature_metadata, metadata_schema

# def populate_feature_metadata(feature_metadata, feature_descriptions, metadata_schema):
#     """
#     Populate feature metadata with descriptions.

#     Parameters:
#     feature_metadata (dict): The feature metadata to update.
#     feature_descriptions (dict): The feature descriptions to add.
#     metadata_schema (dict): The metadata schema to use for default attributes.
    
#     Returns:
#     dict: The updated feature metadata.
#     """
#     for feature, description in feature_descriptions['features'].items():
#         if feature not in feature_metadata["features"]:
#             feature_metadata["features"][feature] = metadata_schema['default_attributes'].copy()
#         feature_metadata["features"][feature]['description'] = description.get('description', '')
#         feature_metadata["features"][feature]['security_context'] = description.get('security_context', '')

#     return feature_metadata

# def load_manual_review(manual_review_path):
#     """
#     Load the manually updated review file.

#     Parameters:
#     manual_review_path (str): Path to the manual review file.

#     Returns:
#     DataFrame: Loaded manual review DataFrame.
#     """
#     try:
#         return pd.read_csv(manual_review_path)
#     except Exception as e:
#         print(f"Error loading manual review file: {e}")
#         raise

# def update_metadata_from_review(manual_review, metadata, schema):
#     """
#     Update the metadata based on manual review.

#     Parameters:
#     manual_review (DataFrame): DataFrame containing the manual review.
#     metadata (dict): Current feature metadata.
#     schema (dict): Schema for feature metadata.

#     Returns:
#     dict: Updated feature metadata.
#     """
#     try:
#         metadata = ensure_metadata_completeness(metadata, schema)
#         for index, row in manual_review.iterrows():
#             feature = row['Feature']
#             new_type = row['Manual Review and Update']
#             updates = {'classified_data_type': new_type}
#             metadata = update_feature_metadata(metadata, feature, updates)
#         return metadata
#     except Exception as e:
#         print(f"Error updating metadata from review: {e}")
#         raise

# def initialize_feature_metadata(schema_path):
#     """
#     Initialize feature metadata based on a schema.

#     Parameters:
#     schema_path (str): Path to the schema YAML file.

#     Returns:
#     dict: Initialized feature metadata.
#     """
#     try:
#         schema = load_yaml(schema_path)
#         feature_metadata = {"features": {}}
#         for attribute, default_value in schema["default_attributes"].items():
#             feature_metadata[attribute] = default_value
#         return feature_metadata
#     except Exception as e:
#         print(f"Error initializing feature metadata: {e}")
#         raise

# def update_metadata_with_data_types(metadata, feature_data_types, attribute_name):
#     """
#     Update metadata with feature data types.

#     Parameters:
#     metadata (dict): Existing metadata.
#     feature_data_types (list): List of feature data types.
#     attribute_name (str): Attribute name to update in the metadata.

#     Returns:
#     dict: Updated metadata.
#     """
#     for item in feature_data_types:
#         feature = item['Feature']
#         data_type = item[attribute_name]
#         if feature in metadata['features']:
#             metadata['features'][feature][attribute_name] = data_type
#     return metadata



# def print_update_message(paths, config_key):
#     """
#     Print an update message for classified data types.

#     Parameters:
#     paths (dict): Dictionary containing file paths.
#     config_key (str): Configuration key indicating the path to update.
#     """
#     try:
#         config_path = paths['config'][config_key]
#         print(f"Updated classified_data_type attribute in {config_path}")
#     except Exception as e:
#         print(f"Error printing update message: {e}")
#         raise

# def update_classified_data_type(manual_updates, metadata, schema):
#     """
#     Update the classified_data_type attribute in the metadata based on manual updates.

#     Parameters:
#     manual_updates (dict): Dictionary containing manual feature classification updates.
#     metadata (dict): Existing feature metadata.
#     schema (dict): Schema for feature metadata.

#     Returns:
#     dict: Updated feature metadata.
#     """
#     try:
#         metadata = ensure_metadata_completeness(metadata, schema)
#         for feature, update in manual_updates.items():
#             new_type = update['Manual Review and Update']
#             updates = {'classified_data_type': new_type}
#             metadata = update_feature_metadata(metadata, feature, updates)
#         return metadata
#     except Exception as e:
#         print(f"Error updating metadata from review: {e}")
#         raise








# def execute_metadata_update_pipeline(train_sample, paths, analysis_results_dir, json_files, fields_to_update=None):
#     """
#     Execute the metadata update pipeline with various steps.

#     Parameters:
#     train_sample (pd.DataFrame): The training sample DataFrame.
#     paths (dict): Dictionary containing file paths.
#     analysis_results_dir (str): Directory for saving analysis results.
#     json_files (list): List of JSON files to initialize.
#     fields_to_update (list): List of fields to update in the metadata.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         results = load_analysis_results(analysis_results_dir, json_files)
#         logging.debug(f"Loaded analysis results: {results.keys()}")
#         metadata_path = paths['config']['feature_metadata']
#         metadata = load_metadata(metadata_path)
#         schema_path = paths['config']['feature_metadata_schema']
#         schema = load_yaml(schema_path)
        
#         backup_metadata(paths)
        
#         metadata = apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update)

#         save_updated_metadata(metadata, paths, schema)
        
#         logging.debug("Metadata before displaying:")
#         display_metadata(metadata)
        
#         return metadata
#     except Exception as e:
#         logging.error(f"Error in metadata update pipeline: {e}", exc_info=True)
#         raise

# def load_analysis_results(analysis_results_dir, json_files):
#     """
#     Load analysis results from JSON files.

#     Parameters:
#     analysis_results_dir (str): Directory containing analysis result files.
#     json_files (list): List of JSON files to load.

#     Returns:
#     dict: Loaded analysis results.
#     """
#     results = {}
#     for file in json_files:
#         file_path = os.path.join(analysis_results_dir, file)
#         if os.path.isfile(file_path):
#             with open(file_path, 'r') as f:
#                 results[file.split('.')[0]] = json.load(f)
#             logging.debug(f"Loaded {file} from {file_path}")
#     return results

# def load_metadata(metadata_path):
#     """
#     Load metadata from a YAML file.

#     Parameters:
#     metadata_path (str): Path to the metadata file.

#     Returns:
#     dict: Loaded metadata.
#     """
#     try:
#         return load_yaml(metadata_path)
#     except Exception as e:
#         print(f"Error loading metadata: {e}")
#         raise

# def apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update=None):
#     """
#     Update metadata with analysis results.

#     Parameters:
#     metadata (dict): Existing metadata.
#     schema (dict): Schema to validate against.
#     results (dict): Analysis results.
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.
#     fields_to_update (list): List of fields to update in the metadata.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         update_functions = {
#             'data_overview': update_data_overview,
#             'data_types': update_data_types,
#             'stat_summary': update_stat_summary,
#             'missing_values': update_missing_values,
#             'feature_balance': update_feature_balance,
#             'target_correlations': update_target_correlations,
#             'target_analysis': update_target_analysis,
#             'descriptions': update_metadata_with_descriptions,
#             'classifications_initial': update_feature_classifications_initial,
#             'classifications_manual': update_feature_classifications_manual,
#             'example_values': update_example_values,
#             'high_correlation_pairs': update_high_correlation_pairs,
#             'missing_values_target_correlations': update_missing_values_target_correlations,
#             'missing_values_pair_correlations': update_missing_values_pair_correlations
#         }

#         fields_to_update = fields_to_update or update_functions.keys()

#         for field in fields_to_update:
#             if field in update_functions:
#                 update_functions[field](metadata, results, paths, train_sample)

#         return metadata
#     except Exception as e:
#         logging.error(f"Error updating metadata with results: {e}", exc_info=True)
#         raise

# def update_feature_metadata(metadata, feature, updates):
#     """
#     Update the feature metadata with the provided updates.

#     Parameters:
#     metadata (dict): Existing metadata.
#     feature (str): Feature to update.
#     updates (dict): Dictionary of updates to apply.

#     Returns:
#     dict: Updated metadata.
#     """
#     for key, value in updates.items():
#         if isinstance(value, dict):
#             if metadata['features'][feature].get(key) is None:
#                 metadata['features'][feature][key] = {}
#             for sub_key, sub_value in value.items():
#                 metadata['features'][feature][key][sub_key] = sub_value
#         else:
#             metadata['features'][feature][key] = value
#     return metadata


# def update_missing_values(metadata, results, paths, train_sample):
#     """
#     Update metadata with missing values analysis results.

#     Parameters:
#     metadata (dict): Existing metadata.
#     results (dict): Analysis results.
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.
#     """
#     missing_values = results.get('missing_values', {})
#     for feature, data in missing_values.items():
#         if feature in metadata['features']:
#             metadata['features'][feature]['missing_values'] = {
#                 'count': data['count'],
#                 'percentage': data['percentage'],
#                 'correlation_with_target': data.get('correlation_with_target', metadata['features'][feature]['missing_values'].get('correlation_with_target'))
#             }
#             logging.debug(f"Updated missing values for feature: {feature}")
#         else:
#             logging.warning(f"Feature {feature} not found in metadata.")


# def load_analysis_results(analysis_results_dir, json_files):
#     """
#     Load analysis results from JSON files.

#     Parameters:
#     analysis_results_dir (str): Directory containing analysis result files.
#     json_files (list): List of JSON files to load.

#     Returns:
#     dict: Loaded analysis results.
#     """
#     results = {}
#     for file in json_files:
#         file_path = os.path.join(analysis_results_dir, file)
#         if os.path.isfile(file_path):
#             with open(file_path, 'r') as f:
#                 results[file.split('.')[0]] = json.load(f)
#             logging.debug(f"Loaded {file} from {file_path}")
#         else:
#             logging.warning(f"File {file} not found in {analysis_results_dir}")
#     return results

# def load_metadata(metadata_path):
#     """
#     Load metadata from a YAML file.

#     Parameters:
#     metadata_path (str): Path to the metadata file.

#     Returns:
#     dict: Loaded metadata.
#     """
#     try:
#         return load_yaml(metadata_path)
#     except Exception as e:
#         print(f"Error loading metadata: {e}")
#         raise

# def apply_analysis_results_to_metadata(metadata, schema, results, paths, train_sample, fields_to_update=None):
#     """
#     Update metadata with analysis results.

#     Parameters:
#     metadata (dict): Existing metadata.
#     schema (dict): Schema to validate against.
#     results (dict): Analysis results.
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.
#     fields_to_update (list): List of fields to update in the metadata.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         update_functions = {
#             'data_overview': update_data_overview,
#             'data_types': update_data_types,
#             'stat_summary': update_stat_summary,
#             'missing_values': update_missing_values,
#             'feature_balance': update_feature_balance,
#             'target_correlations': update_target_correlations,
#             'target_analysis': update_target_analysis,
#             'descriptions': update_metadata_with_descriptions,
#             'classifications_initial': update_feature_classifications_initial,
#             'classifications_manual': update_feature_classifications_manual,
#             'example_values': update_example_values,
#             'high_correlation_pairs': update_high_correlation_pairs,
#             'missing_values_target_correlations': update_missing_values_target_correlations,
#             'missing_values_pair_correlations': update_missing_values_pair_correlations,
#         }

#         fields_to_update = fields_to_update or update_functions.keys()

#         for field in fields_to_update:
#             if field in update_functions:
#                 logging.debug(f"Updating field: {field}")
#                 update_functions[field](metadata, results, paths, train_sample)

#         return metadata
#     except Exception as e:
#         logging.error(f"Error updating metadata with results: {e}", exc_info=True)
#         raise


# def update_missing_values_target_correlations(metadata, results, paths, train_sample):
#     """
#     Update metadata with missing values' correlation with the target variable.

#     Parameters:
#     metadata (dict): Existing metadata.
#     results (dict): Analysis results containing missing values correlations.
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.

#     Returns:
#     dict: Updated metadata.
#     """
#     if 'missing_values_correlations_with_target_variable' in results:
#         correlations = results['missing_values_correlations_with_target_variable']
#         logging.debug(f"Correlations found: {correlations}")
#         for item in correlations:
#             feature = item['Feature']
#             correlation = item['Correlation']
#             if feature in metadata['features']:
#                 if 'missing_values' not in metadata['features'][feature]:
#                     metadata['features'][feature]['missing_values'] = {}
#                 metadata['features'][feature]['missing_values']['missing_values_correlation_with_target'] = correlation
#                 logging.debug(f"Updated missing_values.missing_values_correlation_with_target for feature: {feature} to {correlation}")
#             else:
#                 logging.warning(f"Feature {feature} not found in metadata.")
#     else:
#         logging.warning("No missing values correlations with target found in the results.")
#     return metadata

# def update_missing_values_pair_correlations(metadata, results, paths, train_sample):
#     """
#     Update metadata with missing values' pair correlations.

#     Parameters:
#     metadata (dict): Existing metadata.
#     results (dict): Analysis results containing pair correlations.
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.

#     Returns:
#     dict: Updated metadata.
#     """
#     if 'missing_value_pair_correlations' in results:
#         pair_correlations = results['missing_value_pair_correlations']
#         logging.debug(f"Pair correlations found: {pair_correlations}")
#         for item in pair_correlations:
#             feature1 = item['Feature1']
#             feature2 = item['Feature2']
#             correlation = item['Correlation']
#             if feature1 in metadata['features']:
#                 if 'missing_values' not in metadata['features'][feature1]:
#                     metadata['features'][feature1]['missing_values'] = {}
#                 if 'missing_values_correlation_with_other_features' not in metadata['features'][feature1]['missing_values']:
#                     metadata['features'][feature1]['missing_values']['missing_values_correlation_with_other_features'] = {}
#                 metadata['features'][feature1]['missing_values']['missing_values_correlation_with_other_features'][feature2] = correlation
#                 logging.debug(f"Updated missing_values.missing_values_correlation_with_other_features for feature: {feature1} to {correlation} with {feature2}")
#             else:
#                 logging.warning(f"Feature {feature1} not found in metadata.")
#     else:
#         logging.warning("No missing values pair correlations found in the results.")
#     return metadata

# def update_data_overview(metadata, results, paths, train_sample):
#     if 'data_overview' in results:
#         metadata['data_overview'] = results['data_overview']
#         logging.debug("Updated data_overview.")

# def update_data_types(metadata, results, paths, train_sample):
#     if 'data_types' in results:
#         for feature, dtype in results['data_types'].items():
#             metadata = update_feature_metadata(metadata, feature, {'technical_data_type': dtype})
#             logging.debug(f"Updated technical_data_type for feature: {feature} to {dtype}")

# def update_stat_summary(metadata, results, paths, train_sample):
#     if 'stat_summary' in results:
#         for feature, stats in results['stat_summary'].items():
#             metadata = update_feature_metadata(metadata, feature, {'summary_statistics': stats})
#             logging.debug(f"Updated summary_statistics for feature: {feature} to {stats}")

# def update_missing_values(metadata, results, paths, train_sample):
#     if 'missing_values' in results:
#         for feature, missing_data in results['missing_values'].items():
#             metadata = update_feature_metadata(metadata, feature, {'missing_values': missing_data})
#             logging.debug(f"Updated missing_values for feature: {feature} to {missing_data}")

# def update_feature_balance(metadata, results, paths, train_sample):
#     if 'feature_balance' in results:
#         for feature, balance_data in results['feature_balance'].items():
#             metadata = update_feature_metadata(metadata, feature, {'balance': {'most_common_value_weight': balance_data['most_common_value_weight']}})
#             logging.debug(f"Updated balance for feature: {feature} to {balance_data['most_common_value_weight']}")

# def update_correlations(metadata, results, paths, train_sample):
#     if 'correlations' in results:
#         for feature, correlation_data in results['correlations'].items():
#             metadata = update_feature_metadata(metadata, feature, {'correlation_with_other_features': correlation_data})
#             logging.debug(f"Updated correlation_with_other_features for feature: {feature} to {correlation_data}")

# def update_target_correlations(metadata, results, paths, train_sample):
#     if 'target_correlations' in results:
#         for feature, correlation in results['target_correlations'].items():
#             metadata = update_feature_metadata(metadata, feature, {'correlation_with_target': correlation})
#             logging.debug(f"Updated correlation_with_target for feature: {feature} to {correlation}")

# def update_target_analysis(metadata, results, paths, train_sample):
#     if 'target_analysis' in results:
#         metadata['data_overview']['target_variable_analysis'] = results['target_analysis']
#         logging.debug("Updated target_variable_analysis.")


    
# def update_metadata_with_descriptions(metadata, results, paths, train_sample):
#     """
#     Update metadata with feature descriptions.

#     Parameters:
#     metadata (dict): Existing metadata.
#     results (dict): Analysis results (not used in this function).
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         descriptions_data_path = paths['config']['feature_descriptions']
#         descriptions_data = load_yaml(descriptions_data_path)

#         for feature, details in descriptions_data['features'].items():
#             metadata = update_feature_metadata(metadata, feature, {
#                 'description': details['description'],
#                 'security_context': details['security_context']
#             })
#         return metadata
#     except Exception as e:
#         logging.error(f"Error updating metadata with descriptions: {e}", exc_info=True)
#         raise

# def update_feature_classifications_initial(metadata, results, paths, train_sample):
#     """
#     Classify and update feature types in metadata.

#     Parameters:
#     metadata (dict): Existing metadata.
#     results (dict): Analysis results (not used in this function).
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         from src.analysis.data_understanding import feature_classification

#         binary_features_auto, categorical_features_auto, numerical_features_auto = feature_classification(train_sample)

#         for feature in binary_features_auto:
#             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'binary'})

#         for feature in categorical_features_auto:
#             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'categorical'})

#         for feature in numerical_features_auto:
#             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': 'numerical'})

#         return metadata
#     except Exception as e:
#         logging.error(f"Error classifying and updating features: {e}")
#         raise

# def update_feature_classifications_manual(metadata, results, paths, train_sample):
#     """
#     Update metadata with manual feature classifications.

#     Parameters:
#     metadata (dict): Existing metadata.
#     results (dict): Analysis results (not used in this function).
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         manual_classifications_path = paths['config']['manual_feature_classification_update']
#         manual_classifications = load_yaml(manual_classifications_path)

#         for feature, details in manual_classifications['manual_feature_classification_updates'].items():
#             classified_data_type = details['Manual Review and Update']
#             metadata = update_feature_metadata(metadata, feature, {'classified_data_type': classified_data_type})
#         return metadata
#     except Exception as e:
#         logging.error(f"Error updating manual feature classifications: {e}", exc_info=True)
#         raise

# def update_example_values(metadata, results, paths, train_sample):
#     """
#     Update metadata with example values for features.

#     Parameters:
#     metadata (dict): Existing metadata.
#     results (dict): Analysis results.
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.

#     Returns:
#     dict: Updated metadata.
#     """
#     def select_example_values(feature, dtype, dataframe):
#         if dtype in ['int64', 'float64']:
#             return [
#                 dataframe[feature].min(),
#                 dataframe[feature].max(),
#                 dataframe[feature].mean(),
#                 dataframe[feature].median(),
#                 dataframe[feature].std()
#             ]
#         elif dtype == 'object':
#             return dataframe[feature].value_counts().index.tolist()[:5]
#         else:
#             return dataframe[feature].unique().tolist()

#     try:
#         if 'data_types' in results:
#             for feature, dtype in results['data_types'].items():
#                 example_values = select_example_values(feature, dtype, train_sample)
#                 metadata = update_feature_metadata(metadata, feature, {'example_values': example_values})
#         return metadata
#     except Exception as e:
#         logging.error(f"Error updating example values: {e}", exc_info=True)
#         raise

# def update_high_correlation_pairs(metadata, results, paths, train_sample):
#     """
#     Update metadata with high correlation pairs.

#     Parameters:
#     metadata (dict): Existing metadata.
#     results (dict): Analysis results.
#     paths (dict): Dictionary containing file paths.
#     train_sample (pd.DataFrame): The training sample data.

#     Returns:
#     dict: Updated metadata.
#     """
#     try:
#         if 'high_correlation_pairs' in results:
#             high_corr_pairs = results['high_correlation_pairs']
#             for pair_str, corr_data in high_corr_pairs.items():
#                 feature1, feature2 = eval(pair_str)
#                 metadata = update_feature_metadata(metadata, feature1, {
#                     'correlation_with_other_features': {feature2: corr_data['correlation']}
#                 })
#                 metadata = update_feature_metadata(metadata, feature2, {
#                     'correlation_with_other_features': {feature1: corr_data['correlation']}
#                 })
#         return metadata
#     except Exception as e:
#         logging.error(f"Error updating high correlation pairs: {e}", exc_info=True)
#         raise

# def update_feature_metadata(feature_metadata, feature, updates):
#     """
#     Update feature metadata with given updates.

#     Parameters:
#     feature_metadata (dict): Existing feature metadata.
#     feature (str): Feature name to update.
#     updates (dict): Dictionary containing updates.

#     Returns:
#     dict: Updated feature metadata.
#     """
#     logging.debug(f"Updating feature: {feature} with updates: {updates}")
#     if feature not in feature_metadata["features"]:
#         feature_metadata["features"][feature] = {}
#     for key, value in updates.items():
#         if isinstance(value, dict):
#             feature_metadata["features"][feature].setdefault(key, {})
#             for sub_key, sub_value in value.items():
#                 feature_metadata["features"][feature][key][sub_key] = sub_value
#                 logging.debug(f"Updated {key}.{sub_key} for {feature} to {sub_value}")
#         else:
#             feature_metadata["features"][feature][key] = value
#             logging.debug(f"Updated {key} for {feature} to {value}")
#     return feature_metadata

# def save_updated_metadata(metadata, paths, schema):
#     """
#     Save updated metadata after ensuring completeness and converting to native types.

#     Parameters:
#     metadata (dict): Existing metadata.
#     paths (dict): Dictionary containing file paths.
#     schema (dict): Schema to validate against.
#     """
#     try:
#         metadata = ensure_metadata_completeness(metadata, schema)
#         metadata = convert_to_native_types(metadata)
#         save_metadata(metadata, paths)
#     except Exception as e:
#         print(f"Error saving updated metadata: {e}")
#         raise

# def ensure_metadata_completeness(metadata, schema):
#     """
#     Ensure metadata completeness based on a schema.

#     Parameters:
#     metadata (dict): Existing metadata.
#     schema (dict): Schema to validate against.

#     Returns:
#     dict: Complete metadata.
#     """
#     for feature in metadata['features']:
#         for attribute, default_value in schema['default_attributes'].items():
#             metadata['features'][feature].setdefault(attribute, default_value)
#     return metadata

# def save_metadata(metadata, paths):
#     """
#     Save metadata to a YAML file.

#     Parameters:
#     metadata (dict): Metadata to save.
#     paths (dict): Dictionary containing file paths.
#     """
#     try:
#         save_yaml(metadata, paths['config']['feature_metadata'])
#     except Exception as e:
#         print(f"Error saving metadata: {e}")
#         raise

# def display_metadata(metadata):
#     """
#     Display metadata content.

#     Parameters:
#     metadata (dict): Metadata to display.
#     """
#     try:
#         print("\n--- Metadata to be stored in config/feature_metadata.yaml ---\n")
#         print(yaml.dump(metadata, default_flow_style=False))
#     except Exception as e:
#         logging.error(f"Error displaying metadata: {e}", exc_info=True)
#         raise








































# def backup_metadata(paths):
#     """
#     Create a backup of the metadata file.

#     Parameters:
#     paths (dict): Dictionary containing file paths.
#     """
#     try:
#         backup_dir = os.path.join('config', 'backup')
#         os.makedirs(backup_dir, exist_ok=True)
#         timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
#         backup_path = os.path.join(backup_dir, f'feature_metadata_{timestamp}.yaml')
#         shutil.copy(paths['config']['feature_metadata'], backup_path)
#         print(f"Backup created at {backup_path}")
#     except Exception as e:
#         print(f"Error creating metadata backup: {e}")
#         raise




