# src/utils/validation_utils.py

import logging
import pandas as pd
from tqdm.notebook import tqdm
from src.utils.display_utils import display_dataframe_as_html


def validate_stratification_with_progress(original_df, sample_df, stratify_by):
    """
    Validate that the stratification of the sample matches the original dataset.
    
    Parameters:
    original_df (pd.DataFrame): The original dataframe.
    sample_df (pd.DataFrame): The sampled dataframe.
    stratify_by (str): The column to stratify by.
    
    Returns:
    bool: True if stratification is correct, False otherwise.
    pd.DataFrame: DataFrame showing the comparison of distributions.
    """
    if stratify_by not in original_df.columns or stratify_by not in sample_df.columns:
        raise KeyError(f"Column '{stratify_by}' not found in one of the dataframes.")
    
    original_distribution = original_df[stratify_by].value_counts(normalize=True)
    sample_distribution = sample_df[stratify_by].value_counts(normalize=True)
    
    comparison_df = pd.DataFrame({
        'Original': original_distribution,
        'Sample': sample_distribution
    })

    progress_bar = tqdm(total=len(comparison_df), desc="Validating stratification")
    for i in range(len(comparison_df)):
        progress_bar.update(1)
    progress_bar.close()

    return comparison_df['Original'].equals(comparison_df['Sample']), comparison_df


def validate_size_and_shape(original_df, sample_df):
    """
    Validate the size and shape of the sample dataset against the original dataset.
    
    Parameters:
    original_df (pd.DataFrame): The original dataframe.
    sample_df (pd.DataFrame): The sampled dataframe.

    Returns:
    tuple: Shapes of original and sample datasets and size of the sample dataset in MB.
    """
    original_shape = original_df.shape
    sample_shape = sample_df.shape
    sample_size_mb = sample_df.memory_usage(deep=True).sum() / (1024 * 1024)

    return original_shape, sample_shape, sample_size_mb


def ensure_all_features_in_comparison_table(dataframe, comparison_table, metadata, comparison_table_path):
    """
    Ensure all features in the dataset are included in the comparison table.
    
    Parameters:
    dataframe (pd.DataFrame): The dataset.
    comparison_table (pd.DataFrame): The existing comparison table.
    metadata (dict): The feature metadata.
    comparison_table_path (str): The path to save the updated comparison table.
    
    Returns:
    pd.DataFrame: The updated comparison table.
    """
    # Extract existing features from the comparison table
    existing_features = comparison_table['Feature'].tolist()
    
    # Find missing features
    missing_features = [feature for feature in dataframe.columns if feature not in existing_features]
    
    # Create entries for missing features
    new_entries = []
    for feature in missing_features:
        classified_type = metadata['features'][feature].get('classified_data_type', 'Unknown')
        determined_type = determine_feature_type(feature, metadata)
        new_entries.append({
            'Feature': feature,
            'Previously Analyzed Type': classified_type,
            'Current Analyzed Type': determined_type,
            'Discrepancy': classified_type != determined_type,
            'Manual Review and Update': determined_type  # Initialize with current analyzed type
        })
    
    # Append new entries to the comparison table
    if new_entries:
        new_entries_df = pd.DataFrame(new_entries)
        comparison_table = pd.concat([comparison_table, new_entries_df], ignore_index=True)
        comparison_table.sort_values(by=['Discrepancy', 'Previously Analyzed Type'], ascending=[False, True], inplace=True)
    
    # Save the updated comparison table
    comparison_table.to_csv(comparison_table_path, index=False)
    
    # Print validation results
    print(f"Total features in the dataset: {len(dataframe.columns)}")
    print(f"Total features in the comparison table: {comparison_table.shape[0]}")
    print(f"Missing features added: {len(missing_features)}")
    
    return comparison_table

# src/utils/validation_utils.py
import logging


def validate_data_types(data, metadata):
    """
    Validate that data types of the DataFrame columns match the metadata specifications.

    Parameters:
    data (pd.DataFrame): The DataFrame to validate.
    metadata (dict): Metadata containing feature information.

    Returns:
    None
    """
    logging.info("Starting data type validation")
    validation_errors = []
    
    for feature, details in metadata['features'].items():
        if feature in data.columns:
            expected_dtype = details.get('technical_data_type', None)
            actual_dtype = str(data[feature].dtype)
            
            if expected_dtype and expected_dtype != actual_dtype:
                error_message = f"Feature '{feature}': Expected dtype technical_data_type '{expected_dtype}', but got dtype '{actual_dtype}'"
                validation_errors.append(error_message)
                logging.error(error_message)
            else:
                logging.info(f"Feature '{feature}': Data type dtype '{actual_dtype}' matches the expected technical_data_type type.")
    
    if not validation_errors:
        logging.info("Data type validation passed with no errors.")
    else:
        logging.warning("Data type validation found discrepancies.")
        for error in validation_errors:
            logging.warning(error)

    logging.info("Data type validation completed.")