# src/utils/initialization.py

import os
import json
import logging
import pandas as pd
from jsonschema import validate, ValidationError
from src.utils.file_operations import load_json_file
from src.config_loader import load_paths
from src.utils.data_loading_utils import read_csv_with_progress
from typing import Dict, Any, Tuple  
from src.feature_engineering.data_types import convert_data_types
from src.validation.validate_data_types import validate_data_types

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def initialize_data(train_data_path: str, test_data_path: str, paths: Dict[str, Dict[str, str]]) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Dict[str, str]]]:
    """
    Initialize the project by loading, converting, and validating the train and test datasets.

    Parameters:
    train_data_path (str): Path to the train data file.
    test_data_path (str): Path to the test data file.
    paths (Dict[str, Dict[str, str]]): Dictionary containing project paths.

    Returns:
    Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Dict[str, str]]]: The validated train data, test data, and metadata.
    """
    try:
        # Define the metadata path
        metadata_path = paths['config']['feature_metadata']

        # Load metadata
        metadata = load_json_file(metadata_path)
        if not isinstance(metadata, dict):
            logging.error("Expected metadata to be a dictionary, but got a different type.")
            raise TypeError("Metadata should be a dictionary, not a string.")

        # Load train and test data
        train_data = read_csv_with_progress(train_data_path)
        test_data = read_csv_with_progress(test_data_path)

        # Convert data types
        train_data = convert_data_types(train_data, metadata)
        test_data = convert_data_types(test_data, metadata)

        # Validate data types
        validate_data_types(train_data, metadata)
        validate_data_types(test_data, metadata)

        logging.info("Data initialization completed successfully.")
        return train_data, test_data, metadata

    except Exception as e:
        logging.error(f"Error in data initialization: {e}")
        raise


def initialize_train_test_and_data_update(train_data_path: str, test_data_path: str):
    """
    Load the specific CSV data file and its corresponding schema, then convert data types based on metadata.

    Parameters:
    data_file_path (str): Path to the CSV data file.

    Returns:
    pd.DataFrame: The loaded data.
    dict: The loaded schema.
    dict: The loaded feature metadata.
    list: The loaded JSON files.
    dict: The paths dictionary.
    """
    try:
        logging.info(f"Loading train data from {train_data_path}")
        train_data = read_csv_with_progress(train_data_path)

        logging.info(f"Loading train data from {test_data_path}")
        test_data = read_csv_with_progress(test_data_path)
        
        logging.info("Loading paths configuration")
        paths = load_paths()
        
        logging.info("Loading feature metadata schema")
        schema = load_json_file(paths['config']['schemas']['feature_metadata_complete_schema_json'])
        logging.debug(f"Schema: {schema}")
        
        logging.info("Loading feature metadata")
        feature_metadata = load_json_file(paths['config']['feature_metadata'])
        logging.debug(f"Metadata: {feature_metadata}")
        
        # logging.info("Converting train_data data types based on feature metadata")
        train_data = convert_data_types(train_data, feature_metadata)

        # Validate data types after conversion
        # logging.info("Validating data types after conversion")
        validate_data_types(train_data, feature_metadata)

        # logging.info("Converting test_data data types based on feature metadata")
        test_data = convert_data_types(test_data, feature_metadata)
        
        # Validate data types after conversion
        # logging.info("Validating data types after conversion")
        validate_data_types(test_data, feature_metadata)
        
        return train_data, test_data, schema, feature_metadata
    except Exception as e:
        logging.error(f"Error initializing feature metadata update: {e}")
        raise


def initialize_data_and_metadata(paths: Dict[str, Any]) -> Tuple[Any, Any, Dict[str, Any], Dict[str, Any]]:
    """
    Initialize and load both training and test data, along with the schema and feature metadata.

    Parameters:
    paths (Dict[str, Any]): The paths dictionary loaded from config.

    Returns:
    Tuple[Any, Any, Dict[str, Any], Dict[str, Any]]: Loaded train data, test data, schema, and feature metadata.
    """
    train_data_file_path = paths['data']['train_sample']
    test_data_file_path = paths['data']['test_sample']

    # Load train data and associated schema and metadata
    train_data, schema, feature_metadata, _ = initialize_feature_metadata_update_pre_data_type_handling(train_data_file_path, paths)

    # Load test data (reusing schema and metadata)
    test_data, _, _, _ = initialize_feature_metadata_update_pre_data_type_handling(test_data_file_path, paths)
    
    return train_data, test_data, schema, feature_metadata


def initialize_feature_metadata_update_pre_data_type_handling(data_file_path: str, paths):
    """
    Load the specific CSV data file and its corresponding schema, then convert data types based on metadata.

    Parameters:
    data_file_path (str): Path to the CSV data file.

    Returns:
    pd.DataFrame: The loaded data.
    dict: The loaded schema.
    dict: The loaded feature metadata.
    dict: The paths dictionary.
    """
    try:
        logging.info(f"Loading data from {data_file_path}")
        data = read_csv_with_progress(data_file_path)
        
        logging.info("Loading feature metadata schema")
        schema = load_json_file(paths['config']['schemas']['feature_metadata_complete_schema_json'])
        logging.debug(f"Schema: {schema}")

        logging.info("Loading feature metadata")
        feature_metadata = load_json_file(paths['config']['feature_metadata'])
        logging.debug(f"Metadata: {feature_metadata}")
        
        return data, schema, feature_metadata, paths
    except Exception as e:
        logging.error(f"Error initializing feature metadata update: {e}")
        raise


def load_train_and_test_data_initialize_schema(train_data_path: str, test_data_path: str, schema_path: str):
    """
    Load train and test data, and initialize the schema.
    """
    try:
        logging.info(f"Loading train data from {train_data_path}")
        train_data = read_csv_with_progress(train_data_path)

        logging.info(f"Loading test data from {test_data_path}")
        test_data = read_csv_with_progress(test_data_path)

        logging.info("Loading feature metadata schema")
        schema = load_json_file(schema_path)
        
        return train_data, test_data, schema
    except Exception as e:
        logging.error(f"Error initializing feature metadata update: {e}")
        raise


def initialize_nested_dict(schema_section):
    """
    Recursively initialize a nested dictionary based on the schema.
    """
    initialized_dict = {}
    for key, value in schema_section.get('properties', {}).items():
        if isinstance(value, dict) and value.get('type') == 'object':
            initialized_dict[key] = initialize_nested_dict(value)
        elif isinstance(value, dict) and value.get('type') == 'array':
            initialized_dict[key] = []
        else:
            initialized_dict[key] = None
    return initialized_dict


def initialize_metadata(schema, train_sample, test_sample):
    """
    Initialize metadata based on the provided schema and data samples.
    """
    metadata = {'features': {}}
    default_attributes = schema['metadata_schema']['properties']['default_attributes']
    
    # Initialize metadata for each feature in train_sample
    for feature in train_sample.columns:
        metadata['features'][feature] = initialize_nested_dict(default_attributes)
    
    # Initialize data_overview section
    data_overview_template = schema['metadata_schema']['properties']['data_overview']
    metadata['data_overview'] = initialize_nested_dict(data_overview_template)
    # metadata['data_overview'].update({
    #     'train_shape': list(train_sample.shape),
    #     'test_shape': list(test_sample.shape),
    #     'train_columns': list(train_sample.columns),
    #     'test_columns': list(test_sample.columns)
    # })

    return metadata


def validate_metadata(metadata, schema_path):
    """
    Validate metadata against a JSON schema.

    Parameters:
    metadata (dict): The metadata to validate.
    schema_path (str): Path to the JSON schema file.
    """
    schema = load_json_file(schema_path)

    try:
        validate(instance=metadata, schema=schema)
        logging.info("Metadata is consistent with the JSON schema.")
    except ValidationError as e:
        logging.error(f"Metadata is not consistent with the JSON schema: {e}")
        raise


def load_train_and_test_data(train_data_path: str, test_data_path: str, paths):
    """
    Load train and test data.

    Parameters:
    train_data_path (str): Path to the train data file.
    test_data_path (str): Path to the test data file.
    paths (dict): Dictionary containing file paths.

    Returns:
    tuple: Train data and test data.
    """
    try:
        logging.info(f"Loading train data from {train_data_path}")
        train_data = read_csv_with_progress(train_data_path)

        logging.info(f"Loading test data from {test_data_path}")
        test_data = read_csv_with_progress(test_data_path)
        
        return train_data, test_data
    except Exception as e:
        logging.error(f"Error initializing feature metadata update: {e}")
        raise






