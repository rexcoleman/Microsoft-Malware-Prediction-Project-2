# src/utils/file_operations.py

import yaml
import os
import json
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm

def save_dataframe_with_progress(df, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    total_rows = len(df)
    progress_bar = tqdm(total=total_rows, desc=f"Saving {os.path.basename(path)}")
    with open(path, 'w') as file:
        for i, chunk in enumerate(np.array_split(df, 100)):
            chunk.to_csv(file, header=(i == 0), index=False)
            progress_bar.update(len(chunk))
    progress_bar.close()

def load_csv_with_progress(file_path, chunksize=10000):
    total_lines = sum(1 for _ in open(file_path)) - 1
    chunk_list = []
    progress_bar = tqdm(total=total_lines // chunksize + 1, desc=f"Loading {os.path.basename(file_path)}")
    for chunk in pd.read_csv(file_path, chunksize=chunksize, low_memory=False):
        chunk_list.append(chunk)
        progress_bar.update(1)
    progress_bar.close()
    return pd.concat(chunk_list, axis=0)

def load_yaml(file_path):
    with open(file_path, 'r') as file:
        return yaml.safe_load(file)

def save_yaml(data, file_path):
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'w') as file:
        yaml.safe_dump(data, file)

def load_json_file(filepath):
    try:
        with open(filepath, 'r') as file:
            return json.load(file)
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON from file {filepath}: {e}")
        return None
    except FileNotFoundError:
        return {}

def save_to_json(data, filename):
    with open(filename, 'w') as f:
        json.dump(data, f, indent=4)

def save_json_file(data, filepath):
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, 'w') as file:
        json.dump(data, file, indent=4, default=convert_to_native_types)

def initialize_json_files(json_files, save_dir):
    for json_file in json_files:
        json_path = os.path.join(save_dir, json_file)
        if not os.path.exists(json_path):
            save_json_file({}, json_path)

def convert_to_native_types(data):
    if isinstance(data, np.integer):
        return int(data)
    elif isinstance(data, np.floating):
        return float(data)
    elif isinstance(data, np.ndarray):
        return data.tolist()
    elif isinstance(data, dict):
        return {k: convert_to_native_types(v) for k, v in data.items()}
    elif isinstance(data, list):
        return [convert_to_native_types(v) for v in data]
    else:
        return data

def save_analysis_results(train_summary_stats, test_summary_stats, imputation_strategies, report_dir):
    tables_dir = os.path.join(report_dir, 'tables')
    os.makedirs(tables_dir, exist_ok=True)

    train_summary_stats.to_csv(os.path.join(tables_dir, 'train_summary_statistics.csv'), index=False)
    test_summary_stats.to_csv(os.path.join(tables_dir, 'test_summary_statistics.csv'), index=False)

    with open(os.path.join(tables_dir, 'imputation_strategies.yaml'), 'w') as file:
        yaml.safe_dump(imputation_strategies, file)





# # src/utils/file_operations.py

# import yaml
# import os
# import json
# import numpy as np
# import pandas as pd
# from tqdm.notebook import tqdm

# def save_dataframe_with_progress(df, path):
#     os.makedirs(os.path.dirname(path), exist_ok=True)
#     total_rows = len(df)
#     progress_bar = tqdm(total=total_rows, desc=f"Saving {os.path.basename(path)}")
#     with open(path, 'w') as file:
#         for i, chunk in enumerate(np.array_split(df, 100)):
#             chunk.to_csv(file, header=(i == 0), index=False)
#             progress_bar.update(len(chunk))
#     progress_bar.close()

# def load_csv_with_progress(file_path, chunksize=10000):
#     total_lines = sum(1 for _ in open(file_path)) - 1  # Calculate total lines in the file
#     chunk_list = []
#     progress_bar = tqdm(total=total_lines // chunksize + 1, desc=f"Loading {os.path.basename(file_path)}")
#     for chunk in pd.read_csv(file_path, chunksize=chunksize, low_memory=False):
#         chunk_list.append(chunk)
#         progress_bar.update(1)
#     progress_bar.close()
#     return pd.concat(chunk_list, axis=0)

# def load_yaml(file_path):
#     with open(file_path, 'r') as file:
#         return yaml.safe_load(file)

# def save_yaml(data, file_path):
#     os.makedirs(os.path.dirname(file_path), exist_ok=True)
#     with open(file_path, 'w') as file:
#         yaml.safe_dump(data, file)

# def load_json_file(filepath):
#     try:
#         with open(filepath, 'r') as file:
#             return json.load(file)
#     except json.JSONDecodeError as e:
#         print(f"Error decoding JSON from file {filepath}: {e}")
#         return None
#     except FileNotFoundError:
#         return {}

# def save_json_file(data, filepath):
#     os.makedirs(os.path.dirname(filepath), exist_ok=True)
#     with open(filepath, 'w') as file:
#         json.dump(data, file, indent=4, default=convert_to_native_types)

# def clean_json_file(filepath):
#     data = load_json_file(filepath)
#     if data:
#         cleaned_data = clean_balance_format(data)
#         save_json_file(cleaned_data, filepath)

# def convert_to_native_types(data):
#     if isinstance(data, np.integer):
#         return int(data)
#     elif isinstance(data, np.floating):
#         return float(data)
#     elif isinstance(data, np.ndarray):
#         return data.tolist()
#     elif isinstance(data, dict):
#         return {k: convert_to_native_types(v) for k, v in data.items()}
#     elif isinstance(data, list):
#         return [convert_to_native_types(v) for v in data]
#     else:
#         return data

