# src/utils/data_loading_utils.py

import os
import pandas as pd
from tqdm.notebook import tqdm
from src.config_loader import load_paths
from src.utils.file_operations import initialize_json_files

def read_csv_with_progress(file_path, chunksize=10000, sample_size=None):
    """
    Reads a CSV file with a progress bar.

    Parameters:
    file_path (str): Path to the CSV file.
    chunksize (int): Number of rows per chunk to read.
    sample_size (int, optional): Number of rows to sample. Defaults to None.

    Returns:
    pd.DataFrame: The loaded DataFrame.
    
    Raises:
    Exception: If there is an error reading the CSV file.
    """
    try:
        total_lines = sum(1 for _ in open(file_path)) - 1  # Calculate total lines in the file
        print(f"Total lines: {total_lines}")

        if sample_size:
            # Load a sample of the data
            chunks = []
            for chunk in tqdm(pd.read_csv(file_path, chunksize=chunksize, low_memory=False), desc="Loading data"):
                chunks.append(chunk)
                if len(chunks) * chunksize >= sample_size:
                    break
            return pd.concat(chunks, axis=0).head(sample_size)
        else:
            # Load the full dataset
            chunk_list = []
            for chunk in tqdm(pd.read_csv(file_path, chunksize=chunksize, low_memory=False), total=total_lines // chunksize + 1, desc="Loading data"):
                chunk_list.append(chunk)
            return pd.concat(chunk_list, axis=0)
    except Exception as e:
        print(f"Error reading CSV with progress: {e}")
        raise

def load_sample_data_and_paths():
    """
    Loads sample data and configuration paths.

    Returns:
    tuple: A tuple containing train_sample DataFrame, test_sample DataFrame, and paths dictionary.

    Raises:
    ValueError: If 'HasDetections' column is missing in train_sample.
    Exception: If there is an error loading sample data and paths.
    """
    try:
        paths = load_paths()
        train_sample = read_csv_with_progress(paths['data']['train_sample'], chunksize=10000)
        test_sample = read_csv_with_progress(paths['data']['test_sample'], chunksize=10000)
        print(f"Shape of train_sample: {train_sample.shape}")
        print(f"Shape of test_sample: {test_sample.shape}")
        if 'HasDetections' not in train_sample.columns:
            raise ValueError("'HasDetections' column is missing in train_sample.")
        return train_sample, test_sample, paths
    except Exception as e:
        print(f"Error loading sample data and paths: {e}")
        raise

def initialize_analysis_results(analysis_results_dir, json_files):
    """
    Initialize JSON files for storing analysis results.

    Parameters:
    analysis_results_dir (str): Directory to store analysis results.
    json_files (list): List of JSON files to initialize.

    Raises:
    Exception: If there is an error initializing analysis results.
    """
    try:
        os.makedirs(analysis_results_dir, exist_ok=True)
        initialize_json_files(json_files, analysis_results_dir)
    except Exception as e:
        print(f"Error initializing analysis results: {e}")
        raise

def save_dataframe_with_progress(df, path, chunk_size=1000):
    """
    Saves a DataFrame to a CSV file with a progress bar.

    Parameters:
    df (pd.DataFrame): DataFrame to save.
    path (str): Path to save the CSV file.
    chunk_size (int): Number of rows per chunk to save. Defaults to 1000.

    Raises:
    Exception: If there is an error saving the DataFrame.
    """
    try:
        total_rows = df.shape[0]
        chunks = [df[i:i + chunk_size] for i in range(0, total_rows, chunk_size)]
        
        for chunk in tqdm(chunks, desc=f"Saving {os.path.basename(path)}", total=len(chunks)):
            mode = 'w' if chunk is chunks[0] else 'a'
            header = True if chunk is chunks[0] else False
            chunk.to_csv(path, mode=mode, header=header, index=False)
    except Exception as e:
        print(f"Error saving DataFrame with progress: {e}")
        raise

def save_dataframe(df, path):
    """
    Saves a DataFrame to a CSV file.

    Parameters:
    df (pd.DataFrame): DataFrame to save.
    path (str): Path to save the CSV file.

    Raises:
    Exception: If there is an error saving the DataFrame.
    """
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        df.to_csv(path, index=False)
    except Exception as e:
        print(f"Error saving DataFrame: {e}")
        raise













# # src/utils/data_loading_utils.py

# import os
# import pandas as pd
# from tqdm.notebook import tqdm
# from src.config_loader import load_paths
# from src.utils.file_operations import initialize_json_files

# def diagnose_csv_issues(file_path, expected_columns=83):
#     """
#     Diagnoses issues in a CSV file by identifying problematic lines.

#     Parameters:
#     file_path (str): Path to the CSV file.
#     expected_columns (int): Expected number of columns in the CSV file.

#     Returns:
#     None
#     """
#     problematic_lines = []
#     with open(file_path, 'r') as file:
#         for i, line in enumerate(file):
#             if line.count(',') != expected_columns - 1:
#                 problematic_lines.append((i + 1, line))

#     if problematic_lines:
#         print("Problematic lines found:")
#         for line_num, line in problematic_lines:
#             print(f"Line {line_num}: {line}")
#     else:
#         print("No problematic lines found.")

# def read_csv_with_progress(file_path, chunksize=10000, sample_size=None, expected_columns=83):
#     """
#     Reads a CSV file with a progress bar.

#     Parameters:
#     file_path (str): Path to the CSV file.
#     chunksize (int): Number of rows per chunk to read.
#     sample_size (int, optional): Number of rows to sample. Defaults to None.
#     expected_columns (int): Expected number of columns in the CSV file.

#     Returns:
#     pd.DataFrame: The loaded DataFrame.
    
#     Raises:
#     Exception: If there is an error reading the CSV file.
#     """
#     try:
#         diagnose_csv_issues(file_path, expected_columns)
        
#         total_lines = sum(1 for _ in open(file_path)) - 1  # Calculate total lines in the file
#         print(f"Total lines: {total_lines}")

#         if sample_size:
#             # Load a sample of the data
#             chunks = []
#             for chunk in tqdm(pd.read_csv(file_path, chunksize=chunksize, low_memory=False), desc="Loading data"):
#                 chunks.append(chunk)
#                 if len(chunks) * chunksize >= sample_size:
#                     break
#             return pd.concat(chunks, axis=0).head(sample_size)
#         else:
#             # Load the full dataset
#             chunk_list = []
#             for chunk in tqdm(pd.read_csv(file_path, chunksize=chunksize, low_memory=False), total=total_lines // chunksize + 1, desc="Loading data"):
#                 chunk_list.append(chunk)
#             return pd.concat(chunk_list, axis=0)
#     except Exception as e:
#         print(f"Error reading CSV with progress: {e}")
#         raise

# def load_sample_data_and_paths():
#     """
#     Loads sample data and configuration paths.

#     Returns:
#     tuple: A tuple containing train_sample DataFrame, test_sample DataFrame, and paths dictionary.

#     Raises:
#     ValueError: If 'HasDetections' column is missing in train_sample.
#     Exception: If there is an error loading sample data and paths.
#     """
#     try:
#         paths = load_paths()
#         train_sample = read_csv_with_progress(paths['data']['train_sample'], chunksize=10000)
#         test_sample = read_csv_with_progress(paths['data']['test_sample'], chunksize=10000)
#         print(f"Shape of train_sample: {train_sample.shape}")
#         print(f"Shape of test_sample: {test_sample.shape}")
#         if 'HasDetections' not in train_sample.columns:
#             raise ValueError("'HasDetections' column is missing in train_sample.")
#         return train_sample, test_sample, paths
#     except Exception as e:
#         print(f"Error loading sample data and paths: {e}")
#         raise

# def initialize_analysis_results(analysis_results_dir, json_files):
#     """
#     Initialize JSON files for storing analysis results.

#     Parameters:
#     analysis_results_dir (str): Directory to store analysis results.
#     json_files (list): List of JSON files to initialize.

#     Raises:
#     Exception: If there is an error initializing analysis results.
#     """
#     try:
#         os.makedirs(analysis_results_dir, exist_ok=True)
#         initialize_json_files(json_files, analysis_results_dir)
#     except Exception as e:
#         print(f"Error initializing analysis results: {e}")
#         raise

# def save_dataframe_with_progress(df, path, chunk_size=1000):
#     """
#     Saves a DataFrame to a CSV file with a progress bar.

#     Parameters:
#     df (pd.DataFrame): DataFrame to save.
#     path (str): Path to save the CSV file.
#     chunk_size (int): Number of rows per chunk to save. Defaults to 1000.

#     Raises:
#     Exception: If there is an error saving the DataFrame.
#     """
#     try:
#         total_rows = df.shape[0]
#         chunks = [df[i:i + chunk_size] for i in range(0, total_rows, chunk_size)]
        
#         for chunk in tqdm(chunks, desc=f"Saving {os.path.basename(path)}", total=len(chunks)):
#             mode = 'w' if chunk is chunks[0] else 'a'
#             header = True if chunk is chunks[0] else False
#             chunk.to_csv(path, mode=mode, header=header, index=False)
#     except Exception as e:
#         print(f"Error saving DataFrame with progress: {e}")
#         raise

# def load_data_and_initialize_results(paths, json_files):
#     """
#     Load sample data and initialize analysis results.

#     Parameters:
#     paths (dict): Dictionary containing file paths.
#     json_files (list): List of JSON files to initialize.

#     Returns:
#     tuple: A tuple containing train_sample DataFrame, test_sample DataFrame, and paths dictionary.
#     """
#     try:
#         train_sample, test_sample, paths = load_sample_data_and_paths()
#         initialize_analysis_results(paths['reports']['analysis_results'], json_files)
#         return train_sample, test_sample, paths
#     except Exception as e:
#         print(f"Error loading data and initializing results: {e}")
#         raise

















# # src/utils/data_loading_utils.py

# import os
# import pandas as pd
# from src.config_loader import load_paths
# from src.utils.file_operations import initialize_json_files
# from src.utils.csv_utils import read_csv_with_progress
# from src.utils.data_preparation_utils import create_feature_classification_comparison_table



# # def read_csv_with_progress(file_path, chunksize=10000, sample_size=None):
# #     """
# #     Reads a CSV file with a progress bar.

# #     Parameters:
# #     file_path (str): Path to the CSV file.
# #     chunksize (int): Number of rows per chunk to read.
# #     sample_size (int, optional): Number of rows to sample. Defaults to None.

# #     Returns:
# #     pd.DataFrame: The loaded DataFrame.
    
# #     Raises:
# #     Exception: If there is an error reading the CSV file.
# #     """
# #     try:
# #         total_lines = sum(1 for _ in open(file_path)) - 1  # Calculate total lines in the file
# #         print(f"Total lines: {total_lines}")

# #         if sample_size:
# #             # Load a sample of the data
# #             chunks = []
# #             for chunk in tqdm(pd.read_csv(file_path, chunksize=chunksize, low_memory=False), desc="Loading data"):
# #                 chunks.append(chunk)
# #                 if len(chunks) * chunksize >= sample_size:
# #                     break
# #             return pd.concat(chunks, axis=0).head(sample_size)
# #         else:
# #             # Load the full dataset
# #             chunk_list = []
# #             for chunk in tqdm(pd.read_csv(file_path, chunksize=chunksize, low_memory=False), total=total_lines // chunksize + 1, desc="Loading data"):
# #                 chunk_list.append(chunk)
# #             return pd.concat(chunk_list, axis=0)
# #     except Exception as e:
# #         print(f"Error reading CSV with progress: {e}")
# #         raise

# def load_sample_data_and_paths():
#     """
#     Loads sample data and configuration paths.

#     Returns:
#     tuple: A tuple containing train_sample DataFrame, test_sample DataFrame, and paths dictionary.

#     Raises:
#     ValueError: If 'HasDetections' column is missing in train_sample.
#     Exception: If there is an error loading sample data and paths.
#     """
#     try:
#         paths = load_paths()
#         train_sample = read_csv_with_progress(paths['data']['train_sample'], chunksize=10000)
#         test_sample = read_csv_with_progress(paths['data']['test_sample'], chunksize=10000)
#         print(f"Shape of train_sample: {train_sample.shape}")
#         print(f"Shape of test_sample: {test_sample.shape}")
#         if 'HasDetections' not in train_sample.columns:
#             raise ValueError("'HasDetections' column is missing in train_sample.")
#         return train_sample, test_sample, paths
#     except Exception as e:
#         print(f"Error loading sample data and paths: {e}")
#         raise

# def initialize_analysis_results(analysis_results_dir, json_files):
#     """
#     Initialize JSON files for storing analysis results.

#     Parameters:
#     analysis_results_dir (str): Directory to store analysis results.
#     json_files (list): List of JSON files to initialize.

#     Raises:
#     Exception: If there is an error initializing analysis results.
#     """
#     try:
#         os.makedirs(analysis_results_dir, exist_ok=True)
#         initialize_json_files(json_files, analysis_results_dir)
#     except Exception as e:
#         print(f"Error initializing analysis results: {e}")
#         raise

# def save_dataframe_with_progress(df, path, chunk_size=1000):
#     """
#     Saves a DataFrame to a CSV file with a progress bar.

#     Parameters:
#     df (pd.DataFrame): DataFrame to save.
#     path (str): Path to save the CSV file.
#     chunk_size (int): Number of rows per chunk to save. Defaults to 1000.

#     Raises:
#     Exception: If there is an error saving the DataFrame.
#     """
#     try:
#         total_rows = df.shape[0]
#         chunks = [df[i:i + chunk_size] for i in range(0, total_rows, chunk_size)]
        
#         for chunk in tqdm(chunks, desc=f"Saving {os.path.basename(path)}", total=len(chunks)):
#             mode = 'w' if chunk is chunks[0] else 'a'
#             header = True if chunk is chunks[0] else False
#             chunk.to_csv(path, mode=mode, header=header, index=False)
#     except Exception as e:
#         print(f"Error saving DataFrame with progress: {e}")
#         raise

# def load_data_and_initialize_results(paths, json_files):
#     """
#     Load sample data and initialize analysis results.

#     Parameters:
#     paths (dict): Dictionary containing file paths.
#     json_files (list): List of JSON files to initialize.

#     Returns:
#     tuple: A tuple containing train_sample DataFrame, test_sample DataFrame, paths dictionary, and comparison table.
#     """
#     try:
#         train_sample, test_sample, paths = load_sample_data_and_paths()
#         initialize_analysis_results(paths['reports']['analysis_results'], json_files)
        
#         # Create comparison table
#         comparison_table = create_feature_classification_comparison_table(train_sample, paths['config']['feature_metadata'])
        
#         return train_sample, test_sample, paths, comparison_table
#     except Exception as e:
#         print(f"Error loading data and initializing results: {e}")
#         raise







# # def save_dataframe(df, path):
# #     """
# #     Saves a DataFrame to a CSV file.

# #     Parameters:
# #     df (pd.DataFrame): DataFrame to save.
# #     path (str): Path to save the CSV file.

# #     Raises:
# #     Exception: If there is an error saving the DataFrame.
# #     """
# #     try:
# #         os.makedirs(os.path.dirname(path), exist_ok=True)
# #         df.to_csv(path, index=False)
# #     except Exception as e:
# #         print(f"Error saving DataFrame: {e}")
# #         raise


















# # def load_sample_data_and_paths():
# #     """
# #     Loads sample data and configuration paths.

# #     Returns:
# #     tuple: A tuple containing train_sample DataFrame, test_sample DataFrame, and paths dictionary.

# #     Raises:
# #     ValueError: If 'HasDetections' column is missing in train_sample.
# #     """
# #     paths = load_paths()
# #     train_sample = read_csv_with_progress(paths['data']['train_sample'], chunksize=10000)
# #     test_sample = read_csv_with_progress(paths['data']['test_sample'], chunksize=10000)
# #     print(f"Shape of train_sample: {train_sample.shape}")
# #     print(f"Shape of test_sample: {test_sample.shape}")
# #     if 'HasDetections' not in train_sample.columns:
# #         raise ValueError("'HasDetections' column is missing in train_sample.")
# #     return train_sample, test_sample, paths

# # def initialize_analysis_results(analysis_results_dir, json_files):
# #     """
# #     Initialize JSON files for storing analysis results.

# #     Parameters:
# #     analysis_results_dir (str): Directory to store analysis results.
# #     json_files (list): List of JSON files to initialize.
# #     """
# #     try:
# #         os.makedirs(analysis_results_dir, exist_ok=True)
# #         initialize_json_files(json_files, analysis_results_dir)
# #     except Exception as e:
# #         print(f"Error initializing analysis results: {e}")
# #         raise









# # def read_csv_with_progress(file_path, chunksize=10000, sample_size=None):
# #     """
# #     Reads a CSV file with a progress bar. Optionally reads a sample of the data.

# #     Parameters:
# #     file_path (str): Path to the CSV file.
# #     chunksize (int): Number of rows per chunk to read. Default is 10000.
# #     sample_size (int): Number of rows to read for the sample. If None, reads the full dataset. Default is None.

# #     Returns:
# #     pd.DataFrame: DataFrame containing the loaded data.
# #     """
# #     total_lines = sum(1 for _ in open(file_path)) - 1  # Calculate total lines in the file
# #     print(f"Total lines: {total_lines}")
    
# #     if sample_size:
# #         # Load a sample of the data
# #         chunks = []
# #         for chunk in tqdm(pd.read_csv(file_path, chunksize=chunksize, low_memory=False), desc="Loading data"):
# #             chunks.append(chunk)
# #             if len(chunks) * chunksize >= sample_size:
# #                 break
# #         return pd.concat(chunks, axis=0).head(sample_size)
# #     else:
# #         # Load the full dataset
# #         chunk_list = []
# #         for chunk in tqdm(pd.read_csv(file_path, chunksize=chunksize, low_memory=False), total=total_lines // chunksize + 1, desc="Loading data"):
# #             chunk_list.append(chunk)
# #         return pd.concat(chunk_list, axis=0)




# # # src/utils/data_loading_utils.py

# # import os
# # import pandas as pd
# # from tqdm.notebook import tqdm
# # from src.config_loader import load_paths

# # def read_csv_with_progress(file_path, chunksize=10000, sample_size=None):
# #     total_lines = sum(1 for _ in open(file_path)) - 1  # Calculate total lines in the file
# #     print(f"Total lines: {total_lines}")
    
# #     if sample_size:
# #         # Load a sample of the data
# #         chunks = []
# #         for chunk in tqdm(pd.read_csv(file_path, chunksize=chunksize, low_memory=False), desc="Loading data"):
# #             chunks.append(chunk)
# #             if len(chunks) * chunksize >= sample_size:
# #                 break
# #         return pd.concat(chunks, axis=0).head(sample_size)
# #     else:
# #         # Load the full dataset
# #         chunk_list = []
# #         for chunk in tqdm(pd.read_csv(file_path, chunksize=chunksize, low_memory=False), total=total_lines // chunksize + 1, desc="Loading data"):
# #             chunk_list.append(chunk)
# #         return pd.concat(chunk_list, axis=0)

# # def save_dataframe_with_progress(df, path):
# #     total_rows = df.shape[0]
# #     chunk_size = 1000  # Adjust as needed for performance
# #     chunks = [df[i:i + chunk_size] for i in range(0, total_rows, chunk_size)]
    
# #     for chunk in tqdm(chunks, desc=f"Saving {os.path.basename(path)}", total=len(chunks)):
# #         mode = 'w' if chunk is chunks[0] else 'a'
# #         header = True if chunk is chunks[0] else False
# #         chunk.to_csv(path, mode=mode, header=header, index=False)

# # def save_dataframe(df, path):
# #     os.makedirs(os.path.dirname(path), exist_ok=True)
# #     df.to_csv(path, index=False)

# # def load_sample_data_and_paths():
# #     paths = load_paths()
# #     train_sample = read_csv_with_progress(paths['data']['train_sample'], chunksize=10000)
# #     test_sample = read_csv_with_progress(paths['data']['test_sample'], chunksize=10000)
# #     print(f"Shape of train_sample: {train_sample.shape}")
# #     print(f"Shape of test_sample: {test_sample.shape}")
# #     if 'HasDetections' not in train_sample.columns:
# #         raise ValueError("'HasDetections' column is missing in train_sample.")
# #     return train_sample, test_sample, paths

