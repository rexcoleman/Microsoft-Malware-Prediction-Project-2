{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16771ac2-45b4-4c74-8c8b-6100cd5b354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e795b4-fcb0-4ebd-9421-a75e87daaaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 19:50:57,135 - INFO - Project root set to: /Users/rexcoleman/Documents/DataScienceAndMachineLearning/Rex_Coleman_Machine_Learning_Cybersecurity_Portfolio/Microsoft-Malware-Prediction-Project-2\n",
      "2024-08-18 19:50:57,161 - INFO - Project root set to: /Users/rexcoleman/Documents/DataScienceAndMachineLearning/Rex_Coleman_Machine_Learning_Cybersecurity_Portfolio/Microsoft-Malware-Prediction-Project-2\n",
      "2024-08-18 19:50:57,169 - INFO - Metadata loaded from config/feature_metadata.json\n",
      "2024-08-18 19:50:57,170 - ERROR - Error in main function: load_json_file() takes 1 positional argument but 2 were given\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_json_file() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 227\u001b[0m\n\u001b[1;32m    224\u001b[0m metadata_path \u001b[38;5;241m=\u001b[39m paths[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    225\u001b[0m reports_dir \u001b[38;5;241m=\u001b[39m paths[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreports\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manalysis_results\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 227\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreports_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 196\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(train_data_path, test_data_path, train_save_path, test_save_path, metadata_path, reports_dir)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetadata should be a dictionary, not a string.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Convert data types\u001b[39;00m\n\u001b[1;32m    199\u001b[0m train_data \u001b[38;5;241m=\u001b[39m convert_data_types(train_data, metadata)\n",
      "\u001b[0;31mTypeError\u001b[0m: load_json_file() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# notebooks/02_initial_data_preparation/01_imputation_and_data_types/01d_convert_data_types.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def find_project_root(marker_file: str = 'src/config_loader.py') -> str:\n",
    "    \"\"\"\n",
    "    Locate the project root using the specified marker file.\n",
    "\n",
    "    Parameters:\n",
    "    marker_file (str): The marker file to identify the project root.\n",
    "\n",
    "    Returns:\n",
    "    str: The path to the project root directory.\n",
    "    \"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    while current_dir != os.path.dirname(current_dir):\n",
    "        if os.path.isfile(os.path.join(current_dir, marker_file)):\n",
    "            return current_dir\n",
    "        current_dir = os.path.dirname(current_dir)\n",
    "    raise FileNotFoundError(f\"Marker file '{marker_file}' not found in any parent directories.\")\n",
    "\n",
    "def set_project_root() -> str:\n",
    "    \"\"\"Set the project root directory.\"\"\"\n",
    "    project_root = find_project_root()\n",
    "    os.chdir(project_root)\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.append(project_root)\n",
    "    logging.info(f\"Project root set to: {project_root}\")\n",
    "    return project_root\n",
    "\n",
    "# Find and set the project root directory\n",
    "project_root = set_project_root()\n",
    "\n",
    "# Import setup function from custom module\n",
    "from src.utils.environment_setup import setup_project_environment\n",
    "from src.utils.initialization import load_train_and_test_data\n",
    "from src.utils.metadata_operations import extract_classified_data_type, load_json_file\n",
    "from src.utils.json_pipeline import save_json_with_pipeline\n",
    "from src.utils.file_operations import save_dataframe_with_progress\n",
    "\n",
    "# Set up the project environment\n",
    "paths, directories = setup_project_environment()\n",
    "\n",
    "def load_data(train_data_path: str, test_data_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load the train and test datasets.\n",
    "\n",
    "    Parameters:\n",
    "    train_data_path (str): Path to the training data.\n",
    "    test_data_path (str): Path to the test data.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]: The loaded training and test datasets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        train_data = pd.read_csv(train_data_path)\n",
    "        test_data = pd.read_csv(test_data_path)\n",
    "        logging.info(f\"Loaded train data from {train_data_path}\")\n",
    "        logging.info(f\"Loaded test data from {test_data_path}\")\n",
    "        return train_data, test_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_type_mapping() -> dict:\n",
    "    \"\"\"\n",
    "    Returns a dictionary that maps classified data types to their corresponding technical data types.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary mapping classified data types to technical data types.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'categorical': 'category',\n",
    "        'numerical': 'float64',\n",
    "        'binary': 'bool'\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_data_types(df: pd.DataFrame, metadata: Dict[str, Dict[str, str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert the data types of DataFrame columns based on the classified data type in the metadata.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    metadata (Dict[str, Dict[str, str]]): The metadata dictionary containing the classified data type for each feature.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with converted data types.\n",
    "    \"\"\"\n",
    "    type_mapping = get_type_mapping()\n",
    "\n",
    "    try:\n",
    "        for column in tqdm(df.columns, desc=\"Converting data types\"):\n",
    "            classified_data_type = extract_classified_data_type(metadata, column)\n",
    "            if classified_data_type in type_mapping:\n",
    "                target_dtype = type_mapping[classified_data_type]\n",
    "                try:\n",
    "                    df[column] = df[column].astype(target_dtype)\n",
    "                    logging.info(f\"Converted '{column}' to {target_dtype}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error converting column '{column}' to {target_dtype}: {e}\")\n",
    "            else:\n",
    "                logging.warning(f\"Classified data type '{classified_data_type}' for column '{column}' not recognized.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error converting data types: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def validate_data_types(df: pd.DataFrame, metadata: Dict[str, Dict[str, str]]) -> bool:\n",
    "    \"\"\"\n",
    "    Validate that the DataFrame columns have the correct data types according to the metadata.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to validate.\n",
    "    metadata (Dict[str, Dict[str, str]]): The metadata dictionary containing the classified data type for each feature.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if all data types are as expected, otherwise raises a ValueError.\n",
    "    \"\"\"\n",
    "    type_mapping = get_type_mapping()\n",
    "\n",
    "    try:\n",
    "        for column in df.columns:\n",
    "            classified_data_type = extract_classified_data_type(metadata, column)\n",
    "            if classified_data_type in type_mapping:\n",
    "                expected_dtype = type_mapping[classified_data_type]\n",
    "                actual_dtype = str(df[column].dtype)\n",
    "                if actual_dtype != expected_dtype:\n",
    "                    logging.error(f\"Column '{column}' has dtype '{actual_dtype}', expected '{expected_dtype}'.\")\n",
    "                    raise ValueError(f\"Data type mismatch for column '{column}': expected {expected_dtype}, got {actual_dtype}.\")\n",
    "            else:\n",
    "                logging.warning(f\"Classified data type '{classified_data_type}' for column '{column}' not recognized.\")\n",
    "        logging.info(\"All data types validated successfully.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error validating data types: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def create_feature_data_types_report(df: pd.DataFrame, save_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a JSON report of the data types for each feature in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    save_dir (str): The directory where the JSON report should be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        feature_data_types = {\n",
    "            \"features\": {\n",
    "                column: {\n",
    "                    \"general_attributes\": {\n",
    "                        \"technical_data_type\": str(df[column].dtype)\n",
    "                    }\n",
    "                }\n",
    "                for column in df.columns\n",
    "            }\n",
    "        }\n",
    "\n",
    "        save_path = os.path.join(save_dir, 'feature_data_types.json')\n",
    "        save_json_with_pipeline(feature_data_types, save_path)\n",
    "        logging.info(f\"Feature data types report saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating feature data types report: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def main(train_data_path: str, test_data_path: str, train_save_path: str, test_save_path: str, metadata_path: str, reports_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the loading, processing, validation, and saving of data with converted data types.\n",
    "\n",
    "    Parameters:\n",
    "    train_data_path (str): Path to the training data.\n",
    "    test_data_path (str): Path to the test data.\n",
    "    train_save_path (str): Path to save the processed training data.\n",
    "    test_save_path (str): Path to save the processed test data.\n",
    "    metadata_path (str): Path to the metadata JSON file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load metadata\n",
    "        metadata = load_json_file(metadata_path)\n",
    "        if not isinstance(metadata, dict):\n",
    "            logging.error(\"Expected metadata to be a dictionary, but got a different type.\")\n",
    "            raise TypeError(\"Metadata should be a dictionary, not a string.\")\n",
    "\n",
    "        # Load data\n",
    "        # train_data, test_data = load_json_file(train_data_path, test_data_path)\n",
    "        train_data, test_data = load_train_and_test_data(train_data_path, test_data_path, paths)\n",
    "\n",
    "        # Convert data types\n",
    "        train_data = convert_data_types(train_data, metadata)\n",
    "        test_data = convert_data_types(test_data, metadata)\n",
    "\n",
    "        # Validate data types\n",
    "        validate_data_types(train_data, metadata)\n",
    "        validate_data_types(test_data, metadata)\n",
    "\n",
    "        # Save processed data\n",
    "        save_dataframe_with_progress(train_data, train_save_path)\n",
    "        save_dataframe_with_progress(test_data, test_save_path)\n",
    "                \n",
    "        # Create and save feature data types report\n",
    "        create_feature_data_types_report(train_data, reports_dir)\n",
    "\n",
    "        logging.info(\"Data type conversion and validation completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main function: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths (replace with actual paths from your environment setup)\n",
    "    train_data_path = os.path.join(paths['data']['intermediate_train_drop_high_missing_value_features'])\n",
    "    test_data_path = os.path.join(paths['data']['intermediate_test_drop_high_missing_value_features'])\n",
    "    train_save_path = os.path.join(paths['data']['intermediate_train_convert_data_types'])\n",
    "    test_save_path = os.path.join(paths['data']['intermediate_test_convert_data_types'])\n",
    "    metadata_path = paths['config']['feature_metadata']\n",
    "    reports_dir = paths['reports']['analysis_results']\n",
    "\n",
    "    main(train_data_path, test_data_path, train_save_path, test_save_path, metadata_path, reports_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f8c7dd-9b39-4b4d-a3e6-70397aaf44a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6696db-e106-497e-896b-e2a18f8e5aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767cba20-235d-48d5-8dd1-72bd9c412f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79401b4-6351-4ae5-bf09-d702fd34ff31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113e9dd2-69b4-4e3b-9d16-025d015da241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 03:38:25,714 - INFO - Project root set to: /Users/rexcoleman/Documents/DataScienceAndMachineLearning/Rex_Coleman_Machine_Learning_Cybersecurity_Portfolio/Microsoft-Malware-Prediction-Project-2\n",
      "2024-08-18 03:38:25,730 - INFO - Project root set to: /Users/rexcoleman/Documents/DataScienceAndMachineLearning/Rex_Coleman_Machine_Learning_Cybersecurity_Portfolio/Microsoft-Malware-Prediction-Project-2\n",
      "2024-08-18 03:38:25,738 - INFO - Metadata loaded from config/feature_metadata.json\n",
      "2024-08-18 03:38:25,845 - INFO - Loaded train data from data/intermediate/intermediate_01_train_missing_values_indicators_added.csv\n",
      "2024-08-18 03:38:25,846 - INFO - Loaded test data from data/intermediate/intermediate_01_test_missing_values_indicators_added.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8c649b96ad47ebb975e1f8a99984fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting data types:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 03:38:25,872 - INFO - Converted 'MachineIdentifier' to category\n",
      "2024-08-18 03:38:25,874 - INFO - Converted 'ProductName' to category\n",
      "2024-08-18 03:38:25,875 - INFO - Converted 'EngineVersion' to category\n",
      "2024-08-18 03:38:25,876 - INFO - Converted 'AppVersion' to category\n",
      "2024-08-18 03:38:25,878 - INFO - Converted 'AvSigVersion' to category\n",
      "2024-08-18 03:38:25,879 - INFO - Converted 'IsBeta' to bool\n",
      "2024-08-18 03:38:25,880 - INFO - Converted 'RtpStateBitfield' to category\n",
      "2024-08-18 03:38:25,881 - INFO - Converted 'IsSxsPassiveMode' to bool\n",
      "2024-08-18 03:38:25,882 - INFO - Converted 'AVProductStatesIdentifier' to category\n",
      "2024-08-18 03:38:25,883 - INFO - Converted 'AVProductsInstalled' to float64\n",
      "2024-08-18 03:38:25,883 - INFO - Converted 'AVProductsEnabled' to float64\n",
      "2024-08-18 03:38:25,884 - INFO - Converted 'HasTpm' to bool\n",
      "2024-08-18 03:38:25,885 - INFO - Converted 'CountryIdentifier' to category\n",
      "2024-08-18 03:38:25,887 - INFO - Converted 'CityIdentifier' to category\n",
      "2024-08-18 03:38:25,888 - INFO - Converted 'OrganizationIdentifier' to category\n",
      "2024-08-18 03:38:25,889 - INFO - Converted 'GeoNameIdentifier' to category\n",
      "2024-08-18 03:38:25,890 - INFO - Converted 'LocaleEnglishNameIdentifier' to category\n",
      "2024-08-18 03:38:25,891 - INFO - Converted 'Platform' to category\n",
      "2024-08-18 03:38:25,892 - INFO - Converted 'Processor' to category\n",
      "2024-08-18 03:38:25,893 - INFO - Converted 'OsVer' to category\n",
      "2024-08-18 03:38:25,893 - INFO - Converted 'OsBuild' to category\n",
      "2024-08-18 03:38:25,894 - INFO - Converted 'OsSuite' to category\n",
      "2024-08-18 03:38:25,896 - INFO - Converted 'OsPlatformSubRelease' to category\n",
      "2024-08-18 03:38:25,897 - INFO - Converted 'OsBuildLab' to category\n",
      "2024-08-18 03:38:25,898 - INFO - Converted 'SkuEdition' to category\n",
      "2024-08-18 03:38:25,899 - INFO - Converted 'IsProtected' to bool\n",
      "2024-08-18 03:38:25,899 - INFO - Converted 'AutoSampleOptIn' to bool\n",
      "2024-08-18 03:38:25,900 - INFO - Converted 'SMode' to bool\n",
      "2024-08-18 03:38:25,901 - INFO - Converted 'IeVerIdentifier' to category\n",
      "2024-08-18 03:38:25,902 - INFO - Converted 'SmartScreen' to category\n",
      "2024-08-18 03:38:25,903 - INFO - Converted 'Firewall' to bool\n",
      "2024-08-18 03:38:25,903 - INFO - Converted 'UacLuaenable' to bool\n",
      "2024-08-18 03:38:25,904 - INFO - Converted 'Census_MDC2FormFactor' to category\n",
      "2024-08-18 03:38:25,905 - INFO - Converted 'Census_DeviceFamily' to category\n",
      "2024-08-18 03:38:25,906 - INFO - Converted 'Census_OEMNameIdentifier' to category\n",
      "2024-08-18 03:38:25,908 - INFO - Converted 'Census_OEMModelIdentifier' to category\n",
      "2024-08-18 03:38:25,908 - INFO - Converted 'Census_ProcessorCoreCount' to float64\n",
      "2024-08-18 03:38:25,909 - INFO - Converted 'Census_ProcessorManufacturerIdentifier' to category\n",
      "2024-08-18 03:38:25,910 - INFO - Converted 'Census_ProcessorModelIdentifier' to category\n",
      "2024-08-18 03:38:25,911 - INFO - Converted 'Census_PrimaryDiskTotalCapacity' to float64\n",
      "2024-08-18 03:38:25,911 - INFO - Converted 'Census_PrimaryDiskTypeName' to category\n",
      "2024-08-18 03:38:25,912 - INFO - Converted 'Census_SystemVolumeTotalCapacity' to float64\n",
      "2024-08-18 03:38:25,913 - INFO - Converted 'Census_HasOpticalDiskDrive' to bool\n",
      "2024-08-18 03:38:25,914 - INFO - Converted 'Census_TotalPhysicalRAM' to float64\n",
      "2024-08-18 03:38:25,915 - INFO - Converted 'Census_ChassisTypeName' to category\n",
      "2024-08-18 03:38:25,915 - INFO - Converted 'Census_InternalPrimaryDiagonalDisplaySizeInInches' to float64\n",
      "2024-08-18 03:38:25,915 - INFO - Converted 'Census_InternalPrimaryDisplayResolutionHorizontal' to float64\n",
      "2024-08-18 03:38:25,916 - INFO - Converted 'Census_InternalPrimaryDisplayResolutionVertical' to float64\n",
      "2024-08-18 03:38:25,917 - INFO - Converted 'Census_PowerPlatformRoleName' to category\n",
      "2024-08-18 03:38:25,918 - INFO - Converted 'Census_InternalBatteryNumberOfCharges' to float64\n",
      "2024-08-18 03:38:25,919 - INFO - Converted 'Census_OSVersion' to category\n",
      "2024-08-18 03:38:25,920 - INFO - Converted 'Census_OSArchitecture' to category\n",
      "2024-08-18 03:38:25,921 - INFO - Converted 'Census_OSBranch' to category\n",
      "2024-08-18 03:38:25,923 - INFO - Converted 'Census_OSBuildNumber' to category\n",
      "2024-08-18 03:38:25,943 - INFO - Converted 'Census_OSBuildRevision' to category\n",
      "2024-08-18 03:38:25,964 - INFO - Converted 'Census_OSEdition' to category\n",
      "2024-08-18 03:38:25,965 - INFO - Converted 'Census_OSSkuName' to category\n",
      "2024-08-18 03:38:25,966 - INFO - Converted 'Census_OSInstallTypeName' to category\n",
      "2024-08-18 03:38:25,967 - INFO - Converted 'Census_OSInstallLanguageIdentifier' to category\n",
      "2024-08-18 03:38:25,968 - INFO - Converted 'Census_OSUILocaleIdentifier' to category\n",
      "2024-08-18 03:38:25,970 - INFO - Converted 'Census_OSWUAutoUpdateOptionsName' to category\n",
      "2024-08-18 03:38:25,971 - INFO - Converted 'Census_IsPortableOperatingSystem' to bool\n",
      "2024-08-18 03:38:25,974 - INFO - Converted 'Census_GenuineStateName' to category\n",
      "2024-08-18 03:38:25,978 - INFO - Converted 'Census_ActivationChannel' to category\n",
      "2024-08-18 03:38:25,979 - INFO - Converted 'Census_IsFlightsDisabled' to bool\n",
      "2024-08-18 03:38:25,980 - INFO - Converted 'Census_FlightRing' to category\n",
      "2024-08-18 03:38:25,981 - INFO - Converted 'Census_FirmwareManufacturerIdentifier' to category\n",
      "2024-08-18 03:38:25,983 - INFO - Converted 'Census_FirmwareVersionIdentifier' to category\n",
      "2024-08-18 03:38:25,983 - INFO - Converted 'Census_IsSecureBootEnabled' to bool\n",
      "2024-08-18 03:38:25,984 - INFO - Converted 'Census_IsVirtualDevice' to bool\n",
      "2024-08-18 03:38:25,984 - INFO - Converted 'Census_IsTouchEnabled' to bool\n",
      "2024-08-18 03:38:25,985 - INFO - Converted 'Census_IsPenCapable' to bool\n",
      "2024-08-18 03:38:25,986 - INFO - Converted 'Census_IsAlwaysOnAlwaysConnectedCapable' to bool\n",
      "2024-08-18 03:38:25,986 - INFO - Converted 'Wdft_IsGamer' to bool\n",
      "2024-08-18 03:38:25,987 - INFO - Converted 'Wdft_RegionIdentifier' to category\n",
      "2024-08-18 03:38:25,987 - INFO - Converted 'HasDetections' to bool\n",
      "2024-08-18 03:38:25,988 - INFO - Converted 'OrganizationIdentifier_is_missing' to bool\n",
      "2024-08-18 03:38:25,989 - INFO - Converted 'SMode_is_missing' to bool\n",
      "2024-08-18 03:38:25,989 - INFO - Converted 'SmartScreen_is_missing' to bool\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f709c70ba0474611ab0f6491b39d2912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting data types:   0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 03:38:26,000 - INFO - Converted 'MachineIdentifier' to category\n",
      "2024-08-18 03:38:26,002 - INFO - Converted 'ProductName' to category\n",
      "2024-08-18 03:38:26,003 - INFO - Converted 'EngineVersion' to category\n",
      "2024-08-18 03:38:26,004 - INFO - Converted 'AppVersion' to category\n",
      "2024-08-18 03:38:26,006 - INFO - Converted 'AvSigVersion' to category\n",
      "2024-08-18 03:38:26,006 - INFO - Converted 'IsBeta' to bool\n",
      "2024-08-18 03:38:26,007 - INFO - Converted 'RtpStateBitfield' to category\n",
      "2024-08-18 03:38:26,009 - INFO - Converted 'IsSxsPassiveMode' to bool\n",
      "2024-08-18 03:38:26,010 - INFO - Converted 'AVProductStatesIdentifier' to category\n",
      "2024-08-18 03:38:26,011 - INFO - Converted 'AVProductsInstalled' to float64\n",
      "2024-08-18 03:38:26,011 - INFO - Converted 'AVProductsEnabled' to float64\n",
      "2024-08-18 03:38:26,012 - INFO - Converted 'HasTpm' to bool\n",
      "2024-08-18 03:38:26,013 - INFO - Converted 'CountryIdentifier' to category\n",
      "2024-08-18 03:38:26,015 - INFO - Converted 'CityIdentifier' to category\n",
      "2024-08-18 03:38:26,016 - INFO - Converted 'OrganizationIdentifier' to category\n",
      "2024-08-18 03:38:26,017 - INFO - Converted 'GeoNameIdentifier' to category\n",
      "2024-08-18 03:38:26,017 - INFO - Converted 'LocaleEnglishNameIdentifier' to category\n",
      "2024-08-18 03:38:26,018 - INFO - Converted 'Platform' to category\n",
      "2024-08-18 03:38:26,019 - INFO - Converted 'Processor' to category\n",
      "2024-08-18 03:38:26,020 - INFO - Converted 'OsVer' to category\n",
      "2024-08-18 03:38:26,021 - INFO - Converted 'OsBuild' to category\n",
      "2024-08-18 03:38:26,022 - INFO - Converted 'OsSuite' to category\n",
      "2024-08-18 03:38:26,023 - INFO - Converted 'OsPlatformSubRelease' to category\n",
      "2024-08-18 03:38:26,026 - INFO - Converted 'OsBuildLab' to category\n",
      "2024-08-18 03:38:26,027 - INFO - Converted 'SkuEdition' to category\n",
      "2024-08-18 03:38:26,027 - INFO - Converted 'IsProtected' to bool\n",
      "2024-08-18 03:38:26,027 - INFO - Converted 'AutoSampleOptIn' to bool\n",
      "2024-08-18 03:38:26,028 - INFO - Converted 'SMode' to bool\n",
      "2024-08-18 03:38:26,029 - INFO - Converted 'IeVerIdentifier' to category\n",
      "2024-08-18 03:38:26,030 - INFO - Converted 'SmartScreen' to category\n",
      "2024-08-18 03:38:26,030 - INFO - Converted 'Firewall' to bool\n",
      "2024-08-18 03:38:26,031 - INFO - Converted 'UacLuaenable' to bool\n",
      "2024-08-18 03:38:26,032 - INFO - Converted 'Census_MDC2FormFactor' to category\n",
      "2024-08-18 03:38:26,033 - INFO - Converted 'Census_DeviceFamily' to category\n",
      "2024-08-18 03:38:26,034 - INFO - Converted 'Census_OEMNameIdentifier' to category\n",
      "2024-08-18 03:38:26,036 - INFO - Converted 'Census_OEMModelIdentifier' to category\n",
      "2024-08-18 03:38:26,037 - INFO - Converted 'Census_ProcessorCoreCount' to float64\n",
      "2024-08-18 03:38:26,038 - INFO - Converted 'Census_ProcessorManufacturerIdentifier' to category\n",
      "2024-08-18 03:38:26,039 - INFO - Converted 'Census_ProcessorModelIdentifier' to category\n",
      "2024-08-18 03:38:26,039 - INFO - Converted 'Census_PrimaryDiskTotalCapacity' to float64\n",
      "2024-08-18 03:38:26,040 - INFO - Converted 'Census_PrimaryDiskTypeName' to category\n",
      "2024-08-18 03:38:26,041 - INFO - Converted 'Census_SystemVolumeTotalCapacity' to float64\n",
      "2024-08-18 03:38:26,042 - INFO - Converted 'Census_HasOpticalDiskDrive' to bool\n",
      "2024-08-18 03:38:26,042 - INFO - Converted 'Census_TotalPhysicalRAM' to float64\n",
      "2024-08-18 03:38:26,043 - INFO - Converted 'Census_ChassisTypeName' to category\n",
      "2024-08-18 03:38:26,044 - INFO - Converted 'Census_InternalPrimaryDiagonalDisplaySizeInInches' to float64\n",
      "2024-08-18 03:38:26,044 - INFO - Converted 'Census_InternalPrimaryDisplayResolutionHorizontal' to float64\n",
      "2024-08-18 03:38:26,045 - INFO - Converted 'Census_InternalPrimaryDisplayResolutionVertical' to float64\n",
      "2024-08-18 03:38:26,046 - INFO - Converted 'Census_PowerPlatformRoleName' to category\n",
      "2024-08-18 03:38:26,046 - INFO - Converted 'Census_InternalBatteryNumberOfCharges' to float64\n",
      "2024-08-18 03:38:26,048 - INFO - Converted 'Census_OSVersion' to category\n",
      "2024-08-18 03:38:26,049 - INFO - Converted 'Census_OSArchitecture' to category\n",
      "2024-08-18 03:38:26,050 - INFO - Converted 'Census_OSBranch' to category\n",
      "2024-08-18 03:38:26,051 - INFO - Converted 'Census_OSBuildNumber' to category\n",
      "2024-08-18 03:38:26,052 - INFO - Converted 'Census_OSBuildRevision' to category\n",
      "2024-08-18 03:38:26,053 - INFO - Converted 'Census_OSEdition' to category\n",
      "2024-08-18 03:38:26,055 - INFO - Converted 'Census_OSSkuName' to category\n",
      "2024-08-18 03:38:26,056 - INFO - Converted 'Census_OSInstallTypeName' to category\n",
      "2024-08-18 03:38:26,057 - INFO - Converted 'Census_OSInstallLanguageIdentifier' to category\n",
      "2024-08-18 03:38:26,057 - INFO - Converted 'Census_OSUILocaleIdentifier' to category\n",
      "2024-08-18 03:38:26,058 - INFO - Converted 'Census_OSWUAutoUpdateOptionsName' to category\n",
      "2024-08-18 03:38:26,059 - INFO - Converted 'Census_IsPortableOperatingSystem' to bool\n",
      "2024-08-18 03:38:26,060 - INFO - Converted 'Census_GenuineStateName' to category\n",
      "2024-08-18 03:38:26,061 - INFO - Converted 'Census_ActivationChannel' to category\n",
      "2024-08-18 03:38:26,061 - INFO - Converted 'Census_IsFlightsDisabled' to bool\n",
      "2024-08-18 03:38:26,063 - INFO - Converted 'Census_FlightRing' to category\n",
      "2024-08-18 03:38:26,064 - INFO - Converted 'Census_FirmwareManufacturerIdentifier' to category\n",
      "2024-08-18 03:38:26,066 - INFO - Converted 'Census_FirmwareVersionIdentifier' to category\n",
      "2024-08-18 03:38:26,066 - INFO - Converted 'Census_IsSecureBootEnabled' to bool\n",
      "2024-08-18 03:38:26,067 - INFO - Converted 'Census_IsVirtualDevice' to bool\n",
      "2024-08-18 03:38:26,068 - INFO - Converted 'Census_IsTouchEnabled' to bool\n",
      "2024-08-18 03:38:26,068 - INFO - Converted 'Census_IsPenCapable' to bool\n",
      "2024-08-18 03:38:26,069 - INFO - Converted 'Census_IsAlwaysOnAlwaysConnectedCapable' to bool\n",
      "2024-08-18 03:38:26,069 - INFO - Converted 'Wdft_IsGamer' to bool\n",
      "2024-08-18 03:38:26,070 - INFO - Converted 'Wdft_RegionIdentifier' to category\n",
      "2024-08-18 03:38:26,071 - INFO - Converted 'OrganizationIdentifier_is_missing' to bool\n",
      "2024-08-18 03:38:26,071 - INFO - Converted 'SMode_is_missing' to bool\n",
      "2024-08-18 03:38:26,072 - INFO - Converted 'SmartScreen_is_missing' to bool\n",
      "2024-08-18 03:38:26,074 - INFO - All data types validated successfully.\n",
      "2024-08-18 03:38:26,076 - INFO - All data types validated successfully.\n",
      "2024-08-18 03:38:26,218 - INFO - Processed data saved to data/intermediate/intermediate_02_train_convert_data_types.csv\n",
      "2024-08-18 03:38:26,363 - INFO - Processed data saved to data/intermediate/intermediate_02_train_convert_data_types.csv\n",
      "2024-08-18 03:38:26,363 - INFO - Data type conversion and validation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# notebooks/02_initial_data_preparation/01_imputation_and_data_types/01b_convert_data_types.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def find_project_root(marker_file: str = 'src/config_loader.py') -> str:\n",
    "    \"\"\"\n",
    "    Locate the project root using the specified marker file.\n",
    "\n",
    "    Parameters:\n",
    "    marker_file (str): The marker file to identify the project root.\n",
    "\n",
    "    Returns:\n",
    "    str: The path to the project root directory.\n",
    "    \"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    while current_dir != os.path.dirname(current_dir):\n",
    "        if os.path.isfile(os.path.join(current_dir, marker_file)):\n",
    "            return current_dir\n",
    "        current_dir = os.path.dirname(current_dir)\n",
    "    raise FileNotFoundError(f\"Marker file '{marker_file}' not found in any parent directories.\")\n",
    "\n",
    "def set_project_root() -> str:\n",
    "    \"\"\"Set the project root directory.\"\"\"\n",
    "    project_root = find_project_root()\n",
    "    os.chdir(project_root)\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.append(project_root)\n",
    "    logging.info(f\"Project root set to: {project_root}\")\n",
    "    return project_root\n",
    "\n",
    "# Find and set the project root directory\n",
    "project_root = set_project_root()\n",
    "\n",
    "# Import setup function from custom module\n",
    "from src.utils.environment_setup import setup_project_environment\n",
    "from src.utils.metadata_operations import extract_classified_data_type, load_json_file\n",
    "\n",
    "# Set up the project environment\n",
    "paths, directories = setup_project_environment()\n",
    "\n",
    "def load_data(train_data_path: str, test_data_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load the train and test datasets.\n",
    "\n",
    "    Parameters:\n",
    "    train_data_path (str): Path to the training data.\n",
    "    test_data_path (str): Path to the test data.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]: The loaded training and test datasets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        train_data = pd.read_csv(train_data_path)\n",
    "        test_data = pd.read_csv(test_data_path)\n",
    "        logging.info(f\"Loaded train data from {train_data_path}\")\n",
    "        logging.info(f\"Loaded test data from {test_data_path}\")\n",
    "        return train_data, test_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def convert_data_types(df: pd.DataFrame, metadata: Dict[str, Dict[str, str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert the data types of DataFrame columns based on the classified data type in the metadata.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    metadata (Dict[str, Dict[str, str]]): The metadata dictionary containing the classified data type for each feature.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with converted data types.\n",
    "    \"\"\"\n",
    "    type_mapping = {\n",
    "        'categorical': 'category',\n",
    "        'numerical': 'float64',\n",
    "        'binary': 'bool'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        for column in tqdm(df.columns, desc=\"Converting data types\"):\n",
    "            classified_data_type = extract_classified_data_type(metadata, column)\n",
    "            if classified_data_type in type_mapping:\n",
    "                target_dtype = type_mapping[classified_data_type]\n",
    "                try:\n",
    "                    df[column] = df[column].astype(target_dtype)\n",
    "                    logging.info(f\"Converted '{column}' to {target_dtype}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error converting column '{column}' to {target_dtype}: {e}\")\n",
    "            else:\n",
    "                logging.warning(f\"Classified data type '{classified_data_type}' for column '{column}' not recognized.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error converting data types: {e}\")\n",
    "        raise\n",
    "\n",
    "def save_processed_data(df: pd.DataFrame, save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the processed DataFrame to the specified path.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to save.\n",
    "    save_path (str): The path to save the DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_csv(save_path, index=False)\n",
    "        logging.info(f\"Processed data saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving processed data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def validate_data_types(df: pd.DataFrame, metadata: Dict[str, Dict[str, str]]) -> bool:\n",
    "    \"\"\"\n",
    "    Validate that the DataFrame columns have the correct data types according to the metadata.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to validate.\n",
    "    metadata (Dict[str, Dict[str, str]]): The metadata dictionary containing the classified data type for each feature.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if all data types are as expected, otherwise raises a ValueError.\n",
    "    \"\"\"\n",
    "    type_mapping = {\n",
    "        'categorical': 'category',\n",
    "        'numerical': 'float64',\n",
    "        'binary': 'bool'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        for column in df.columns:\n",
    "            classified_data_type = extract_classified_data_type(metadata, column)\n",
    "            if classified_data_type in type_mapping:\n",
    "                expected_dtype = type_mapping[classified_data_type]\n",
    "                actual_dtype = str(df[column].dtype)\n",
    "                if actual_dtype != expected_dtype:\n",
    "                    logging.error(f\"Column '{column}' has dtype '{actual_dtype}', expected '{expected_dtype}'.\")\n",
    "                    raise ValueError(f\"Data type mismatch for column '{column}': expected {expected_dtype}, got {actual_dtype}.\")\n",
    "            else:\n",
    "                logging.warning(f\"Classified data type '{classified_data_type}' for column '{column}' not recognized.\")\n",
    "        logging.info(\"All data types validated successfully.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error validating data types: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(train_data_path: str, test_data_path: str, train_save_path: str, test_save_path: str, metadata_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the loading, processing, validation, and saving of data with converted data types.\n",
    "\n",
    "    Parameters:\n",
    "    train_data_path (str): Path to the training data.\n",
    "    test_data_path (str): Path to the test data.\n",
    "    train_save_path (str): Path to save the processed training data.\n",
    "    test_save_path (str): Path to save the processed test data.\n",
    "    metadata_path (str): Path to the metadata JSON file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load metadata\n",
    "        metadata = load_json_file(metadata_path)\n",
    "        if not isinstance(metadata, dict):\n",
    "            logging.error(\"Expected metadata to be a dictionary, but got a different type.\")\n",
    "            raise TypeError(\"Metadata should be a dictionary, not a string.\")\n",
    "\n",
    "        # Load data\n",
    "        train_data, test_data = load_data(train_data_path, test_data_path)\n",
    "\n",
    "        # Convert data types\n",
    "        train_data = convert_data_types(train_data, metadata)\n",
    "        test_data = convert_data_types(test_data, metadata)\n",
    "\n",
    "        # Validate data types\n",
    "        validate_data_types(train_data, metadata)\n",
    "        validate_data_types(test_data, metadata)\n",
    "\n",
    "        # Save processed data\n",
    "        save_processed_data(train_data, train_save_path)\n",
    "        save_processed_data(test_data, test_save_path)\n",
    "\n",
    "        logging.info(\"Data type conversion and validation completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main function: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths (replace with actual paths from your environment setup)\n",
    "    train_data_path = os.path.join(paths['data']['intermediate_train_missing_values_indicators_added'])\n",
    "    test_data_path = os.path.join(paths['data']['intermediate_test_missing_values_indicators_added'])\n",
    "    train_save_path = os.path.join(paths['data']['intermediate_train_convert_data_types'])\n",
    "    test_save_path = os.path.join(paths['data']['intermediate_train_convert_data_types'])\n",
    "    metadata_path = paths['config']['feature_metadata']\n",
    "\n",
    "    main(train_data_path, test_data_path, train_save_path, test_save_path, metadata_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b44f7ee-9301-43a0-beaa-5568490872d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346db0f8-ea74-4b34-adff-c5d3e2ac26e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1c513-05e5-4b31-87b5-116a26e37901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a91e4-46fc-4f22-8e5a-a16425edd26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5740fc-1b3e-4370-a43d-9f508565041d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb35f2f-02d2-4c55-b757-97bdc7f0d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/02_initial_data_preparation/01_imputation_and_data_types/01b_convert_data_types.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def find_project_root(marker_file='src/config_loader.py') -> str:\n",
    "    \"\"\"\n",
    "    Locate the project root using the specified marker file.\n",
    "    Parameters:\n",
    "    marker_file (str): The marker file to identify the project root.\n",
    "    Returns:\n",
    "    str: The path to the project root directory.\n",
    "    \"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    while current_dir != os.path.dirname(current_dir):\n",
    "        if os.path.isfile(os.path.join(current_dir, marker_file)):\n",
    "            return current_dir\n",
    "        current_dir = os.path.dirname(current_dir)\n",
    "    raise FileNotFoundError(f\"Marker file '{marker_file}' not found in any parent directories.\")\n",
    "\n",
    "def set_project_root() -> str:\n",
    "    \"\"\"Set the project root directory.\"\"\"\n",
    "    project_root = find_project_root()\n",
    "    os.chdir(project_root)\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.append(project_root)\n",
    "    logging.info(f\"Project root set to: {project_root}\")\n",
    "    return project_root\n",
    "\n",
    "# Find and set the project root directory\n",
    "project_root = set_project_root()\n",
    "\n",
    "# Import setup function from custom module\n",
    "from src.utils.environment_setup import setup_project_environment\n",
    "\n",
    "# Set up the project environment\n",
    "paths, directories = setup_project_environment()\n",
    "\n",
    "# Import necessary custom modules\n",
    "\n",
    "\n",
    "def load_data(train_data_path: str, test_data_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load the train and test datasets.\n",
    "\n",
    "    Parameters:\n",
    "    train_data_path (str): Path to the training data.\n",
    "    test_data_path (str): Path to the test data.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]: The loaded training and test datasets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        train_data = pd.read_csv(train_data_path)\n",
    "        test_data = pd.read_csv(test_data_path)\n",
    "        logging.info(f\"Loaded train data from {train_data_path}\")\n",
    "        logging.info(f\"Loaded test data from {test_data_path}\")\n",
    "        return train_data, test_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def save_processed_data(df: pd.DataFrame, save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the processed DataFrame to the specified path.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to save.\n",
    "    save_path (str): The path to save the DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_csv(save_path, index=False)\n",
    "        logging.info(f\"Processed data saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving processed data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def main(train_data_path: str, test_data_path: str, train_save_path: str, test_save_path: str, metadata_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the loading, processing, validating, and saving of data with missing value indicators.\n",
    "\n",
    "    Parameters:\n",
    "    train_data_path (str): Path to the training data.\n",
    "    test_data_path (str): Path to the test data.\n",
    "    train_save_path (str): Path to save the processed training data.\n",
    "    test_save_path (str): Path to save the processed test data.\n",
    "    metadata_path (str): Path to the metadata JSON file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load metadata\n",
    "        metadata = load_json_file(metadata_path)\n",
    "        if not isinstance(metadata, dict):\n",
    "            logging.error(\"Expected metadata to be a dictionary, but got a different type.\")\n",
    "            raise TypeError(\"Metadata should be a dictionary, not a string.\")\n",
    "\n",
    "        # Load data\n",
    "        train_data, test_data = load_data(train_data_path, test_data_path)\n",
    "\n",
    "\n",
    "        # Save processed data\n",
    "        save_processed_data(train_data, train_save_path)\n",
    "        save_processed_data(test_data, test_save_path)\n",
    "        \n",
    "        logging.info(\"Missing value indicators processing and validation completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main function: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths (replace with actual paths from your environment setup)\n",
    "    train_data_path = os.path.join(paths['data']['intermediate_train_missing_values_indicators_added'])\n",
    "    test_data_path = os.path.join(paths['data']['intermediate_test_missing_values_indicators_added'])\n",
    "    train_save_path = os.path.join(paths['data']['intermediate_train_convert_data_types'])\n",
    "    test_save_path = os.path.join(paths['data']['intermediate_train_convert_data_types'])\n",
    "    metadata_path = paths['config']['feature_metadata']\n",
    "\n",
    "    main(train_data_path, test_data_path, train_save_path, test_save_path, metadata_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
