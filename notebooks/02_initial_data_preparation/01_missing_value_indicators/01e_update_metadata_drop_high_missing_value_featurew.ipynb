{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "551761d2-8bf0-476c-ba9b-c6e9e031c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250688e8-f678-461b-b83d-52ea318d7b10",
   "metadata": {},
   "source": [
    "## Guidelines for Data Cleaning in Professional Data Science Projects\n",
    "\n",
    "For a professional data science project, the elements of data cleaning should be performed in a logical and systematic order to ensure the highest quality and integrity of the data before any analysis or modeling is done. Hereâ€™s a recommended order for data cleaning:\n",
    "\n",
    "### 1. Data Collection and Integration\n",
    "- **Collect data from multiple sources:** Ensure all required data is gathered.\n",
    "- **Data integration:** Combine data from different sources and formats into a single dataset.\n",
    "\n",
    "### 2. Initial Data Inspection\n",
    "- **Understand the dataset:** Get an overview of the data structure, including the number of rows, columns, and data types.\n",
    "- **Check for duplicates:** Identify and handle any duplicate records.\n",
    "\n",
    "### 3. Handling Missing Values\n",
    "- **Identify missing values:** Check for missing data in each column.\n",
    "- **Decide on imputation or removal:** Based on the proportion and importance of missing values, decide whether to impute or remove them.\n",
    "\n",
    "### 4. Data Type Conversion\n",
    "- **Convert data types:** Ensure each column has the correct data type (e.g., convert strings to dates, integers to floats where necessary).\n",
    "\n",
    "### 5. Handling Outliers\n",
    "- **Identify outliers:** Use statistical methods or visualization to detect outliers.\n",
    "- **Decide on treatment:** Decide whether to remove, cap, or transform outliers.\n",
    "\n",
    "### 6. Data Normalization and Standardization\n",
    "- **Scale numerical data:** Normalize or standardize numerical features to ensure they have similar scales, especially important for models sensitive to feature scales (e.g., KNN, SVM).\n",
    "\n",
    "### 7. Encoding Categorical Variables\n",
    "- **Encode categorical features:** Use techniques like one-hot encoding, label encoding, or ordinal encoding to convert categorical data into numerical format.\n",
    "\n",
    "### 8. Feature Engineering\n",
    "- **Create new features:** Derive new features from existing ones that could provide more predictive power.\n",
    "- **Feature transformation:** Apply transformations such as log, square root, or polynomial transformations if needed.\n",
    "\n",
    "### 9. Data Quality Checks\n",
    "- **Consistency checks:** Ensure that the data is consistent and there are no logical errors (e.g., negative ages, future dates).\n",
    "- **Handle inconsistencies:** Correct or remove inconsistent data points.\n",
    "\n",
    "### 10. Data Reduction\n",
    "- **Dimensionality reduction:** Use techniques like PCA, t-SNE, or feature selection to reduce the number of features if needed.\n",
    "- **Aggregation:** Aggregate data if necessary, especially for time-series data.\n",
    "\n",
    "### 11. Data Splitting\n",
    "- **Split data:** Divide the data into training, validation, and test sets to evaluate model performance accurately.\n",
    "\n",
    "### 12. Data Augmentation (if applicable)\n",
    "- **Augment data:** For certain types of data, like images or time series, data augmentation techniques can be used to increase the dataset size artificially.\n",
    "\n",
    "### 13. Final Data Validation\n",
    "- **Final checks:** Perform a final inspection of the cleaned dataset to ensure there are no errors or inconsistencies left.\n",
    "- **Documentation:** Document the data cleaning steps, decisions made, and justifications for those decisions for transparency and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67839cf1-1dff-4cb8-b0c4-771418898d5f",
   "metadata": {},
   "source": [
    "## Labeling Missing Values\n",
    "\n",
    "The labeling process in the provided script works as follows:\n",
    "\n",
    "1. For each feature (column) in the dataset, a new column is created.\n",
    "2. This new column will have the same name as the original feature, but with a suffix `_is_missing`.\n",
    "3. Each value in the new column will be `1` if the corresponding value in the original feature is missing (`NaN`), and `0` if it is not missing.\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose you have a dataset with the following columns and some missing values:\n",
    "\n",
    "| FeatureA | FeatureB |\n",
    "|----------|----------|\n",
    "| 1        | NaN      |\n",
    "| NaN      | 2        |\n",
    "| 3        | 3        |\n",
    "| 4        | NaN      |\n",
    "\n",
    "The script will create two new columns: `FeatureA_is_missing` and `FeatureB_is_missing`, where each cell indicates whether the corresponding value in the original feature is missing:\n",
    "\n",
    "| FeatureA | FeatureB | FeatureA_is_missing | FeatureB_is_missing |\n",
    "|----------|----------|---------------------|---------------------|\n",
    "| 1        | NaN      | 0                   | 1                   |\n",
    "| NaN      | 2        | 1                   | 0                   |\n",
    "| 3        | 3        | 0                   | 0                   |\n",
    "| 4        | NaN      | 0                   | 1                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dcb2d9-8efa-4a78-922b-3f4401d1ad1e",
   "metadata": {},
   "source": [
    "# Data Processing Script Summary\n",
    "\n",
    "This script processes a large dataset by performing the following key steps:\n",
    "\n",
    "1. **Set Project Root and Adjust Paths**: Ensures correct file path handling.\n",
    "2. **Load Configuration Paths and Metadata**: Loads necessary configuration and metadata for data processing.\n",
    "3. **Load Raw Data with Progress Bar**: Reads the dataset with a progress bar for better monitoring.\n",
    "4. **Initial Data Validation**: Validates initial data shape and types.\n",
    "5. **Feature Dropping**: Identifies and drops features flagged for dropping based on metadata.\n",
    "6. **Imputation Strategy Extraction**: Extracts imputation strategy information and displays it.\n",
    "7. **Labeling Missing Values**: Labels missing values for future reference.\n",
    "8. **Imputation of Missing Values**: Imputes missing values according to the specified strategy.\n",
    "9. **Data Type Conversion**: Converts data types based on metadata and validates the conversion.\n",
    "10. **File Handling**: Removes existing file before saving to avoid appending issues.\n",
    "11. **Final Data Shape and Type Validation**: Validates the final data shape and types after processing.\n",
    "12. **Drop Missing Indicator Columns**: Drops columns with '_is_missing' suffix if no longer needed.\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "- **read_csv_with_progress**: Reads CSV data in chunks with a progress bar.\n",
    "- **load_paths**: Loads configuration paths.\n",
    "- **load_yaml**: Loads YAML configuration files.\n",
    "- **save_yaml**: Saves YAML configuration files.\n",
    "\n",
    "### Sample Output and Data Validation\n",
    "\n",
    "The script provides detailed logging of each step, including:\n",
    "- Initial and final data shapes.\n",
    "- Initial and final data types.\n",
    "- Features dropped and imputation strategies applied.\n",
    "- Validation of numerical data types and missing value indicators.\n",
    "- Final data saved to `data/intermediate/01_clean_imputation.csv` with consistent file size across runs.\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- The file `01_clean_imputation.csv` is deleted before each run to prevent appending issues.\n",
    "- Data type conversions are validated, and any discrepancies are logged for review.\n",
    "- Missing value indicators are only added for features with significant missing values (e.g., > 5%).\n",
    "\n",
    "This approach ensures robust data processing, consistent output, and clear traceability of each processing step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8af2ded-92c9-49dc-8ab9-ef0b15e2bdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 14:56:27,157 - INFO - Project root set to: /Users/rexcoleman/Documents/DataScienceAndMachineLearning/Rex_Coleman_Machine_Learning_Cybersecurity_Portfolio/Microsoft-Malware-Prediction-Project-2\n",
      "2024-08-19 14:56:27,264 - INFO - NumExpr defaulting to 8 threads.\n",
      "2024-08-19 14:56:27,455 - INFO - Project root set to: /Users/rexcoleman/Documents/DataScienceAndMachineLearning/Rex_Coleman_Machine_Learning_Cybersecurity_Portfolio/Microsoft-Malware-Prediction-Project-2\n",
      "2024-08-19 14:56:27,462 - INFO - Starting the metadata update process.\n",
      "2024-08-19 14:56:27,462 - INFO - Loading data from data/intermediate/train_sample.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3c04c315b34c919b74a2a87aa33720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 14:56:27,547 - INFO - Loading feature metadata schema\n",
      "2024-08-19 14:56:27,548 - INFO - Metadata loaded from config/schemas/feature_metadata_complete_schema.json\n",
      "2024-08-19 14:56:27,548 - INFO - Loading feature metadata\n",
      "2024-08-19 14:56:27,549 - INFO - Metadata loaded from config/feature_metadata.json\n",
      "2024-08-19 14:56:27,551 - INFO - Loading data from data/intermediate/test_sample.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12df131f2fb40fb99a0c71fc44ddce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 14:56:27,656 - INFO - Loading feature metadata schema\n",
      "2024-08-19 14:56:27,657 - INFO - Metadata loaded from config/schemas/feature_metadata_complete_schema.json\n",
      "2024-08-19 14:56:27,657 - INFO - Loading feature metadata\n",
      "2024-08-19 14:56:27,659 - INFO - Metadata loaded from config/feature_metadata.json\n",
      "2024-08-19 14:56:27,662 - INFO - Metadata loaded from config/feature_metadata.json\n",
      "2024-08-19 14:56:27,664 - INFO - Metadata loaded from config/schemas/feature_metadata_complete_schema.json\n",
      "2024-08-19 14:56:27,664 - INFO - Loaded schema type: <class 'dict'>\n",
      "2024-08-19 14:56:27,664 - INFO - Loaded schema content: {'metadata_schema': {'type': 'object', 'properties': {'default_attributes': {'type': 'object', 'properties': {'general_attributes': {'type': 'object', 'properties': {'description': {'type': 'string'}, 'security_context': {'type': 'string'}, 'technical_data_type': {'type': 'string'}, 'classified_data_type': {'type': 'string'}, 'importance': {'type': 'number'}, 'data_source': {'type': 'string'}}, 'required': ['description', 'security_context', 'technical_data_type', 'classified_data_type', 'importance', 'data_source']}, 'missing_values': {'type': 'object', 'properties': {'count': {'type': 'integer'}, 'percentage': {'type': 'number'}}, 'required': ['count', 'percentage']}, 'balance': {'type': 'object', 'properties': {'most_common_value_weight': {'type': 'number'}}, 'required': ['most_common_value_weight']}, 'correlations': {'type': 'object', 'properties': {'feature_correlation_with_target': {'type': 'number'}, 'feature_correlation_with_other_features': {'type': 'object'}, 'missing_value_correlation_with_target': {'type': 'number'}, 'missing_value_correlation_with_other_features': {'type': 'object'}}, 'required': ['feature_correlation_with_target', 'feature_correlation_with_other_features', 'missing_value_correlation_with_target', 'missing_value_correlation_with_other_features']}, 'outliers': {'type': 'object', 'properties': {'metrics': {'type': 'object', 'properties': {'z_score': {'type': 'array'}, 'iqr': {'type': 'array'}, 'mad': {'type': 'array'}, 'isolation_forest': {'type': 'array'}, 'lof': {'type': 'array'}, 'dbscan': {'type': 'array'}}, 'required': ['z_score', 'iqr', 'mad', 'isolation_forest', 'lof', 'dbscan']}, 'thresholds_exceeded': {'type': 'object', 'properties': {'log_transformation': {'type': 'boolean'}, 'square_root_transformation': {'type': 'boolean'}, 'clipping': {'type': 'boolean'}, 'removing_outliers': {'type': 'boolean'}}, 'required': ['log_transformation', 'square_root_transformation', 'clipping', 'removing_outliers']}}, 'required': ['metrics', 'thresholds_exceeded']}, 'feature_engineering': {'type': 'object', 'properties': {'imputation': {'type': 'object', 'properties': {'type': {'type': 'string'}, 'comments': {'type': 'string'}}, 'required': ['type', 'comments']}, 'encoding': {'type': 'object', 'properties': {'type': {'type': 'string'}, 'comments': {'type': 'string'}}, 'required': ['type', 'comments']}, 'transformation': {'type': 'object', 'properties': {'type': {'type': 'string'}, 'comments': {'type': 'string'}}, 'required': ['type', 'comments']}, 'outlier_handling': {'type': 'object', 'properties': {'remediation_type': {'type': 'string'}, 'comments': {'type': 'string'}}, 'required': ['remediation_type', 'comments']}}, 'required': ['imputation', 'encoding', 'transformation', 'outlier_handling']}, 'additional_attributes': {'type': 'object', 'properties': {'distribution': {'type': 'string'}, 'data_quality': {'type': 'string'}, 'example_values': {'type': 'array'}, 'summary_statistics': {'type': 'object'}, 'feature_interactions': {'type': 'object'}, 'temporal_stability': {'type': 'string'}, 'data_leakage_risk': {'type': 'string'}}, 'required': ['distribution', 'data_quality', 'example_values', 'summary_statistics', 'feature_interactions', 'temporal_stability', 'data_leakage_risk']}}, 'required': ['general_attributes', 'missing_values', 'balance', 'correlations', 'outliers', 'feature_engineering', 'additional_attributes']}, 'data_overview': {'type': 'object', 'properties': {'train_sample_shape': {'type': 'array', 'items': {'type': 'integer'}}, 'test_sample_shape': {'type': 'array', 'items': {'type': 'integer'}}, 'train_intermediate_01_missing_value_indicators_shape': {'type': 'array', 'items': {'type': 'integer'}}, 'test_intermediate_01_missing_value_indicators_shape': {'type': 'array', 'items': {'type': 'integer'}}, 'train_intermediate_02_drop_high_missing_value_features_shape': {'type': 'array', 'items': {'type': 'integer'}}, 'test_intermediate_02_drop_high_missing_value_features_shape': {'type': 'array', 'items': {'type': 'integer'}}, 'train_intermediate_03_convert_data_types_shape': {'type': 'array', 'items': {'type': 'integer'}}, 'test_intermediate_03_convert_data_types_shape': {'type': 'array', 'items': {'type': 'integer'}}, 'train_intermediate_04_imputation_shape': {'type': 'array', 'items': {'type': 'integer'}}, 'test_intermediate_04_imputation_shape': {'type': 'array', 'items': {'type': 'integer'}}, 'train_columns_sample': {'type': 'array', 'items': {'type': 'string'}}, 'test_columns_sample': {'type': 'array', 'items': {'type': 'string'}}, 'train_columns_intermediate_01_missing_value_indicators': {'type': 'array', 'items': {'type': 'string'}}, 'test_columns_intermediate_01_missing_value_indicators': {'type': 'array', 'items': {'type': 'string'}}, 'train_columns_intermediate_02_drop_high_missing_value_features': {'type': 'array', 'items': {'type': 'string'}}, 'test_columns_intermediate_02_drop_high_missing_value_features': {'type': 'array', 'items': {'type': 'string'}}, 'train_columns_intermediate_03_convert_data_types': {'type': 'array', 'items': {'type': 'string'}}, 'test_columns_intermediate_03_convert_data_types': {'type': 'array', 'items': {'type': 'string'}}, 'train_columns_intermediate_04_imputation': {'type': 'array', 'items': {'type': 'string'}}, 'test_columns_intermediate_04_imputation': {'type': 'array', 'items': {'type': 'string'}}, 'target_variable_analysis': {'type': 'object'}}, 'required': ['train_sample_shape', 'test_sample_shape', 'train_columns_sample', 'test_columns_sample', 'train_intermediate_01_missing_value_indicators_shape', 'test_intermediate_01_missing_value_indicators_shape', 'train_columns_intermediate_01_missing_value_indicators', 'test_columns_intermediate_01_missing_value_indicators', 'train_intermediate_02_drop_high_missing_value_features_shape', 'test_intermediate_02_drop_high_missing_value_features_shape', 'train_columns_intermediate_02_drop_high_missing_value_features', 'test_columns_intermediate_02_drop_high_missing_value_features', 'train_intermediate_03_convert_data_types_shape', 'test_intermediate_03_convert_data_types_shape', 'train_columns_intermediate_03_convert_data_types', 'test_columns_intermediate_03_convert_data_types', 'train_intermediate_04_imputation_shape', 'test_intermediate_04_imputation_shape', 'train_columns_intermediate_04_imputation', 'test_columns_intermediate_04_imputation', 'target_variable_analysis']}}, 'required': ['default_attributes', 'data_overview']}}\n",
      "2024-08-19 14:56:27,665 - INFO - Metadata loaded from reports/analysis_results/essential_missing_value_features.json\n",
      "2024-08-19 14:56:27,666 - INFO - Processing update data from: reports/analysis_results/essential_missing_value_features.json\n",
      "2024-08-19 14:56:27,666 - INFO - Updated metadata with data from: reports/analysis_results/essential_missing_value_features.json\n",
      "2024-08-19 14:56:27,666 - INFO - Metadata loaded from reports/analysis_results/data_overview_02_drop_high_missing_value_features.json\n",
      "2024-08-19 14:56:27,667 - INFO - Processing update data from: reports/analysis_results/data_overview_02_drop_high_missing_value_features.json\n",
      "2024-08-19 14:56:27,667 - INFO - Updated metadata with data from: reports/analysis_results/data_overview_02_drop_high_missing_value_features.json\n",
      "2024-08-19 14:56:27,668 - INFO - Metadata validation successful.\n",
      "2024-08-19 14:56:27,676 - INFO - Metadata saved to config/feature_metadata.json\n",
      "2024-08-19 14:56:27,676 - INFO - Dynamic metadata pipeline completed successfully.\n",
      "2024-08-19 14:56:27,679 - INFO - Metadata loaded from config/feature_metadata.json\n",
      "2024-08-19 14:56:27,679 - INFO - Metadata loaded from config/schemas/feature_metadata_complete_schema.json\n",
      "2024-08-19 14:56:27,680 - INFO - Metadata is consistent with the JSON schema.\n",
      "2024-08-19 14:56:27,680 - INFO - Metadata loaded from config/schemas/feature_metadata_complete_schema.json\n",
      "2024-08-19 14:56:27,721 - INFO - JSON and YAML schemas are consistent.\n",
      "2024-08-19 14:56:27,721 - INFO - Metadata update process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# notebooks/02_initial_data_preparation/01_missing_value_indicators/01d_update_metadata_drop_high_missing_value_featurew.ipynb\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def find_project_root(marker_file: str = 'src/config_loader.py') -> str:\n",
    "    \"\"\"\n",
    "    Locate the project root using the specified marker file.\n",
    "\n",
    "    Parameters:\n",
    "    marker_file (str): The marker file to identify the project root.\n",
    "    \n",
    "    Returns:\n",
    "    str: The path to the project root directory.\n",
    "    \n",
    "    Raises:\n",
    "    FileNotFoundError: If the marker file is not found in any parent directories.\n",
    "    \"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    while current_dir != os.path.dirname(current_dir):\n",
    "        if os.path.isfile(os.path.join(current_dir, marker_file)):\n",
    "            return current_dir\n",
    "        current_dir = os.path.dirname(current_dir)\n",
    "    raise FileNotFoundError(f\"Marker file '{marker_file}' not found in any parent directories.\")\n",
    "\n",
    "def set_project_root() -> str:\n",
    "    \"\"\"\n",
    "    Set the project root directory and ensure it's added to sys.path.\n",
    "\n",
    "    Returns:\n",
    "    str: The project root directory path.\n",
    "    \"\"\"\n",
    "    project_root = find_project_root()\n",
    "    os.chdir(project_root)\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.append(project_root)\n",
    "    logger.info(f\"Project root set to: {project_root}\")\n",
    "    return project_root\n",
    "\n",
    "# Find and set the project root directory\n",
    "project_root = set_project_root()\n",
    "\n",
    "# Import setup function from custom module\n",
    "from src.utils.environment_setup import setup_project_environment\n",
    "from src.utils.initialization import initialize_data_and_metadata \n",
    "from src.utils.metadata_operations import dynamic_metadata_pipeline\n",
    "from src.validation.validate_schemas import validate_schemas\n",
    "from src.validation.validate_metadata import validate_metadata\n",
    "\n",
    "# Set up the project environment\n",
    "paths, directories = setup_project_environment()\n",
    "\n",
    "def main(paths: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Main function to update feature metadata using the dynamic metadata pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    paths (Dict[str, Any]): A dictionary of paths loaded from the config file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting the metadata update process.\")\n",
    "        \n",
    "        # Initialize train and test data, schema, and feature metadata\n",
    "        train_data_path = paths['data']['intermediate_02_train_drop_high_missing_value_features']\n",
    "        test_data_path = paths['data']['intermediate_02_test_drop_high_missing_value_features']\n",
    "        train_data, test_data, schema, feature_metadata = initialize_data_and_metadata(paths)\n",
    "\n",
    "        # Define the paths to the required JSON files\n",
    "        metadata_path = paths['config']['feature_metadata']\n",
    "        schema_path = paths['config']['schemas']['feature_metadata_complete_schema_json']\n",
    "        essential_missing_value_features_path = os.path.join(paths['reports']['analysis_results'], 'essential_missing_value_features.json')\n",
    "        data_overview_missing_value_indicators_path = os.path.join(paths['reports']['analysis_results'], 'data_overview_02_drop_high_missing_value_features.json')\n",
    "\n",
    "        # List of JSON files to update the metadata with\n",
    "        json_files = [essential_missing_value_features_path, data_overview_missing_value_indicators_path]\n",
    "\n",
    "        # Run the dynamic metadata pipeline\n",
    "        dynamic_metadata_pipeline(metadata_path, json_files, schema_path)\n",
    "\n",
    "        # Validate the updated metadata and schema\n",
    "        if validate_metadata(metadata_path, schema_path):\n",
    "            logger.info(\"Metadata validation passed successfully.\")\n",
    "\n",
    "        yaml_schema_path = paths['config']['schemas']['feature_metadata_complete_schema_yaml']\n",
    "        validate_schemas(schema_path, yaml_schema_path)\n",
    "        \n",
    "        logger.info(\"Metadata update process completed successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main function: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3464f9-d089-470e-afec-ecabf0b18d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
