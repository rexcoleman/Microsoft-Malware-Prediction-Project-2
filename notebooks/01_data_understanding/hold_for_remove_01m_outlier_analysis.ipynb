{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7daf1798-096b-45ff-9210-787046070c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79893571-cae1-4f3f-9b98-72e9a391b579",
   "metadata": {},
   "source": [
    "## Automated Outlier Remediation Strategy\n",
    "\n",
    "An automated outlier remediation strategy can indeed be devised using the four metrics mentioned (Z-Score Outliers, IQR Outliers, Z-Score Outliers (%), and IQR Outliers (%)), along with additional metrics for a more robust approach. Here's a step-by-step outline and potential implementation:\n",
    "\n",
    "### Identify Outliers\n",
    "- **Using Z-Score and IQR methods** to identify outliers.\n",
    "\n",
    "### Determine Severity\n",
    "- **Based on the percentage of outliers**.\n",
    "\n",
    "### Decide on Action\n",
    "- **Based on a predefined threshold** of outlier percentages.\n",
    "\n",
    "### Additional Metrics for Automated Approach\n",
    "- **Skewness**: Measure the asymmetry of the distribution. High skewness indicates a potential need for transformation.\n",
    "- **Kurtosis**: Measure the \"tailedness\" of the distribution. High kurtosis indicates a potential need for addressing extreme values.\n",
    "- **Missing Values**: High number of missing values can sometimes be related to outliers or data entry errors.\n",
    "- **Standard Deviation**: Features with high standard deviation relative to their mean might need special attention.\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "#### `identify_outliers` Function:\n",
    "- Calculates Z-Score Outliers, IQR Outliers, Skewness, and Kurtosis for each numerical feature.\n",
    "- Returns a DataFrame with these metrics.\n",
    "\n",
    "#### `remediate_outliers` Function:\n",
    "- Checks the percentage of Z-Score and IQR Outliers.\n",
    "- Applies log transformation if skewness is high.\n",
    "- Applies square root transformation if kurtosis is high.\n",
    "- Clips values using IQR method if outlier percentages are high but not excessively skewed or kurtotic.\n",
    "- Returns the cleaned DataFrame.\n",
    "\n",
    "### Additional Considerations:\n",
    "- **Customizable Thresholds**: The thresholds for outlier detection (e.g., 5% for Z-Score and IQR outliers, 1 for skewness, and 3 for kurtosis) can be adjusted based on the specific dataset and domain requirements.\n",
    "- **Feature Engineering**: Further steps might include feature engineering based on domain knowledge to handle specific types of outliers or anomalies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c795f0-3a09-4c47-8d8b-6cd223660887",
   "metadata": {},
   "source": [
    "### Order of Priority for Outlier Remediation Strategies\n",
    "\n",
    "The order of priority for outlier remediation strategies is based on the severity of the impact that different types of outliers can have on the distribution and the modeling process. Hereâ€™s the data science reasoning behind the proposed order of priority:\n",
    "\n",
    "1. **Skewness and Kurtosis Transformations**:\n",
    "   - **Skewness**: High skewness indicates a long tail in the distribution, which can significantly affect statistical analyses and machine learning models, especially linear models. Transforming skewed data (using log, square root, etc.) can stabilize variance, make the data more normally distributed, and improve model performance.\n",
    "   - **Kurtosis**: High kurtosis indicates heavy tails and potential outliers. Transforming data with high kurtosis can reduce the impact of extreme values and make the distribution more normal, which benefits many statistical methods and models.\n",
    "\n",
    "2. **Clipping**:\n",
    "   - **Z-Score and IQR Outliers**: These methods identify extreme values based on statistical properties of the data. High numbers of outliers can skew the mean and standard deviation, impacting the performance of models. Clipping the extreme values can mitigate their impact without removing too much data, preserving as much information as possible.\n",
    "\n",
    "3. **Removing Outliers**:\n",
    "   - **Machine Learning Outlier Detection Methods**: Algorithms like Isolation Forest, LOF, and DBSCAN are designed to identify points that differ significantly from the majority of the data. Removing these outliers can help in scenarios where extreme values are likely to be errors or noise that do not contribute valuable information to the model.\n",
    "\n",
    "4. **No Action**:\n",
    "   - **When no significant outliers are detected**: If none of the methods identify a significant number of outliers, or if the detected outliers are not expected to impact the analysis significantly, no action is taken. This ensures that the data is not altered unnecessarily, preserving the original characteristics.\n",
    "\n",
    "### Detailed Reasoning for Priority Order\n",
    "\n",
    "1. **Transformations (Log, Square Root)**:\n",
    "   - **Effect on Distribution**: Transformations directly affect the shape of the distribution, making it more normal. This is crucial because many statistical techniques and machine learning algorithms (e.g., linear regression, k-means clustering) assume normally distributed data.\n",
    "   - **Handling Skewness**: Log and square root transformations are effective in handling right-skewed data, reducing the impact of large outliers by compressing the range of values.\n",
    "   - **Handling Kurtosis**: These transformations also help in reducing kurtosis by pulling in extreme values, making the distribution's tails thinner.\n",
    "\n",
    "2. **Clipping**:\n",
    "   - **Mitigating Impact of Extremes**: Clipping reduces the influence of extreme values without completely removing them from the dataset. This is particularly useful when the outliers are genuine but extreme values that can distort the analysis.\n",
    "   - **Preserving Data**: Unlike removal, clipping retains all data points, only modifying the extreme values. This can be crucial in scenarios where data points are valuable and should not be discarded.\n",
    "\n",
    "3. **Removing Outliers**:\n",
    "   - **Significant Deviation**: Outliers detected by machine learning methods like Isolation Forest, LOF, and DBSCAN often represent significant deviations from the norm. These can be errors, noise, or irrelevant anomalies that could mislead the analysis or model.\n",
    "   - **Error Correction**: Removing outliers is justified when they are likely errors or non-representative of the population, ensuring cleaner and more reliable data for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c3cfe8c-d1e8-4e60-95f5-88834070be3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/intermediate/01_clean_imputation.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Load data with specified dtypes to prevent dtype warning\u001b[39;00m\n\u001b[1;32m     41\u001b[0m train_data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintermediate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01_clean_imputation.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mread_csv_with_progress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Calculate outlier metrics\u001b[39;00m\n\u001b[1;32m     45\u001b[0m outlier_metrics \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m, in \u001b[0;36mread_csv_with_progress\u001b[0;34m(file_path, chunksize)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_csv_with_progress\u001b[39m(file_path, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m     total_lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Calculate total lines in the file\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     chunk_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m tqdm(pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, chunksize\u001b[38;5;241m=\u001b[39mchunksize, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), total\u001b[38;5;241m=\u001b[39mtotal_lines\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mchunksize \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/data_science_environment/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/intermediate/01_clean_imputation.csv'"
     ]
    }
   ],
   "source": [
    "# notebooks/01_data_understanding/01m_outlier_analysis.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore, skew, kurtosis\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Set the project root and adjust paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "os.chdir(project_root)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Load necessary custom modules\n",
    "from src.utils.common import load_yaml, save_json_file, save_dataframe\n",
    "from src.config_loader import load_paths\n",
    "\n",
    "# Load configuration paths\n",
    "paths = load_paths()\n",
    "\n",
    "# Load existing metadata\n",
    "metadata_path = paths['config']['feature_metadata']\n",
    "metadata = load_yaml(metadata_path)\n",
    "\n",
    "# Function to read CSV in chunks with progress bar\n",
    "def read_csv_with_progress(file_path, chunksize=10000):\n",
    "    total_lines = sum(1 for _ in open(file_path)) - 1  # Calculate total lines in the file\n",
    "    chunk_list = []\n",
    "    for chunk in tqdm(pd.read_csv(file_path, chunksize=chunksize, low_memory=False), total=total_lines//chunksize + 1, desc=\"Loading data\"):\n",
    "        chunk_list.append(chunk)\n",
    "    return pd.concat(chunk_list, axis=0)\n",
    "\n",
    "# Load data with specified dtypes to prevent dtype warning\n",
    "train_data_path = os.path.join('data', 'intermediate', '01_clean_imputation.csv')\n",
    "train_data = read_csv_with_progress(train_data_path)\n",
    "\n",
    "# Calculate outlier metrics\n",
    "outlier_metrics = []\n",
    "\n",
    "for feature in tqdm(train_data.columns, desc=\"Processing features\"):\n",
    "    if train_data[feature].dtype in ['float64', 'int64']:\n",
    "        feature_data = train_data[feature].dropna()\n",
    "        if feature_data.empty:\n",
    "            continue\n",
    "\n",
    "        # Z-Score Outliers\n",
    "        z_scores = zscore(feature_data)\n",
    "        z_outliers = (z_scores > 3) | (z_scores < -3)\n",
    "\n",
    "        # IQR Outliers\n",
    "        iqr = feature_data.quantile(0.75) - feature_data.quantile(0.25)\n",
    "        lower_bound = feature_data.quantile(0.25) - 1.5 * iqr\n",
    "        upper_bound = feature_data.quantile(0.75) + 1.5 * iqr\n",
    "        iqr_outliers = (feature_data < lower_bound) | (feature_data > upper_bound)\n",
    "\n",
    "        # MAD Scores\n",
    "        median = feature_data.median()\n",
    "        mad = np.median(np.abs(feature_data - median))\n",
    "        mad_scores = np.abs(feature_data - median) / mad\n",
    "\n",
    "        # Isolation Forest Outliers\n",
    "        isolation_forest = IsolationForest(contamination=0.05)\n",
    "        isolation_forest.fit(feature_data.values.reshape(-1, 1))\n",
    "        iso_outliers = isolation_forest.predict(feature_data.values.reshape(-1, 1)) == -1\n",
    "\n",
    "        # LOF Outliers\n",
    "        lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "        lof_outliers = lof.fit_predict(feature_data.values.reshape(-1, 1)) == -1\n",
    "\n",
    "        # DBSCAN Outliers\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "        dbscan_outliers = dbscan.fit_predict(feature_data.values.reshape(-1, 1)) == -1\n",
    "\n",
    "        # Skewness and Kurtosis\n",
    "        skewness = skew(feature_data)\n",
    "        kurtosis_value = kurtosis(feature_data)\n",
    "\n",
    "        metrics = {\n",
    "            'Feature': feature,\n",
    "            'Z-Score Outliers': z_outliers.sum(),\n",
    "            'IQR Outliers': iqr_outliers.sum(),\n",
    "            'Z-Score Outliers (%)': z_outliers.mean() * 100,\n",
    "            'IQR Outliers (%)': iqr_outliers.mean() * 100,\n",
    "            'MAD Scores': (mad_scores > 3).sum(),\n",
    "            'Isolation Forest Outliers': iso_outliers.sum(),\n",
    "            'LOF Outliers': lof_outliers.sum(),\n",
    "            'DBSCAN Outliers': dbscan_outliers.sum(),\n",
    "            'Skewness': skewness,\n",
    "            'Kurtosis': kurtosis_value\n",
    "        }\n",
    "        outlier_metrics.append(metrics)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "outlier_metrics_df = pd.DataFrame(outlier_metrics)\n",
    "\n",
    "# Save outlier metrics to JSON\n",
    "if not outlier_metrics_df.empty:\n",
    "    outlier_metrics_path = os.path.join(paths['reports']['analysis_results'], 'outlier_metrics.json')\n",
    "    save_json_file(outlier_metrics_df.to_dict(orient='records'), outlier_metrics_path)\n",
    "\n",
    "# Ensure the output directories exist\n",
    "output_plot_dir = os.path.join(paths['reports']['figures']['univariate_analysis'], '01_outlier_analysis')\n",
    "output_table_dir = os.path.join(paths['reports']['tables']['univariate_analysis'], '01_outlier_analysis')\n",
    "os.makedirs(output_plot_dir, exist_ok=True)\n",
    "os.makedirs(output_table_dir, exist_ok=True)\n",
    "\n",
    "# Grouping the data for each feature and saving it as a combined plot and table\n",
    "for feature in tqdm(train_data.columns, desc=\"Creating plots\"):\n",
    "    if train_data[feature].dtype in ['float64', 'int64']:\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "        sns.boxplot(x=train_data[feature], ax=ax[0, 0])\n",
    "        ax[0, 0].set_title(f\"Box Plot of {feature}\")\n",
    "\n",
    "        sns.histplot(train_data[feature], bins=30, kde=True, ax=ax[0, 1])\n",
    "        ax[0, 1].set_title(f\"Histogram of {feature}\")\n",
    "\n",
    "        sns.scatterplot(x=train_data.index, y=train_data[feature], ax=ax[1, 0])\n",
    "        ax[1, 0].set_title(f\"Scatter Plot of {feature}\")\n",
    "        ax[1, 0].set_xlabel('Index')\n",
    "\n",
    "        sns.violinplot(x=train_data[feature], ax=ax[1, 1])\n",
    "        ax[1, 1].set_title(f\"Violin Plot of {feature}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the combined plot as a figure\n",
    "        output_plot_path = os.path.join(output_plot_dir, f'{feature}_outlier_analysis_plots.png')\n",
    "        fig.savefig(output_plot_path)\n",
    "        plt.show()  # Display the figure inline\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Create a summary table\n",
    "        summary_data = {\n",
    "            'Metric': ['Z-Score Outliers', 'IQR Outliers', 'Z-Score Outliers (%)', 'IQR Outliers (%)', 'MAD Scores', 'Isolation Forest Outliers', 'LOF Outliers', 'DBSCAN Outliers', 'Skewness', 'Kurtosis'],\n",
    "            'Value': [\n",
    "                outlier_metrics_df[outlier_metrics_df['Feature'] == feature]['Z-Score Outliers'].values[0],\n",
    "                outlier_metrics_df[outlier_metrics_df['Feature'] == feature]['IQR Outliers'].values[0],\n",
    "                outlier_metrics_df[outlier_metrics_df['Feature'] == feature]['Z-Score Outliers (%)'].values[0],\n",
    "                outlier_metrics_df[outlier_metrics_df['Feature'] == feature]['IQR Outliers (%)'].values[0],\n",
    "                outlier_metrics_df[outlier_metrics_df['Feature'] == feature]['MAD Scores'].values[0],\n",
    "                outlier_metrics_df[outlier_metrics_df['Feature'] == feature]['Isolation Forest Outliers'].values[0],\n",
    "                outlier_metrics_df[outlier_metrics_df['Feature'] == feature]['LOF Outliers'].values[0],\n",
    "                outlier_metrics_df[outlier_metrics_df['Feature'] == feature]['DBSCAN Outliers'].values[0],\n",
    "                outlier_metrics_df[outlier_metrics_df['Feature'] == feature]['Skewness'].values[0],\n",
    "                outlier_metrics_df[outlier_metrics_df['Feature'] == feature]['Kurtosis'].values[0]\n",
    "            ]\n",
    "        }\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "        # Save the summary table\n",
    "        output_table_path = os.path.join(output_table_dir, f'{feature}_outlier_analysis_summary.csv')\n",
    "        save_dataframe(summary_df, output_table_path)\n",
    "\n",
    "        # Create a separate figure for the table\n",
    "        fig, ax = plt.subplots(figsize=(12, 2))  # Adjust height to fit the table\n",
    "        ax.axis('off')\n",
    "        table = ax.table(cellText=summary_df.values, colLabels=summary_df.columns, cellLoc='center', loc='center')\n",
    "        table.scale(1, 2)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the table as a separate figure\n",
    "        output_table_plot_path = os.path.join(output_plot_dir, f'{feature}_outlier_analysis_table.png')\n",
    "        plt.savefig(output_table_plot_path)\n",
    "        plt.show()  # Display the table inline\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Display plots and table inline in notebook\n",
    "        from IPython.display import Image, display\n",
    "        display(Image(filename=output_plot_path))\n",
    "        display(summary_df)\n",
    "        display(Image(filename=output_table_plot_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc863ef5-ff33-457c-a46c-bcc4adb97798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
